{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Order Data Generator Tutorial\n",
    "\n",
    "##  Lernziele / Learning Objectives\n",
    "\n",
    "In diesem Tutorial lernen Sie:\n",
    "1. Was First-Order Daten sind und warum sie wichtig sind\n",
    "2. Wie man den `FirstOrderDataGenerator` verwendet\n",
    "3. Wie man Verteilungen speichert und lädt\n",
    "4. Wie man mit `FirstOrderDataset` und DataLoader arbeitet\n",
    "5. Wie man ein Modell mit Soft Targets trainiert\n",
    "\n",
    "**Voraussetzungen:** Grundkenntnisse in PyTorch\n",
    "\n",
    "**Dauer:** ~30 Minuten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 1: Einführung und Setup\n",
    "\n",
    "### Was sind First-Order Daten?\n",
    "\n",
    "In Machine Learning arbeiten wir mit der Verteilung $p(X, Y)$, wobei:\n",
    "- $X$ = Eingabe-Features\n",
    "- $Y$ = Ziel-Labels\n",
    "\n",
    "Die **bedingte Verteilung** $p(Y|X)$ sagt uns: \"Gegeben ein bestimmtes $x$, wie wahrscheinlich sind die verschiedenen Klassen?\"\n",
    "\n",
    "**Problem:** Normalerweise haben wir keinen Zugang zu $p(Y|X)$!\n",
    "\n",
    "**Lösung:** Wir approximieren diese mit einem gut trainierten Modell $\\hat{h}$:\n",
    "$$\\hat{h}(x) \\approx p(\\cdot | x)$$\n",
    "\n",
    "Diese Approximationen nennen wir **First-Order Daten**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# First-Order Generator importieren\n",
    "# HINWEIS: Passen Sie den Import-Pfad an Ihre Projektstruktur an!\n",
    "from probly.data_generator.first_order_generator import (\n",
    "    FirstOrderDataGenerator,\n",
    "    FirstOrderDataset,\n",
    "    output_fo_dataloader\n",
    ")\n",
    "\n",
    "print(\" Alle Imports erfolgreich!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 2: Beispiel-Daten vorbereiten\n",
    "\n",
    "Wir erstellen einen einfachen Datensatz und ein \"Teacher\"-Modell, das wir als Ground Truth verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Dataset\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"Ein einfacher Datensatz für Demonstrations-Zwecke.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples: int = 200, input_dim: int = 10, n_classes: int = 3, seed: int = 42) -> None:\n",
    "        \"\"\"Initialize dataset.\"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        self.n_samples = n_samples\n",
    "        self.input_dim = input_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Generiere synthetische Daten\n",
    "        self.data = torch.randn(n_samples, input_dim)\n",
    "        self.labels = torch.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length.\"\"\"\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        \"\"\"Get item.\"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = SimpleDataset(n_samples=200, input_dim=10, n_classes=3)\n",
    "print(f\" Dataset erstellt: {len(dataset)} Samples, {dataset.n_classes} Klassen\")\n",
    "\n",
    "# Beispiel-Sample anschauen\n",
    "sample_x, sample_y = dataset[0]\n",
    "print(f\"\\nBeispiel Sample:\")\n",
    "print(f\"  Input shape: {sample_x.shape}\")\n",
    "print(f\"  Label: {sample_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-Modell (repräsentiert die \"Ground Truth\")\n",
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Ein einfaches neuronales Netzwerk als Teacher-Modell.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 10, n_classes: int = 3) -> None:\n",
    "        \"\"\"Initialize model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.network(x)  # Gibt Logits zurück\n",
    "\n",
    "# Modell erstellen\n",
    "teacher_model = TeacherModel(input_dim=10, n_classes=3)\n",
    "teacher_model.eval()  # Wichtig: In Evaluation-Modus!\n",
    "\n",
    "print(\" Teacher-Modell erstellt\")\n",
    "print(f\"\\nModell-Architektur:\")\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 3: First-Order Verteilungen generieren\n",
    "\n",
    "Jetzt verwenden wir den `FirstOrderDataGenerator`, um für jedes Sample im Dataset eine Wahrscheinlichkeitsverteilung zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator initialisieren\n",
    "generator = FirstOrderDataGenerator(\n",
    "    model=teacher_model,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size=32,\n",
    "    output_mode='logits',  # Unser Modell gibt Logits aus\n",
    "    model_name='teacher_v1'\n",
    ")\n",
    "\n",
    "print(\" FirstOrderDataGenerator initialisiert\")\n",
    "print(f\"  Device: {generator.device}\")\n",
    "print(f\"  Batch Size: {generator.batch_size}\")\n",
    "print(f\"  Output Mode: {generator.output_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilungen generieren\n",
    "print(\"⏳ Generiere First-Order Verteilungen...\\n\")\n",
    "\n",
    "distributions = generator.generate_distributions(\n",
    "    dataset,\n",
    "    progress=True  # Zeigt Fortschritt an\n",
    ")\n",
    "\n",
    "print(f\"\\n {len(distributions)} Verteilungen generiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Verteilungen anschauen\n",
    "print(\" Beispiel-Verteilungen:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    dist = distributions[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Verteilung: [{dist[0]:.4f}, {dist[1]:.4f}, {dist[2]:.4f}]\")\n",
    "    print(f\"  Summe: {sum(dist):.6f} (sollte ≈ 1.0 sein)\")\n",
    "    print(f\"  Wahrscheinlichste Klasse: {np.argmax(dist)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Verteilungen für erste 10 Samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axes[i]\n",
    "    dist = distributions[i]\n",
    "    \n",
    "    ax.bar(range(len(dist)), dist, color=['blue', 'orange', 'green'])\n",
    "    ax.set_title(f'Sample {i}')\n",
    "    ax.set_xlabel('Klasse')\n",
    "    ax.set_ylabel('Wahrscheinlichkeit')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('First-Order Verteilungen für die ersten 10 Samples', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\" Visualisierung: Jeder Balken zeigt die Wahrscheinlichkeit für eine Klasse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 4: Verteilungen speichern und laden\n",
    "\n",
    "Wir können die generierten Verteilungen als JSON-Datei speichern, um sie später wiederzuverwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verzeichnis erstellen\n",
    "output_dir = Path('tutorial_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Pfad definieren\n",
    "save_path = output_dir / 'first_order_distributions.json'\n",
    "\n",
    "# Metadaten definieren\n",
    "metadata = {\n",
    "    'dataset': 'SimpleDataset',\n",
    "    'n_samples': len(dataset),\n",
    "    'n_classes': dataset.n_classes,\n",
    "    'input_dim': dataset.input_dim,\n",
    "    'note': 'Generated for tutorial purposes',\n",
    "    'teacher_architecture': 'Simple 3-layer network'\n",
    "}\n",
    "\n",
    "# Speichern\n",
    "print(f\" Speichere Verteilungen nach: {save_path}\")\n",
    "generator.save_distributions(\n",
    "    path=save_path,\n",
    "    distributions=distributions,\n",
    "    meta=metadata\n",
    ")\n",
    "print(\" Erfolgreich gespeichert!\")\n",
    "\n",
    "# Dateigröße anzeigen\n",
    "file_size = save_path.stat().st_size / 1024  # in KB\n",
    "print(f\"\\n Dateigröße: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilungen laden\n",
    "print(\" Lade Verteilungen...\\n\")\n",
    "\n",
    "loaded_distributions, loaded_metadata = generator.load_distributions(save_path)\n",
    "\n",
    "print(\" Erfolgreich geladen!\\n\")\n",
    "print(\" Metadaten:\")\n",
    "for key, value in loaded_metadata.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# Verifizierung\n",
    "print(f\"\\n Verifizierung:\")\n",
    "print(f\"  Anzahl Verteilungen: {len(loaded_distributions)}\")\n",
    "print(f\"  Daten identisch: {distributions == loaded_distributions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 5: FirstOrderDataset verwenden\n",
    "\n",
    "`FirstOrderDataset` ist ein PyTorch Dataset-Wrapper, der den ursprünglichen Datensatz mit den First-Order Verteilungen kombiniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FirstOrderDataset erstellen\n",
    "fo_dataset = FirstOrderDataset(\n",
    "    base_dataset=dataset,\n",
    "    distributions=loaded_distributions\n",
    ")\n",
    "\n",
    "print(f\" FirstOrderDataset erstellt mit {len(fo_dataset)} Samples\\n\")\n",
    "\n",
    "# Ein Sample abrufen\n",
    "input_tensor, label, distribution = fo_dataset[0]\n",
    "\n",
    "print(\" Sample 0:\")\n",
    "print(f\"  Input Shape: {input_tensor.shape}\")\n",
    "print(f\"  Label: {label}\")\n",
    "print(f\"  Distribution Shape: {distribution.shape}\")\n",
    "print(f\"  Distribution: [{distribution[0]:.4f}, {distribution[1]:.4f}, {distribution[2]:.4f}]\")\n",
    "print(f\"  Summe: {distribution.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrere Samples durchgehen\n",
    "print(\" Durchlaufe mehrere Samples:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    input_tensor, label, distribution = fo_dataset[i]\n",
    "    predicted_class = torch.argmax(distribution).item()\n",
    "    confidence = distribution[predicted_class].item()\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Ground Truth Label: {label}\")\n",
    "    print(f\"  Predicted Class: {predicted_class}\")\n",
    "    print(f\"  Confidence: {confidence:.4f}\")\n",
    "    print(f\"  Match: {'' if predicted_class == label else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 6: DataLoader erstellen\n",
    "\n",
    "Für das Training brauchen wir einen DataLoader, der Batches mit First-Order Verteilungen liefert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader mit First-Order Verteilungen erstellen\n",
    "fo_loader = output_fo_dataloader(\n",
    "    base_dataset=dataset,\n",
    "    distributions=loaded_distributions,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Für Windows-Kompatibilität\n",
    ")\n",
    "\n",
    "print(f\" DataLoader erstellt\")\n",
    "print(f\"  Batch Size: 32\")\n",
    "print(f\"  Anzahl Batches: {len(fo_loader)}\")\n",
    "print(f\"  Shuffle: True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ersten Batch anschauen\n",
    "batch_inputs, batch_labels, batch_distributions = next(iter(fo_loader))\n",
    "\n",
    "print(\"\\n Erster Batch:\")\n",
    "print(f\"  Inputs Shape: {batch_inputs.shape}\")\n",
    "print(f\"  Labels Shape: {batch_labels.shape}\")\n",
    "print(f\"  Distributions Shape: {batch_distributions.shape}\")\n",
    "print(f\"\\n  Erste 3 Verteilungen im Batch:\")\n",
    "for i in range(3):\n",
    "    dist = batch_distributions[i]\n",
    "    print(f\"    Sample {i}: [{dist[0]:.4f}, {dist[1]:.4f}, {dist[2]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 7: Student-Modell mit Soft Targets trainieren\n",
    "\n",
    "Jetzt trainieren wir ein \"Student\"-Modell, das versucht, die Verteilungen des Teacher-Modells zu lernen. Dies nennt man **Knowledge Distillation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student-Modell (kleineres Netzwerk)\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Ein kleineres Modell, das vom Teacher lernt.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 10, n_classes: int = 3) -> None:\n",
    "        \"\"\"Initialize model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # Kleiner als Teacher\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "# Student-Modell erstellen\n",
    "student_model = StudentModel(input_dim=10, n_classes=3)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "print(\" Student-Modell erstellt\")\n",
    "print(f\"\\nVergleich Teacher vs Student:\")\n",
    "print(f\"  Teacher Parameter: {sum(p.numel() for p in teacher_model.parameters())}\")\n",
    "print(f\"  Student Parameter: {sum(p.numel() for p in student_model.parameters())}\")\n",
    "    print(f\"  Student ist {ratio:.1f}x kleiner!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training-Setup\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "# Listen für Tracking\n",
    "train_losses = []\n",
    "\n",
    "print(\" Starte Training...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    student_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for inputs, _labels, target_distributions in fo_loader:\n",
    "        # Daten zum Device verschieben\n",
    "        batch_inputs = inputs.to(device)\n",
    "        batch_target_distributions = target_distributions.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = student_model(batch_inputs)\n",
    "        \n",
    "        # KL Divergence Loss\n",
    "        # Student versucht, die Teacher-Verteilungen zu imitieren\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.kl_div(\n",
    "            log_probs,\n",
    "            target_distributions,\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Training Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs+1), train_losses, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('KL Divergence Loss', fontsize=12)\n",
    "plt.title('Training Loss über Epochen', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Loss-Reduktion: {train_losses[0]:.4f} → {train_losses[-1]:.4f}\")\n",
    "print(f\"   Verbesserung: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 8: Evaluation - Teacher vs Student\n",
    "\n",
    "Vergleichen wir die Vorhersagen des Teacher-Modells mit denen des trainierten Student-Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation-Modus\n",
    "student_model.eval()\n",
    "teacher_model.eval()\n",
    "\n",
    "# Vorhersagen sammeln\n",
    "all_inputs = []\n",
    "teacher_probs_list = []\n",
    "student_probs_list = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        x, y = dataset[i]\n",
    "        x = x.unsqueeze(0).to(device)  # Batch dimension\n",
    "        \n",
    "        # Teacher predictions\n",
    "        teacher_logits = teacher_model(x)\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=-1)\n",
    "        \n",
    "        # Student predictions\n",
    "        student_logits = student_model(x)\n",
    "        student_probs = F.softmax(student_logits, dim=-1)\n",
    "        \n",
    "        all_inputs.append(x.cpu())\n",
    "        teacher_probs_list.append(teacher_probs.cpu())\n",
    "        student_probs_list.append(student_probs.cpu())\n",
    "        true_labels.append(y)\n",
    "\n",
    "# Zu Tensoren konvertieren\n",
    "teacher_probs_all = torch.cat(teacher_probs_list, dim=0)\n",
    "student_probs_all = torch.cat(student_probs_list, dim=0)\n",
    "true_labels_all = torch.tensor(true_labels)\n",
    "\n",
    "print(\" Evaluation abgeschlossen\")\n",
    "print(f\"\\n Evaluiert auf {len(dataset)} Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy berechnen\n",
    "teacher_predictions = torch.argmax(teacher_probs_all, dim=-1)\n",
    "student_predictions = torch.argmax(student_probs_all, dim=-1)\n",
    "\n",
    "teacher_accuracy = (teacher_predictions == true_labels_all).float().mean().item()\n",
    "student_accuracy = (student_predictions == true_labels_all).float().mean().item()\n",
    "\n",
    "print(\" Accuracy:\")\n",
    "print(f\"  Teacher: {teacher_accuracy*100:.2f}%\")\n",
    "print(f\"  Student: {student_accuracy*100:.2f}%\")\n",
    "print(f\"\\n  Differenz: {abs(teacher_accuracy - student_accuracy)*100:.2f} Prozentpunkte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Divergence zwischen Teacher und Student berechnen\n",
    "kl_div = F.kl_div(\n",
    "    F.log_softmax(student_probs_all, dim=-1),\n",
    "    teacher_probs_all,\n",
    "    reduction='batchmean'\n",
    ").item()\n",
    "\n",
    "print(f\" Durchschnittliche KL Divergence zwischen Teacher und Student: {kl_div:.4f}\")\n",
    "print(f\"\\n Interpretation:\")\n",
    "print(f\"   Niedriger Wert ({kl_div:.4f}) bedeutet, dass der Student die\")\n",
    "print(f\"   Teacher-Verteilungen gut gelernt hat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Teacher vs Student für einige Samples\n",
    "n_samples_to_show = 6\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_samples_to_show):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    teacher_dist = teacher_probs_all[i].numpy()\n",
    "    student_dist = student_probs_all[i].numpy()\n",
    "    true_label = true_labels_all[i].item()\n",
    "    \n",
    "    x = np.arange(len(teacher_dist))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, teacher_dist, width, label='Teacher', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, student_dist, width, label='Student', alpha=0.8)\n",
    "    \n",
    "    # Markiere true label\n",
    "    ax.axvline(x=true_label, color='red', linestyle='--', linewidth=2, label='True Label')\n",
    "    \n",
    "    ax.set_title(f'Sample {i} (True Label: {true_label})', fontweight='bold')\n",
    "    ax.set_xlabel('Klasse')\n",
    "    ax.set_ylabel('Wahrscheinlichkeit')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Vergleich: Teacher vs Student Predictions', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Die Balken zeigen, wie ähnlich die Student-Predictions den Teacher-Predictions sind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 9: Coverage-Metrik berechnen\n",
    "\n",
    "Coverage misst, wie gut die Student-Verteilungen die Teacher-Verteilungen \"abdecken\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coverage(pred_probs: torch.Tensor, target_probs: torch.Tensor, epsilon: float = 0.15) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet epsilon-Credal Coverage.\n",
    "    \n",
    "    Eine Vorhersage \"deckt\" das Target ab, wenn die L1-Distanz <= epsilon ist.\n",
    "    \"\"\"\n",
    "    l1_distance = torch.sum(torch.abs(pred_probs - target_probs), dim=-1)\n",
    "    covered = (l1_distance <= epsilon).float()\n",
    "    return covered.mean().item()\n",
    "\n",
    "# Coverage für verschiedene Epsilon-Werte\n",
    "epsilons = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "coverages = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    cov = compute_coverage(student_probs_all, teacher_probs_all, epsilon=eps)\n",
    "    coverages.append(cov)\n",
    "    print(f\"Coverage bei ε = {eps:.2f}: {cov*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung: Coverage vs Epsilon\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, [c*100 for c in coverages], marker='o', linewidth=2, markersize=10)\n",
    "plt.xlabel('Epsilon (ε)', fontsize=12)\n",
    "plt.ylabel('Coverage (%)', fontsize=12)\n",
    "plt.title('Coverage in Abhängigkeit von Epsilon', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 105])\n",
    "\n",
    "# Markiere optimalen Punkt\n",
    "optimal_idx = len(coverages) // 2\n",
    "plt.axvline(x=epsilons[optimal_idx], color='red', linestyle='--', alpha=0.5, label='Beispiel ε')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Interpretation:\")\n",
    "print(f\"   Je höher das Epsilon, desto mehr Vorhersagen werden als 'covered' gezählt.\")\n",
    "print(f\"   Ein gutes Modell hat hohe Coverage bei kleinem Epsilon!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 10: Erweiterte Analysen\n",
    "\n",
    "Schauen wir uns an, bei welchen Samples der Student am besten und am schlechtesten abschneidet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-Distanzen für alle Samples berechnen\n",
    "l1_distances = torch.sum(torch.abs(student_probs_all - teacher_probs_all), dim=-1)\n",
    "\n",
    "# Beste und schlechteste Samples finden\n",
    "best_indices = torch.argsort(l1_distances)[:5]  # 5 beste\n",
    "worst_indices = torch.argsort(l1_distances, descending=True)[:5]  # 5 schlechteste\n",
    "\n",
    "print(\" Top 5 Samples (kleinste L1-Distanz):\")\n",
    "for i, idx in enumerate(best_indices):\n",
    "    idx = idx.item()\n",
    "    dist = l1_distances[idx].item()\n",
    "    print(f\"  {i+1}. Sample {idx}: L1-Distanz = {dist:.4f}\")\n",
    "\n",
    "print(\"\\n  Bottom 5 Samples (größte L1-Distanz):\")\n",
    "for i, idx in enumerate(worst_indices):\n",
    "    idx = idx.item()\n",
    "    dist = l1_distances[idx].item()\n",
    "    print(f\"  {i+1}. Sample {idx}: L1-Distanz = {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramm der L1-Distanzen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(l1_distances.numpy(), bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('L1-Distanz', fontsize=12)\n",
    "plt.ylabel('Anzahl Samples', fontsize=12)\n",
    "plt.title('Verteilung der L1-Distanzen zwischen Teacher und Student', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=l1_distances.mean().item(), color='red', linestyle='--', linewidth=2, label=f'Mittelwert: {l1_distances.mean().item():.3f}')\n",
    "plt.axvline(x=l1_distances.median().item(), color='green', linestyle='--', linewidth=2, label=f'Median: {l1_distances.median().item():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Statistiken der L1-Distanzen:\")\n",
    "print(f\"   Mittelwert: {l1_distances.mean().item():.4f}\")\n",
    "print(f\"   Median: {l1_distances.median().item():.4f}\")\n",
    "print(f\"   Standardabweichung: {l1_distances.std().item():.4f}\")\n",
    "print(f\"   Min: {l1_distances.min().item():.4f}\")\n",
    "print(f\"   Max: {l1_distances.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Teil 11: Zusammenfassung und Best Practices\n",
    "\n",
    "### Was haben wir gelernt?\n",
    "\n",
    "1. **First-Order Daten** sind Approximationen der bedingten Verteilung $p(Y|X)$\n",
    "2. Der **FirstOrderDataGenerator** macht es einfach, diese zu generieren\n",
    "3. Verteilungen können **gespeichert und geladen** werden (JSON-Format)\n",
    "4. **FirstOrderDataset** kombiniert Daten mit Verteilungen\n",
    "5. **Knowledge Distillation** nutzt First-Order Daten als Soft Targets\n",
    "6. **Coverage** ist eine wichtige Metrik für Unsicherheitsquantifizierung\n",
    "\n",
    "### Best Practices\n",
    "\n",
    " **DO:**\n",
    "- Modell immer in `eval()` Modus setzen vor Generierung\n",
    "- Metadaten beim Speichern hinzufügen\n",
    "- `shuffle=False` beim Generieren verwenden\n",
    "- Verteilungen regelmäßig verifizieren (Summe = 1.0)\n",
    "\n",
    " **DON'T:**\n",
    "- Modell nicht im Training-Modus lassen\n",
    "- Index-Alignment nicht ignorieren\n",
    "- Nicht ohne Metadaten speichern\n",
    "- Gerätekonsistenz nicht vergessen\n",
    "\n",
    "### Nächste Schritte\n",
    "\n",
    "1. Probieren Sie es mit Ihren eigenen Modellen und Datensätzen\n",
    "2. Experimentieren Sie mit verschiedenen `output_mode` Einstellungen\n",
    "3. Implementieren Sie benutzerdefinierte `input_getter` Funktionen\n",
    "4. Erkunden Sie erweiterte Anwendungsfälle (z.B. Credal Sets)\n",
    "5. Vergleichen Sie verschiedene Teacher-Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Übungsaufgaben\n",
    "\n",
    "Versuchen Sie folgende Erweiterungen:\n",
    "\n",
    "1. **Experiment 1**: Ändern Sie die Teacher-Architektur und beobachten Sie die Auswirkungen auf Coverage\n",
    "2. **Experiment 2**: Verwenden Sie verschiedene Temperaturen beim Softmax (`F.softmax(logits/T, dim=-1)`)\n",
    "3. **Experiment 3**: Implementieren Sie einen Ensemble-Ansatz mit mehreren Teacher-Modellen\n",
    "4. **Experiment 4**: Visualisieren Sie die Konfidenz-Kalibrierung\n",
    "5. **Experiment 5**: Testen Sie mit einem echten Datensatz (z.B. MNIST oder CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Weitere Ressourcen\n",
    "\n",
    "- **Dokumentation**: `docs/data_generation_guide.md`\n",
    "- **API-Referenz**: `docs/api_reference.md`\n",
    "- **Beispiel-Skript**: `examples/simple_usage.py`\n",
    "- **Tests**: `tests/test_first_order_generator.py`\n",
    "\n",
    "### Literatur\n",
    "\n",
    "- Hinton et al. (2015): \"Distilling the Knowledge in a Neural Network\"\n",
    "- Guo et al. (2017): \"On Calibration of Modern Neural Networks\"\n",
    "- Lakshminarayanan et al. (2017): \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\"\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial erstellt von**: ProblyPros Team  \n",
    "**Version**: 1.0  \n",
    "**Letzte Aktualisierung**: Dezember 2024\n",
    "\n",
    "Bei Fragen oder Feedback: Erstellen Sie ein Issue im Repository! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}