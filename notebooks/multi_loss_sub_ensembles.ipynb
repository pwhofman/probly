{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e47f87",
   "metadata": {},
   "source": [
    "# Multi-Loss Sub-Ensemble Example\n",
    "\n",
    "In this notebook we explore **Sub-Ensemble** methods for uncertainty estimation, focusing on\n",
    "a **Multi-Loss Sub-Ensemble** architecture inspired by:\n",
    "\n",
    "> \"Multi-Loss Sub-Ensembles for Accurate Classification with Uncertainty Estimation\"\n",
    "> (Achrack et al., 2020, arXiv:2010.01917)\n",
    "\n",
    "Instead of training many independent models as a full ensemble, we:\n",
    "\n",
    "- use a **shared backbone** (trunk) to extract features, and\n",
    "- attach several **classifier heads** (sub-ensembles) on top of the trunk.\n",
    "\n",
    "Each head is trained with a **different loss function**, which introduces diversity between\n",
    "the heads while keeping training in a single phase and with a single backbone.\n",
    "\n",
    "The goals of this notebook are:\n",
    "\n",
    "- to explain the idea of Sub-Ensembles and how Multi-Loss Sub-Ensembles extend them,\n",
    "- to implement a simple shared-trunk + multi-head architecture on CIFAR-10 in PyTorch,\n",
    "- to train different heads with different losses,\n",
    "- and to show how to obtain **accuracy** and **uncertainty** estimates from the disagreement\n",
    "  between heads.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "1. Imports & Setup  \n",
    "2. Full Ensembles vs. Sub-Ensembles  \n",
    "3. Multi-Loss Sub-Ensembles  \n",
    "4. Data Preparation  \n",
    "5. Model Definition (shared trunk + multiple heads)  \n",
    "6. Multi-Loss Sub-Ensemble Training Function  \n",
    "7. Short-run Experiment on CIFAR-10  \n",
    "8. Evaluation: Predictions & Uncertainty  \n",
    "9. Conclusion and connection to `probly`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41308c47",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup\n",
    "\n",
    "We first import the required libraries:\n",
    "\n",
    "- **torch, torchvision**: core tools for neural networks and datasets,\n",
    "- **CIFAR-10**: small image classification dataset,\n",
    "- standard utilities for data loading and device selection.\n",
    "\n",
    "Later, the same ideas can be integrated into the `probly` package and wrapped by a\n",
    "SubEnsemble utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6b4b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b641975",
   "metadata": {},
   "source": [
    "## 2. Full Ensembles vs. Sub-Ensembles\n",
    "\n",
    "### 2.1 Full ensembles\n",
    "\n",
    "A **full ensemble** consists of several independent models:\n",
    "\n",
    "- each model has its own parameters and random initialisation,\n",
    "- all models are trained separately on the same (or slightly resampled) dataset,\n",
    "- at test time, their predictions are averaged (or combined in some other way).\n",
    "\n",
    "Formally, for an input $x$ and $M$ models $f_m$, we obtain predictions\n",
    "$\\hat{y}_m = f_m(x)$ and aggregate them as\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\text{ens}}(x) = \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_m.\n",
    "$$\n",
    "\n",
    "Full ensembles are strong baselines for both **accuracy** and **uncertainty**, but they are:\n",
    "\n",
    "- computationally expensive (M separate models),\n",
    "- memory-intensive (M copies of all weights),\n",
    "- and sometimes hard to deploy in practice.\n",
    "\n",
    "### 2.2 Sub-Ensembles\n",
    "\n",
    "**Sub-Ensembles** aim to reduce this cost by sharing a large part of the model:\n",
    "\n",
    "- a single **trunk** processes the input and produces a feature representation,\n",
    "- multiple **heads** (sub-ensembles) take the same features and output predictions,\n",
    "- at test time, the heads are treated as ensemble members and their outputs are aggregated.\n",
    "\n",
    "This keeps many of the benefits of ensembles:\n",
    "\n",
    "- diversity between heads,\n",
    "- the ability to measure disagreement,\n",
    "- and simple averaging for predictions,\n",
    "\n",
    "while requiring only **one backbone** to be stored and operated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4a86d",
   "metadata": {},
   "source": [
    "## 3. Multi-Loss Sub-Ensembles\n",
    "\n",
    "A **Multi-Loss Sub-Ensemble** keeps the shared trunk + multi-head structure, but introduces\n",
    "an additional source of diversity:\n",
    "\n",
    "> each head is trained with a **different loss function**.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- a standard Sub-Ensemble uses the same training objective for all heads,\n",
    "- a Multi-Loss Sub-Ensemble lets each head see the same data through a different loss.\n",
    "\n",
    "In this notebook we combine:\n",
    "\n",
    "- a head trained with standard cross-entropy,\n",
    "- a head trained with label-smoothed cross-entropy,\n",
    "- a head trained with a simple margin-based loss that enforces a gap between the correct\n",
    "  class and the most likely wrong class.\n",
    "\n",
    "Even though all heads share the same backbone, the different losses encourage:\n",
    "\n",
    "- slightly different decision boundaries,\n",
    "- different confidence behaviours,\n",
    "- and therefore more informative disagreement patterns between heads.\n",
    "\n",
    "We will now implement this setting with a small CNN trunk and three heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e388d",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "We use CIFAR-10 as a small image classification benchmark:\n",
    "\n",
    "- 50,000 training images and 10,000 test images,\n",
    "- 10 classes,\n",
    "- RGB images of size 32x32.\n",
    "\n",
    "We apply:\n",
    "\n",
    "- conversion to tensors,\n",
    "- simple normalisation of pixel values,\n",
    "- `DataLoader` wrappers for batching and shuffling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb71521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:22<00:00, 7.48MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091f120",
   "metadata": {},
   "source": [
    "## 5. Model Definition: shared trunk + multiple heads\n",
    "\n",
    "We now define a small CNN with:\n",
    "\n",
    "- one **shared trunk** that extracts features from the input image, and\n",
    "- several **classifier heads** (sub-ensembles), each producing logits for the 10 classes.\n",
    "\n",
    "During training, each head will be paired with a different loss function.\n",
    "At inference time, all heads are treated as ensemble members and their\n",
    "predictions are aggregated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de693df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubEnsembleNet(\n",
      "  (trunk): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SubEnsembleNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, num_heads: int = 3) -> None:\n",
    "        \"\"\"Initialize the shared-trunk multi-head classifier.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # shared trunk\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        trunk_out_dim = 64 * 8 * 8\n",
    "\n",
    "        # multiple classifier heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [nn.Linear(trunk_out_dim, num_classes) for _ in range(num_heads)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
    "        \"\"\"Compute logits for each head.\"\"\"\n",
    "        feat = self.trunk(x)\n",
    "        logits_per_head = [head(feat) for head in self.heads]\n",
    "        return logits_per_head\n",
    "\n",
    "\n",
    "num_heads = 3\n",
    "model = SubEnsembleNet(num_classes=10, num_heads=num_heads).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d784e05",
   "metadata": {},
   "source": [
    "## 6. Multi-Loss Sub-Ensemble Training Function\n",
    "\n",
    "We now define:\n",
    "\n",
    "1. a small set of loss functions:\n",
    "   - standard cross-entropy,\n",
    "   - label-smoothed cross-entropy,\n",
    "   - a margin-based loss that enforces a gap between the correct and the most likely\n",
    "     incorrect logit;\n",
    "\n",
    "2. a unified training routine:\n",
    "\n",
    "   `train_multi_loss_sub_ensemble(model, dataloader, losses, ...)`\n",
    "\n",
    "The training function:\n",
    "\n",
    "- moves the model to the chosen device,\n",
    "- iterates over epochs and batches,\n",
    "- computes logits for each head,\n",
    "- applies a different loss to each head,\n",
    "- sums the per-head losses into a single scalar,\n",
    "- performs backpropagation and optimisation.\n",
    "\n",
    "This mimics how a future SubEnsemble training utility in `probly` could look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f08d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Small implementation of label-smoothed cross-entropy.\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing: float = 0.1) -> None:\n",
    "        \"\"\"Initialize the loss with a smoothing factor.\"\"\"\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the smoothed cross-entropy loss.\"\"\"\n",
    "        num_classes = logits.size(-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        loss = torch.sum(-true_dist * log_probs, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MarginLoss(nn.Module):\n",
    "    \"\"\"Encourage margin between correct logit and max wrong logit.\"\"\"\n",
    "\n",
    "    def __init__(self, margin: float = 0.5) -> None:\n",
    "        \"\"\"Initialize the margin loss with a target margin.\"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the margin-based loss.\"\"\"\n",
    "        num_classes = logits.size(-1)\n",
    "        one_hot = F.one_hot(target, num_classes=num_classes).bool()\n",
    "\n",
    "        correct_logit = logits[one_hot].view(-1)\n",
    "        wrong_logits = logits.masked_fill(one_hot, float(\"-inf\"))\n",
    "        max_wrong_logit, _ = wrong_logits.max(dim=-1)\n",
    "\n",
    "        margin_term = correct_logit - max_wrong_logit\n",
    "        loss = F.relu(self.margin - margin_term)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21dacc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_loss_sub_ensemble(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    losses: list[nn.Module],\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "    \"\"\"Demonstration of a multi-loss sub-ensemble training function.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if len(losses) != model.num_heads:\n",
    "        msg = f\"Need one loss per head, got {len(losses)} losses for {model.num_heads} heads.\"\n",
    "        raise ValueError(msg)\n",
    "    losses = [ls.to(device) for ls in losses]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            inputs = batch_inputs.to(device)\n",
    "            targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_per_head = model(inputs)\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            for head_logits, loss_fn in zip(logits_per_head, losses, strict=False):\n",
    "                batch_loss = batch_loss + loss_fn(head_logits, targets)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, num_batches)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5e76",
   "metadata": {},
   "source": [
    "## 7. Short-run experiment on CIFAR-10\n",
    "\n",
    "To keep the runtime manageable, we perform a **short-run** experiment on CIFAR-10:\n",
    "\n",
    "- Dataset: CIFAR-10 (50k train, 10k test)\n",
    "- Model: small CNN trunk with 3 classifier heads\n",
    "- Heads / losses:\n",
    "  - Head 1: standard cross-entropy\n",
    "  - Head 2: label-smoothed cross-entropy\n",
    "  - Head 3: margin-based loss\n",
    "- Training: a few epochs with Adam\n",
    "- Evaluation: accuracy and simple uncertainty metrics on the validation set\n",
    "\n",
    "This is not meant to be a state-of-the-art benchmark.  \n",
    "The goal is to clearly illustrate how a Multi-Loss Sub-Ensemble can be trained and how\n",
    "head disagreement can be turned into uncertainty estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "093413ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Loss: 3.6047\n",
      "Epoch [2/2] - Loss: 2.8859\n"
     ]
    }
   ],
   "source": [
    "num_heads = 3\n",
    "base_model = SubEnsembleNet(num_classes=10, num_heads=num_heads)\n",
    "\n",
    "loss_list: list[nn.Module] = [\n",
    "    nn.CrossEntropyLoss(),\n",
    "    LabelSmoothingCrossEntropy(smoothing=0.1),\n",
    "    MarginLoss(margin=0.5),\n",
    "]\n",
    "\n",
    "train_multi_loss_sub_ensemble(\n",
    "    model=base_model,\n",
    "    dataloader=train_loader,\n",
    "    losses=loss_list,\n",
    "    epochs=2,  # keep small for a quick demo\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4817573",
   "metadata": {},
   "source": [
    "## 8. Evaluation: accuracy and uncertainty\n",
    "\n",
    "After training, we treat each head as an ensemble member:\n",
    "\n",
    "1. For each input, we collect the softmax probabilities from all heads.\n",
    "2. We compute the **mean prediction** by averaging probabilities over heads.\n",
    "3. From this, we derive:\n",
    "\n",
    "   - the **ensemble accuracy** (argmax of the mean prediction),\n",
    "   - the **predictive entropy** of the mean prediction,\n",
    "   - the **variance** of the predicted probability of the chosen class across heads.\n",
    "\n",
    "The entropy and variance are simple but useful uncertainty indicators:\n",
    "\n",
    "- high entropy / variance → heads disagree or are unsure,  \n",
    "- low entropy / variance → heads agree and are confident.\n",
    "\n",
    "In practice, misclassified samples tend to have higher uncertainty scores than correctly\n",
    "classified ones, which can be exploited for tasks such as rejection, out-of-distribution\n",
    "detection, or risk-aware decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6888ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_uncertainty(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_mean_probs = []\n",
    "    all_targets = []\n",
    "    all_entropies = []\n",
    "    all_var_maxclass = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            inputs = batch_inputs.to(device)\n",
    "            targets = batch_targets.to(device)\n",
    "\n",
    "            logits_per_head = model(inputs)\n",
    "            probs_per_head = [F.softmax(logits, dim=-1) for logits in logits_per_head]\n",
    "\n",
    "            # stack along head axis: (heads, batch, classes)\n",
    "            probs_stack = torch.stack(probs_per_head, dim=0)\n",
    "\n",
    "            mean_probs = probs_stack.mean(dim=0)  # (batch, classes)\n",
    "            all_mean_probs.append(mean_probs.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "\n",
    "            # predictive entropy of mean prediction\n",
    "            entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-8), dim=-1)\n",
    "\n",
    "            # variance of probability of the predicted class across heads\n",
    "            preds = mean_probs.argmax(dim=-1)  # (batch,)\n",
    "            head_probs_max = []\n",
    "            for probs in probs_per_head:\n",
    "                head_probs_max.append(\n",
    "                    probs[torch.arange(probs.size(0), device=probs.device), preds],\n",
    "                )\n",
    "            head_probs_max = torch.stack(head_probs_max, dim=0)  # (heads, batch)\n",
    "            var_max = head_probs_max.var(dim=0)\n",
    "\n",
    "            all_entropies.append(entropy.cpu())\n",
    "            all_var_maxclass.append(var_max.cpu())\n",
    "\n",
    "    mean_probs = torch.cat(all_mean_probs, dim=0)\n",
    "    targets = torch.cat(all_targets, dim=0)\n",
    "    entropies = torch.cat(all_entropies, dim=0)\n",
    "    var_maxclass = torch.cat(all_var_maxclass, dim=0)\n",
    "\n",
    "    preds = mean_probs.argmax(dim=-1)\n",
    "    acc = (preds == targets).float().mean().item()\n",
    "\n",
    "    print(f\"Validation accuracy (ensemble): {acc:.4f}\")\n",
    "    print(f\"Entropy: mean={entropies.mean():.4f}, std={entropies.std():.4f}\")\n",
    "    print(f\"Var(max-class prob): mean={var_maxclass.mean():.4f}, std={var_maxclass.std():.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mean_probs\": mean_probs,\n",
    "        \"targets\": targets,\n",
    "        \"entropy\": entropies,\n",
    "        \"var_maxclass\": var_maxclass,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbee59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (ensemble): 0.6505\n",
      "Entropy: mean=1.6538, std=0.3284\n",
      "Var(max-class prob): mean=0.0445, std=0.0332\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_with_uncertainty(base_model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fb5f0",
   "metadata": {},
   "source": [
    "## 9. Conclusion and connection to `probly`\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- reviewed the idea of full ensembles and their computational cost,\n",
    "- introduced Sub-Ensembles with a shared backbone and multiple heads,\n",
    "- extended this to Multi-Loss Sub-Ensembles, where each head uses a different loss,\n",
    "- implemented a simple CNN-based Multi-Loss Sub-Ensemble on CIFAR-10,\n",
    "- and demonstrated how to extract accuracy and basic uncertainty metrics from the ensemble.\n",
    "\n",
    "From the perspective of `probly`, Multi-Loss Sub-Ensembles are a natural example of\n",
    "an **extended ensemble structure**:\n",
    "\n",
    "- the ensemble axis is realised by multiple heads,\n",
    "- parameters are partially shared (backbone) and partially head-specific,\n",
    "- each head can be associated with its own loss and possibly its own training settings.\n",
    "\n",
    "Future work in `probly` could expose this pattern via a dedicated SubEnsemble\n",
    "transformation, similar to existing ensemble utilities, so that:\n",
    "\n",
    "- users can quickly wrap an existing backbone into a Multi-Loss Sub-Ensemble,\n",
    "- and obtain uncertainty estimates without writing the full multi-head training loop\n",
    "  themselves.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
