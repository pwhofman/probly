{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34133126",
   "metadata": {},
   "source": [
    "# `probly` Tutorial — Evidential Regression Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620df8ba",
   "metadata": {},
   "source": [
    "This notebook is a practical introduction to the **Evidential Regression transformation** in `probly`. This technique allows a model to predict not just a single value, but a full probability distribution, enabling it to quantify both its confidence in the data (**aleatoric uncertainty**) and its own knowledge (**epistemic uncertainty**).\n",
    "\n",
    "We will start by explaining the core idea behind evidential regression and then see how `probly`'s transformation automates the process of building such a model by replacing the final layer. We will then train this model on a simple 1D dataset and visualize its predictive uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907b7f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# # Part A — Introduction to Evidential Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ad035",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Evidential Regression?\n",
    "### 1.1 The Problem: Standard Regression Predicts a Point\n",
    "\n",
    "A standard regression network is trained to predict a single value. For a given input, it might predict `y = 3.14`. This gives us no information about the model's confidence. Is the prediction `3.14 ± 0.01` or `3.14 ± 10.0`? We have no way of knowing.\n",
    "\n",
    "\n",
    "### 1.2 The Evidential Approach: Predicting a Distribution\n",
    "Evidential Regression reframes the problem. Instead of predicting a single point, the model predicts the four parameters of a **Normal-Inverse-Gamma (NIG)** distribution. These four parameters are: `gamma` (γ), `nu` (ν), `alpha` (α), and `beta` (β).\n",
    "\n",
    "Together, these parameters define a distribution over our prediction. From them, we can directly calculate:\n",
    "-   **The Prediction:** The mean of the distribution (given by `gamma`).\n",
    "-   **Aleatoric Uncertainty (Data Noise):** The inherent noise or ambiguity in the data itself. A high value means the data points are widely scattered.\n",
    "-   **Epistemic Uncertainty (Model Ignorance):** The model's own uncertainty about its predictions. A high value means the model is \"out of its depth,\" perhaps because it's seeing data far from what it was trained on.\n",
    "\n",
    "### 1.3 The Evidential Regression Transformation (probly)\n",
    "The `probly` transformation makes it easy to create an evidential regression model.\n",
    "-   You design your network as usual.\n",
    "-   The `evidential_regression` transformation traverses your model *backwards* and **replaces the final `nn.Linear` layer** with a special `NormalInverseGammaLinear` layer.\n",
    "-   This new final layer is responsible for outputting the four `(γ, ν, α, β)` parameters instead of a single value.\n",
    "\n",
    "The uncertainty can then be calculated from these parameters in a **single forward pass**.\n",
    "\n",
    "### 1.4 Short side‑by‑side comparison\n",
    "\n",
    "| Aspect | Evidential Regression | Standard (Point) Regression |\n",
    "| :--- | --- | --- |\n",
    "| **Model Output** | Four parameters: `(γ, ν, α, β)` | A single predicted value. |\n",
    "| **Final Layer** | `NormalInverseGammaLinear` | `nn.Linear` |\n",
    "| **Uncertainty Source** | Calculated directly from the four output parameters. | None. |\n",
    "| **Inference Cost** | **One single forward pass.** | One single forward pass. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff1d4f",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "Below: build a small MLP and apply `evidential_regression(model)` to see how the *last* linear layer is replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0152dc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "With Evidential transformation:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): NormalInverseGammaLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import evidential_regression\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 1) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "# Apply the Evidential Regression transformation\n",
    "model_evidential = evidential_regression(model)\n",
    "print(f\"\\nWith Evidential transformation:\\n\", model_evidential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9ddf6",
   "metadata": {},
   "source": [
    "### Notes on the structure\n",
    "-   Notice that the transformation has replaced **only the final `nn.Linear` layer** with a `NormalInverseGammaLinear` layer.\n",
    "-   The output of this new model will be a dictionary containing the four parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9c959",
   "metadata": {},
   "source": [
    "## 3. Part A Summary\n",
    "In Part A, we introduced Evidential Regression as a method for a model to predict its own uncertainty. Instead of a single point, the model learns to output the four parameters of a Normal-Inverse-Gamma distribution (`γ, ν, α, β`). We learned that the `probly` transformation automates this by replacing the final linear layer of a network. The key advantage is that both data uncertainty (aleatoric) and model uncertainty (epistemic) can be calculated from these parameters in a single, deterministic forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427617f",
   "metadata": {},
   "source": [
    " ---\n",
    " \n",
    "# # Part B — Applied Evidential Regression\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb107539",
   "metadata": {},
   "source": [
    "In this part, we will train an evidential regression model on a simple 1D dataset and visualize its predicted mean and uncertainty bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from probly.transformation import evidential_regression\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e7440",
   "metadata": {},
   "source": [
    "## 1. Setup and Custom Loss Function\n",
    "To train an evidential regression model, we can't use a standard loss like MSE. We need a loss function that encourages the model to produce a distribution that correctly fits the data. The standard loss is the Negative Log-Likelihood (NLL) of the Normal-Inverse-Gamma distribution.\n",
    "The function below looks complex, but its job is simple: it calculates how well the predicted distribution explains the true `y` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df07657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nig_nll_loss(y_true, gamma, nu, alpha, beta, reduce=True):\n",
    "    \"\"\"The Negative Log-Likelihood loss for the Normal-Inverse-Gamma distribution.\"\"\"\n",
    "    # The two fundamental moments of the distribution\n",
    "    mu = gamma\n",
    "    var = beta / (alpha - 1)\n",
    "    \n",
    "    # The precision term for the Normal distribution\n",
    "    lambda_ = 2 * alpha * (1 + nu) / (nu * (2 * beta))\n",
    "    \n",
    "    log_likelihood = 0.5 * torch.log(lambda_ / (2 * math.pi)) \\\n",
    "                     - 0.5 * lambda_ * (y_true - mu)**2 \\\n",
    "                     - alpha * torch.log(beta) \\\n",
    "                     + torch.lgamma(alpha) \\\n",
    "                     - 0.5 * torch.log(1 + nu)\n",
    "    \n",
    "    loss = -log_likelihood\n",
    "    if reduce:\n",
    "        return torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def build_tiny_mlp(in_dim: int = 1, hidden: int = 64) -> nn.Sequential:\n",
    "    \"\"\"A simple MLP for our 1D regression task.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2047cb",
   "metadata": {},
   "source": [
    "## 2. Create a Synthetic Dataset\n",
    " We'll create a simple 1D dataset with some noise. We will intentionally leave a gap in the training data to see how the model's uncertainty behaves in a region where it has no knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a89d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = torch.linspace(-4, -1, 100).unsqueeze(1)\n",
    "X_train2 = torch.linspace(1, 4, 100).unsqueeze(1)\n",
    "X_train = torch.cat([X_train1, X_train2], dim=0)\n",
    "y_train = X_train.pow(3) + 3 * torch.randn(X_train.shape)\n",
    "X_test = torch.linspace(-6, 6, 200).unsqueeze(1)\n",
    "y_test = X_test.pow(3) + 3 * torch.randn(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa061b4",
   "metadata": {},
   "source": [
    "## 3. Apply Transformation and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the base model\n",
    "base_model = build_tiny_mlp()\n",
    "\n",
    "# 2. Apply the evidential regression transformation\n",
    "evidential_model = evidential_regression(base_model)\n",
    "\n",
    "# 3. Train the model\n",
    "optimizer = torch.optim.Adam(evidential_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # The model now outputs a dictionary of the four parameters\n",
    "    params = evidential_model(X_train)\n",
    "    gamma, nu, alpha, beta = params['gamma'], params['nu'], params['alpha'], params['beta']\n",
    "    \n",
    "    loss = nig_nll_loss(y_train, gamma, nu, alpha, beta)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{1000}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84656f11",
   "metadata": {},
   "source": [
    "## 4. Inference and Uncertainty Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    params = evidential_model(X_test)\n",
    "    gamma, nu, alpha, beta = params['gamma'], params['nu'], params['alpha'], params['beta']\n",
    "\n",
    "# The mean prediction is gamma\n",
    "mean_pred = gamma\n",
    "\n",
    "# Aleatoric (data) uncertainty\n",
    "aleatoric_var = beta / (alpha - 1)\n",
    "\n",
    "# Epistemic (model) uncertainty\n",
    "epistemic_var = beta / (nu * (alpha - 1))\n",
    "\n",
    "total_var = aleatoric_var + epistemic_var\n",
    "total_std = torch.sqrt(total_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac6dba",
   "metadata": {},
   "source": [
    "## 5. Visualization //TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05c223",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "## Final Summary — Evidential Regression Tutorial\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988b2ec",
   "metadata": {},
   "source": [
    "This tutorial demonstrated how to use the **Evidential Regression Transformation** in `probly` to create models that can predict their own uncertainty.\n",
    "We learned that instead of a single point, an evidential model outputs the four parameters of a distribution (`γ, ν, α, β`). The `probly` transformation automates this by replacing the final linear layer of a network. The key advantage is that both **aleatoric (data) uncertainty** and **epistemic (model) uncertainty** can be calculated from these parameters in a single forward pass.\n",
    " We saw this in practice by training a model on a dataset with a gap. The final visualization clearly showed the model's uncertainty increasing in the regions where it had no training data, making it a powerful and interpretable tool for building more reliable regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
