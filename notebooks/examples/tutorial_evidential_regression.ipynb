{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cba0c0-5f9c-48c9-ade6-5ba1aa4aab99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Probly - Evidential Regression Tutorial\n",
    "\n",
    "The `probly.transformation.evidential.regression` module transforms standard regression models into uncertainty-aware ones.\n",
    "\n",
    "Simplified: In standard regression, a neural network predicts **a single number**, like a temperature, price, or position. But in real life, predictions are often uncertain. Sometimes the model *doesn't know* because of noise or missing data. \n",
    "\n",
    "This is where evidential regression comes into play. Instead of giving just one prediction like a normal regression model,  Evidential Regression can also tell us **how certain** the model is about its prediction.  \n",
    "This comes in handy when your data is noisy or incomplete.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1. Build a small neural network for a simple regression task  \n",
    "2. Turn it into an **Evidential Model** that predicts both a value *and* its uncertainty  \n",
    "3. Generate and visualize some test data  \n",
    "4. See how uncertainty looks in the model’s predictions  \n",
    "5. Try it yourself with a small exercise at the end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121797b-776f-470b-a93c-e676dff79204",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let’s import the libraries we need:\n",
    "- **torch** → for building neural networks  \n",
    "- **matplotlib** → for plotting  \n",
    "- **probly.evidential_regression** → to turn a normal model into an uncertainty-aware one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3abbc69-0928-4ef8-9a6c-eaebb50a15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation.evidential.regression.common import evidential_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f24d6-2154-43e6-aecb-0a19667d3b61",
   "metadata": {},
   "source": [
    "## 2. Build a simple regression model\n",
    "\n",
    "Here we make a small neural network that takes one input and predicts one output.  \n",
    "Right now, it’s just a normal regression model — no uncertainty yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c625de2-66c0-4cd2-b1c0-93d69e1e2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "\n",
    "print(\"Base model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a0ee2-7c87-4684-aa40-228df8ecfc89",
   "metadata": {},
   "source": [
    "## 3. Turn it into an Evidential Model\n",
    "\n",
    "With one line of code, Probly replaces the last layer with a special **Evidential layer** that predicts 4 values instead of 1.  \n",
    "These values represent both the prediction and its uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccd1b2-7ff2-4a91-a399-e7258640be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidential_model = evidential_regression(model)\n",
    "\n",
    "print(\"\\nEvidential model:\")\n",
    "print(evidential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b73c8-d901-4c29-ae95-61144a37d49d",
   "metadata": {},
   "source": [
    "## 4. Synthetic Data\n",
    "\n",
    "We’ll generate a simple sine curve with a little noise so the model has something to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4443a-a160-49c7-aa36-de1b57948905",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3, 3, 100).unsqueeze(-1)\n",
    "y = torch.sin(x) + 0.1 * torch.randn_like(x)\n",
    "\n",
    "plt.scatter(x, y, color=\"gray\", alpha=0.6)\n",
    "plt.title(\"Example Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ec9f7-2bb9-42e4-aae8-1247fdb3f7d3",
   "metadata": {},
   "source": [
    "## 5. Run the model\n",
    "\n",
    "Let’s see what the model gives us when we pass the input `x`.  \n",
    "It should output multiple values, not just one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9a85b-e3c2-40b3-98c0-248fc5655b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = evidential_model(x)\n",
    "\n",
    "print(\"Model output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f81a60-fa23-41f9-a88b-209c5d091c3c",
   "metadata": {},
   "source": [
    "## 6. Check the uncertainty\n",
    "\n",
    "Some models return 4 outputs:  \n",
    "- `mu` (mean) → main prediction  \n",
    "- `v`, `alpha`, `beta` → uncertainty parameters\n",
    "\n",
    "We can use these to estimate how uncertain the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b9e55-6822-46a0-93c8-535154c6174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from probly.quantification.regression import (\n",
    "    expected_conditional_variance,\n",
    "    total_variance,\n",
    "    variance_conditional_expectation,\n",
    ")\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "def nig_predictive_variance(\n",
    "    mu: Tensor,  # noqa: ARG001\n",
    "    v: Tensor,\n",
    "    alpha: Tensor,\n",
    "    beta: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute predictive variance of Evidential Regression using a\n",
    "    Normal-Inverse-Gamma likelihood.\n",
    "    \"\"\"  # noqa: D205\n",
    "    return beta / (v * (alpha - 1).clamp(min=1e-3))\n",
    "\n",
    "\n",
    "def build_gaussian_samples(\n",
    "    mu: Tensor,\n",
    "    var: Tensor,\n",
    "    n_samples: int = 50,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Build Gaussian samples (mu_samples, var_samples).\n",
    "\n",
    "    - mu_samples: shape (n_instances, n_samples)\n",
    "    - var_samples: shape (n_instances, n_samples)\n",
    "    \"\"\"\n",
    "    n_instances: int = mu.shape[0]\n",
    "\n",
    "    # Sample from Normal(mu, sqrt(var))\n",
    "    eps: Tensor = torch.randn(n_instances, n_samples, device=mu.device)\n",
    "    mu_samples: Tensor = mu.unsqueeze(1) + eps * var.sqrt().unsqueeze(1)\n",
    "\n",
    "    # Variance stays the same for all samples\n",
    "    var_samples: Tensor = var.unsqueeze(1).repeat(1, n_samples)\n",
    "\n",
    "    return mu_samples, var_samples\n",
    "\n",
    "\n",
    "def gaussian_samples_to_numpy(\n",
    "    mu_samples: Tensor,\n",
    "    var_samples: Tensor,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert PyTorch (mu, var) samples to NumPy array with shape.\n",
    "\n",
    "        (n_instances, n_samples, 2)\n",
    "\n",
    "    with last dimension representing [mu, sigma^2].\n",
    "    \"\"\"\n",
    "    probs: Tensor = torch.stack([mu_samples, var_samples], dim=-1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "def plot_evidential_regression_uncertainty(\n",
    "    x: Tensor,\n",
    "    y: Tensor,\n",
    "    out: tuple[Tensor, Tensor, Tensor, Tensor],\n",
    "    n_samples: int = 50,\n",
    ") -> None:\n",
    "    \"\"\"Visualize evidential regression mean + uncertainty using Problys quantification APIs.\n",
    "\n",
    "    Args:\n",
    "        x: Input values used for plotting (n_instances, 1 or n_instances)\n",
    "        y: Target values (n_instances,)\n",
    "        out: Tuple (mu, v, alpha, beta) from evidential regression predictor\n",
    "        n_samples: Number of Gaussian samples to draw per instance\n",
    "    \"\"\"\n",
    "    mu, v, alpha, beta = out\n",
    "\n",
    "    # 1. Predictive variance from NIG\n",
    "    var: Tensor = nig_predictive_variance(mu, v, alpha, beta)\n",
    "\n",
    "    # 2. Build Gaussian samples\n",
    "    mu_s, var_s = build_gaussian_samples(mu, var, n_samples=n_samples)\n",
    "\n",
    "    # 3. Convert to numpy for Quantification API\n",
    "    probs_np: np.ndarray = gaussian_samples_to_numpy(mu_s, var_s)\n",
    "\n",
    "    # 4. Compute uncertainty measures\n",
    "    total_unc: np.ndarray = total_variance(probs_np)\n",
    "    aleatoric_unc: np.ndarray = expected_conditional_variance(probs_np)  # noqa: F841\n",
    "    epistemic_unc: np.ndarray = variance_conditional_expectation(probs_np)  # noqa: F841\n",
    "\n",
    "    # 5. Plot results\n",
    "    plt.plot(x, mu, color=\"blue\", label=\"Predicted Mean\")\n",
    "\n",
    "    # Total uncertainty → shading band\n",
    "    plt.fill_between(\n",
    "        x.squeeze(),\n",
    "        (mu - torch.from_numpy(total_unc)).squeeze(),\n",
    "        (mu + torch.from_numpy(total_unc)).squeeze(),\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "        label=\"Total Uncertainty\",\n",
    "    )\n",
    "\n",
    "    plt.scatter(x, y, color=\"gray\", alpha=0.5, label=\"Data\")\n",
    "    plt.title(\"Evidential Regression (Probly Quantification)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172a30a-d5f7-4d68-8676-250bffafa7f0",
   "metadata": {},
   "source": [
    "## 7. Exercise \n",
    "\n",
    "Build a small model, make it evidential, and confirm that the last layer was replaced automatically.\n",
    "\n",
    "### Task:\n",
    "1. Create a simple model with a few `Linear` and `ReLU` layers  \n",
    "2. Apply `evidential_regression()` to it  \n",
    "3. Print the model and check what the **last layer** became  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a6efc-516d-4234-a148-dbbd71a98b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: TD003\n",
    "\n",
    "# TODO(@todo): 1. Build a simple model (hint: use nn.Sequential)\n",
    "student_model = None  # your model here\n",
    "\n",
    "# TODO(@todo): 2. Apply evidential regression\n",
    "student_evidential = None  # call evidential_regression()\n",
    "\n",
    "# TODO(@todo): 3. Print to inspect the transformation\n",
    "print(\"Transformed model:\")\n",
    "print(student_evidential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba17f5-99ac-455d-bc14-744693bbbf51",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "You just learned how to:\n",
    "- Build a simple regression model in PyTorch  \n",
    "- Transform it into an uncertainty-aware version using **Probly**  \n",
    "- Visualize what “uncertainty” looks like  \n",
    "\n",
    "Evidential Regression lets a model say *“I’m not sure here”* —  \n",
    "which is extremely valuable in real-world applications like self-driving, healthcare or finance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
