{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Batch Ensemble Networks\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "A) Explain **Batch Ensembles**.\n",
    "\n",
    "B) Implement a **Ensemble MLP**.\n",
    "\n",
    "C) Implement a **BatchEnsemble MLP**.\n",
    "\n",
    "D) Train both networks on CIFAR10 and compare **accuracy and speed**.\n",
    "\n",
    "\n",
    "## 1. Introduction: What are Batch Ensembles?\n",
    "\n",
    "**Batch Ensembles** are a way to efficiently approximate an ensemble of neural networks. Traditional ensembles require training and storing multiple independent networks, which is memory and computation expensive.\n",
    "\n",
    "Key ideas:\n",
    "- Use **shared base weights** for all ensemble members.\n",
    "- Introduce **rank-1 multiplicative factors** for each member.\n",
    "- Much **faster and memory-efficient** than classic ensembles.\n",
    "\n",
    "Mathematically, for a linear layer with weight matrix W:\n",
    "\n",
    "$$y_i = (W \\circ (r_i s_i^T)) x + b$$\n",
    "\n",
    "Where $r_i, s_i$ are the rank-1 vectors for ensemble member $i$, and $\\circ$ denotes element-wise multiplication.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 2. Import CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 3. Classic MLP\n",
    "\n",
    "Creating a MLP to use as a base model with 2 Hidden Layers.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10) -> None:\n",
    "        \"\"\"Initialize the MLP model with two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the MLP model.\n",
    "\n",
    "        Before passing the input through the network, it flattens the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279beedf",
   "metadata": {},
   "source": [
    "While theres currently no training functionality implemented in *probly*, we define the training function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f584d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "    base_cls: nn.Module,\n",
    "    k: int,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> list[nn.Module]:\n",
    "    models = []\n",
    "    for _ in range(k):\n",
    "        print(f\"\\nTraining ensemble member {_ + 1}/{k}\")\n",
    "        m_k = base_cls().to(device)\n",
    "        opt = optim.Adam(m_k.parameters(), lr=lr)\n",
    "        lossfn = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            t0 = time.perf_counter()\n",
    "            m_k.train()\n",
    "            total_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                x = xb.to(device).float()\n",
    "                y = yb.to(device).long()\n",
    "                opt.zero_grad()\n",
    "                out = m_k(x)\n",
    "                loss = lossfn(out, y)\n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                opt.step()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            t1 = time.perf_counter()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {t1 - t0:.2f}s\")\n",
    "        models.append(m_k)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323312",
   "metadata": {},
   "source": [
    "## 4. Define Batch Ensemble Linear Layer and Batch Ensemble MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2eed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BatchEnsembleLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, ensemble_size: int) -> None:\n",
    "        \"\"\"Initialize a BatchEnsemble Linear layer.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            ensemble_size (int): Number of ensemble members.\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ensemble_size = ensemble_size\n",
    "\n",
    "        # Shared weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # Rank-one factors\n",
    "        self.r = nn.Parameter(torch.Tensor(ensemble_size, out_features))\n",
    "        self.s = nn.Parameter(torch.Tensor(ensemble_size, in_features))\n",
    "\n",
    "        # Init\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.r, 1.0, 0.01)\n",
    "        nn.init.normal_(self.s, 1.0, 0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the BatchEnsemble Linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, in_features] or [E, B, in_features],\n",
    "                              where B is the batch size and E is the ensemble size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [E, B, out_features].\n",
    "        \"\"\"\n",
    "        if x.dim() == 2:\n",
    "            # First layer: add ensemble dimension\n",
    "            x = x.unsqueeze(0).expand(self.ensemble_size, -1, -1)  # [E, B, in_features]\n",
    "        elif x.dim() == 3 and x.size(0) != self.ensemble_size:\n",
    "            msg = f\"Expected first dim={self.ensemble_size}, got {x.size(0)}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        x = x * self.s.unsqueeze(1)\n",
    "        y = torch.matmul(x, self.weight.t())\n",
    "        y = y * self.r.unsqueeze(1) + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class BatchEnsembleMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10, ensemble_size: int = 4) -> None:\n",
    "        \"\"\"Initialize the BatchEnsemble MLP model with three fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "            ensemble_size (int): Number of ensemble members. Default is 4.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.fc1 = BatchEnsembleLinear(in_dim, hidden, ensemble_size)\n",
    "        self.fc2 = BatchEnsembleLinear(hidden, hidden, ensemble_size)\n",
    "        self.fc3 = BatchEnsembleLinear(hidden, out_dim, ensemble_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the BatchEnsemble MLP.\n",
    "\n",
    "        This mimics the standard MLP forward pass but uses BatchEnsembleLinear layers.\n",
    "        Meaning having two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (ensemble_size, batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batchensemble(\n",
    "    base_cls: BatchEnsembleMLP,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    model = base_cls().to(device)\n",
    "    ensemble_size = base_cls().ensemble_size\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t0 = time.perf_counter()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            x = xb.to(device).float()\n",
    "            y = yb.to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: [E, B, out_dim]\n",
    "            out = model(x)\n",
    "\n",
    "            # Compute loss per ensemble member\n",
    "            # out: [E, B, out_dim], y: [B]\n",
    "            loss = 0.0\n",
    "            for e in range(ensemble_size):\n",
    "                loss += loss_fn(out[e], y)\n",
    "            loss = loss / ensemble_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {t1 - t0:.2f}s\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d68dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 3 * 32 * 32\n",
    "hidden = 128\n",
    "ensemble_size = 5\n",
    "epochs = 20\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363844fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ensemble member 1/5\n",
      "Epoch 1/20, Loss: 1.6474, Time: 16.91s\n",
      "Epoch 2/20, Loss: 1.4374, Time: 16.87s\n",
      "Epoch 3/20, Loss: 1.3343, Time: 17.16s\n",
      "Epoch 4/20, Loss: 1.2630, Time: 17.01s\n",
      "Epoch 5/20, Loss: 1.1958, Time: 16.78s\n",
      "Epoch 6/20, Loss: 1.1402, Time: 16.79s\n",
      "Epoch 7/20, Loss: 1.0923, Time: 16.97s\n",
      "Epoch 8/20, Loss: 1.0484, Time: 16.72s\n",
      "Epoch 9/20, Loss: 1.0058, Time: 16.89s\n",
      "Epoch 10/20, Loss: 0.9606, Time: 16.83s\n",
      "Epoch 11/20, Loss: 0.9246, Time: 16.62s\n",
      "Epoch 12/20, Loss: 0.8861, Time: 16.78s\n",
      "Epoch 13/20, Loss: 0.8514, Time: 16.47s\n",
      "Epoch 14/20, Loss: 0.8136, Time: 16.57s\n",
      "Epoch 15/20, Loss: 0.7816, Time: 17.85s\n",
      "Epoch 16/20, Loss: 0.7572, Time: 18.07s\n",
      "Epoch 17/20, Loss: 0.7251, Time: 17.66s\n",
      "Epoch 18/20, Loss: 0.6964, Time: 17.39s\n",
      "Epoch 19/20, Loss: 0.6715, Time: 17.22s\n",
      "Epoch 20/20, Loss: 0.6445, Time: 16.93s\n",
      "\n",
      "Training ensemble member 2/5\n",
      "Epoch 1/20, Loss: 1.6444, Time: 17.06s\n",
      "Epoch 2/20, Loss: 1.4395, Time: 16.78s\n",
      "Epoch 3/20, Loss: 1.3384, Time: 16.97s\n",
      "Epoch 4/20, Loss: 1.2563, Time: 17.06s\n",
      "Epoch 5/20, Loss: 1.1979, Time: 17.11s\n",
      "Epoch 6/20, Loss: 1.1410, Time: 16.83s\n",
      "Epoch 7/20, Loss: 1.0924, Time: 17.10s\n",
      "Epoch 8/20, Loss: 1.0434, Time: 16.83s\n",
      "Epoch 9/20, Loss: 0.9978, Time: 16.85s\n",
      "Epoch 10/20, Loss: 0.9578, Time: 16.94s\n",
      "Epoch 11/20, Loss: 0.9212, Time: 16.78s\n",
      "Epoch 12/20, Loss: 0.8857, Time: 18.02s\n",
      "Epoch 13/20, Loss: 0.8444, Time: 18.32s\n",
      "Epoch 14/20, Loss: 0.8166, Time: 18.43s\n",
      "Epoch 15/20, Loss: 0.7830, Time: 19.29s\n",
      "Epoch 16/20, Loss: 0.7517, Time: 20.33s\n",
      "Epoch 17/20, Loss: 0.7226, Time: 20.66s\n",
      "Epoch 18/20, Loss: 0.7013, Time: 20.63s\n",
      "Epoch 19/20, Loss: 0.6713, Time: 20.45s\n",
      "Epoch 20/20, Loss: 0.6462, Time: 20.39s\n",
      "\n",
      "Training ensemble member 3/5\n",
      "Epoch 1/20, Loss: 1.6488, Time: 20.21s\n",
      "Epoch 2/20, Loss: 1.4409, Time: 20.45s\n",
      "Epoch 3/20, Loss: 1.3406, Time: 20.45s\n",
      "Epoch 4/20, Loss: 1.2634, Time: 20.49s\n",
      "Epoch 5/20, Loss: 1.1978, Time: 20.37s\n",
      "Epoch 6/20, Loss: 1.1423, Time: 20.71s\n",
      "Epoch 7/20, Loss: 1.0926, Time: 20.63s\n",
      "Epoch 8/20, Loss: 1.0452, Time: 20.51s\n",
      "Epoch 9/20, Loss: 1.0024, Time: 20.51s\n",
      "Epoch 10/20, Loss: 0.9612, Time: 20.81s\n",
      "Epoch 11/20, Loss: 0.9185, Time: 20.51s\n",
      "Epoch 12/20, Loss: 0.8857, Time: 20.75s\n",
      "Epoch 13/20, Loss: 0.8493, Time: 20.55s\n",
      "Epoch 14/20, Loss: 0.8128, Time: 20.40s\n",
      "Epoch 15/20, Loss: 0.7836, Time: 20.66s\n",
      "Epoch 16/20, Loss: 0.7519, Time: 20.66s\n",
      "Epoch 17/20, Loss: 0.7247, Time: 20.72s\n",
      "Epoch 18/20, Loss: 0.7019, Time: 20.91s\n",
      "Epoch 19/20, Loss: 0.6699, Time: 20.49s\n",
      "Epoch 20/20, Loss: 0.6413, Time: 20.67s\n",
      "\n",
      "Training ensemble member 4/5\n",
      "Epoch 1/20, Loss: 1.6484, Time: 20.66s\n",
      "Epoch 2/20, Loss: 1.4376, Time: 21.08s\n",
      "Epoch 3/20, Loss: 1.3366, Time: 20.35s\n",
      "Epoch 4/20, Loss: 1.2649, Time: 20.44s\n",
      "Epoch 5/20, Loss: 1.2003, Time: 20.68s\n",
      "Epoch 6/20, Loss: 1.1498, Time: 20.82s\n",
      "Epoch 7/20, Loss: 1.1004, Time: 20.65s\n",
      "Epoch 8/20, Loss: 1.0520, Time: 20.25s\n",
      "Epoch 9/20, Loss: 1.0082, Time: 20.48s\n",
      "Epoch 10/20, Loss: 0.9718, Time: 20.80s\n",
      "Epoch 11/20, Loss: 0.9251, Time: 20.68s\n",
      "Epoch 12/20, Loss: 0.8900, Time: 20.57s\n",
      "Epoch 13/20, Loss: 0.8539, Time: 20.43s\n",
      "Epoch 14/20, Loss: 0.8251, Time: 20.87s\n",
      "Epoch 15/20, Loss: 0.7920, Time: 20.60s\n",
      "Epoch 16/20, Loss: 0.7596, Time: 20.68s\n",
      "Epoch 17/20, Loss: 0.7284, Time: 20.93s\n",
      "Epoch 18/20, Loss: 0.7006, Time: 20.28s\n",
      "Epoch 19/20, Loss: 0.6798, Time: 20.68s\n",
      "Epoch 20/20, Loss: 0.6505, Time: 20.26s\n",
      "\n",
      "Training ensemble member 5/5\n",
      "Epoch 1/20, Loss: 1.6529, Time: 20.66s\n",
      "Epoch 2/20, Loss: 1.4455, Time: 20.76s\n",
      "Epoch 3/20, Loss: 1.3439, Time: 20.29s\n",
      "Epoch 4/20, Loss: 1.2660, Time: 20.53s\n",
      "Epoch 5/20, Loss: 1.2025, Time: 20.54s\n",
      "Epoch 6/20, Loss: 1.1464, Time: 20.47s\n",
      "Epoch 7/20, Loss: 1.0972, Time: 20.53s\n",
      "Epoch 8/20, Loss: 1.0494, Time: 20.62s\n",
      "Epoch 9/20, Loss: 1.0059, Time: 20.70s\n",
      "Epoch 10/20, Loss: 0.9666, Time: 20.34s\n",
      "Epoch 11/20, Loss: 0.9249, Time: 20.56s\n",
      "Epoch 12/20, Loss: 0.8936, Time: 20.56s\n",
      "Epoch 13/20, Loss: 0.8561, Time: 20.08s\n",
      "Epoch 14/20, Loss: 0.8238, Time: 20.74s\n",
      "Epoch 15/20, Loss: 0.7904, Time: 20.35s\n",
      "Epoch 16/20, Loss: 0.7629, Time: 20.95s\n",
      "Epoch 17/20, Loss: 0.7313, Time: 20.47s\n",
      "Epoch 18/20, Loss: 0.7030, Time: 20.73s\n",
      "Epoch 19/20, Loss: 0.6846, Time: 20.30s\n",
      "Epoch 20/20, Loss: 0.6514, Time: 20.48s\n",
      "\n",
      "Trained classical ensemble of size 5 in 1937.69s\n"
     ]
    }
   ],
   "source": [
    "t0_e = time.perf_counter()\n",
    "trained_ensemble = train_ensemble(\n",
    "    base_cls=lambda: MLP(in_dim=in_dim, hidden=hidden, out_dim=10),\n",
    "    k=ensemble_size,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_e = time.perf_counter()\n",
    "print(f\"\\nTrained classical ensemble of size {ensemble_size} in {t1_e - t0_e:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.6518, Time: 24.79s\n",
      "Epoch 2/20, Loss: 1.4396, Time: 24.11s\n",
      "Epoch 3/20, Loss: 1.3360, Time: 23.98s\n",
      "Epoch 4/20, Loss: 1.2566, Time: 24.78s\n",
      "Epoch 5/20, Loss: 1.1930, Time: 24.54s\n",
      "Epoch 6/20, Loss: 1.1367, Time: 24.67s\n",
      "Epoch 7/20, Loss: 1.0855, Time: 25.23s\n",
      "Epoch 8/20, Loss: 1.0424, Time: 24.84s\n",
      "Epoch 9/20, Loss: 0.9997, Time: 24.95s\n",
      "Epoch 10/20, Loss: 0.9531, Time: 25.59s\n",
      "Epoch 11/20, Loss: 0.9157, Time: 24.41s\n",
      "Epoch 12/20, Loss: 0.8805, Time: 24.62s\n",
      "Epoch 13/20, Loss: 0.8466, Time: 25.08s\n",
      "Epoch 14/20, Loss: 0.8067, Time: 24.25s\n",
      "Epoch 15/20, Loss: 0.7815, Time: 24.44s\n",
      "Epoch 16/20, Loss: 0.7470, Time: 21.15s\n",
      "Epoch 17/20, Loss: 0.7205, Time: 20.41s\n",
      "Epoch 18/20, Loss: 0.6892, Time: 20.25s\n",
      "Epoch 19/20, Loss: 0.6633, Time: 20.03s\n",
      "Epoch 20/20, Loss: 0.6428, Time: 20.46s\n",
      "\n",
      "Trained BatchEnsemble model of size 5 in 472.59s\n"
     ]
    }
   ],
   "source": [
    "t0_be = time.perf_counter()\n",
    "trained_be_model = train_batchensemble(\n",
    "    base_cls=lambda: BatchEnsembleMLP(in_dim=in_dim, hidden=hidden, out_dim=10, ensemble_size=ensemble_size),\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_be = time.perf_counter()\n",
    "print(f\"\\nTrained BatchEnsemble model of size {ensemble_size} in {t1_be - t0_be:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, data_loader: torch.utils.data.DataLoader, device: str) -> None:\n",
    "        \"\"\"Initialize the Evaluator with a data loader and device.\n",
    "\n",
    "        Args:\n",
    "            data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
    "            device (str): Device to run the evaluation on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        self.member_predictions = []\n",
    "\n",
    "    def evaluate_batchensemble(self, model: BatchEnsembleMLP) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a BatchEnsemble model.\"\"\"\n",
    "        self._setup()\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                out = model(x)  # [E, B, out_dim]\n",
    "                preds = torch.argmax(out, dim=2)  # [E, B]\n",
    "\n",
    "                self.correct += (preds == y.unsqueeze(0)).sum().item()\n",
    "                self.total += y.size(0) * model.ensemble_size\n",
    "                self.member_predictions.append(preds.cpu())\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "\n",
    "        return accuracy, all_member_preds\n",
    "\n",
    "    def evaluate_classical_ensemble(self, models: list[MLP]) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a classical ensemble of models.\"\"\"\n",
    "        self._setup()\n",
    "        for m in models:\n",
    "            m.to(self.device)\n",
    "            m.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                batch_member_preds = []\n",
    "                for m in models:\n",
    "                    out = m(x)  # [B, out_dim]\n",
    "                    preds = torch.argmax(out, dim=1)  # [B]\n",
    "                    batch_member_preds.append(preds.cpu().unsqueeze(0))  # [1, B]\n",
    "\n",
    "                batch_member_preds = torch.cat(batch_member_preds, dim=0)  # [E, B]\n",
    "                self.correct += (batch_member_preds == y.unsqueeze(0).cpu()).sum().item()\n",
    "                self.total += y.size(0) * len(models)\n",
    "                self.member_predictions.append(batch_member_preds)\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "        return accuracy, all_member_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3590b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEnsemble Accuracy: 0.5162\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BatchEnsemble\n",
    "evaluator = Evaluator(val_loader, device)\n",
    "be_acc, be_member_preds = evaluator.evaluate_batchensemble(trained_be_model)\n",
    "print(f\"BatchEnsemble Accuracy: {be_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcebb7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Ensemble Accuracy: 0.5189\n"
     ]
    }
   ],
   "source": [
    "for m in trained_ensemble:\n",
    "    m.to(device)\n",
    "ce_acc, ce_member_preds = evaluator.evaluate_classical_ensemble(trained_ensemble)\n",
    "print(f\"Classical Ensemble Accuracy: {ce_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
