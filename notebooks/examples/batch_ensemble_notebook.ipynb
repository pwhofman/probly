{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Batch Ensemble Networks\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "A) Explain **Batch Ensembles**.\n",
    "\n",
    "B) Implement a **Ensemble MLP**.\n",
    "\n",
    "C) Implement a **BatchEnsemble MLP**.\n",
    "\n",
    "D) Train both networks on CIFAR10 and compare **accuracy and speed**.\n",
    "\n",
    "\n",
    "## 1. Introduction: What are Batch Ensembles?\n",
    "\n",
    "**Batch Ensembles** are a way to efficiently approximate an ensemble of neural networks. Traditional ensembles require training and storing multiple independent networks, which is memory and computation expensive.\n",
    "\n",
    "Key ideas:\n",
    "- Use **shared base weights** for all ensemble members.\n",
    "- Introduce **rank-1 multiplicative factors** for each member.\n",
    "- Much **faster and memory-efficient** than classic ensembles.\n",
    "\n",
    "Mathematically, for a linear layer with weight matrix W:\n",
    "\n",
    "$$y_i = (W \\circ (r_i s_i^T)) x + b$$\n",
    "\n",
    "Where $r_i, s_i$ are the rank-1 vectors for ensemble member $i$, and $\\circ$ denotes element-wise multiplication.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 2. Import CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 3. Classic MLP\n",
    "\n",
    "Creating a MLP to use as a base model with 2 Hidden Layers.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10) -> None:\n",
    "        \"\"\"Initialize the MLP model with two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the MLP model.\n",
    "\n",
    "        Before passing the input through the network, it flattens the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279beedf",
   "metadata": {},
   "source": [
    "While theres currently no training functionality implemented in *probly*, we define the training function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f584d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "    base_cls: nn.Module,\n",
    "    k: int,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> list[nn.Module]:\n",
    "    models = []\n",
    "    for _ in range(k):\n",
    "        print(f\"\\nTraining ensemble member {_ + 1}/{k}\")\n",
    "        m_k = base_cls().to(device)\n",
    "        opt = optim.Adam(m_k.parameters(), lr=lr)\n",
    "        lossfn = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            t0 = time.perf_counter()\n",
    "            m_k.train()\n",
    "            total_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                x = xb.to(device).float()\n",
    "                y = yb.to(device).long()\n",
    "                opt.zero_grad()\n",
    "                out = m_k(x)\n",
    "                loss = lossfn(out, y)\n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                opt.step()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            t1 = time.perf_counter()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {t1 - t0:.2f}s\")\n",
    "        models.append(m_k)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323312",
   "metadata": {},
   "source": [
    "## 4. Define Batch Ensemble Linear Layer and Batch Ensemble MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a2eed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BatchEnsembleLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, ensemble_size: int) -> None:\n",
    "        \"\"\"Initialize a BatchEnsemble Linear layer.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            ensemble_size (int): Number of ensemble members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.ensemble_size = ensemble_size\n",
    "\n",
    "        # Shared weight and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # Rank-one factors\n",
    "        self.r = nn.Parameter(torch.Tensor(ensemble_size, out_features))\n",
    "        self.s = nn.Parameter(torch.Tensor(ensemble_size, in_features))\n",
    "\n",
    "        # Init\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.bias)\n",
    "        nn.init.normal_(self.r, 1.0, 0.01)\n",
    "        nn.init.normal_(self.s, 1.0, 0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the BatchEnsemble Linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, in_features] or [E, B, in_features],\n",
    "                              where B is the batch size and E is the ensemble size.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [E, B, out_features].\n",
    "        \"\"\"\n",
    "        if x.dim() == 2:\n",
    "            # First layer: add ensemble dimension\n",
    "            x = x.unsqueeze(0).expand(self.ensemble_size, -1, -1)  # [E, B, in_features]\n",
    "        elif x.dim() == 3 and x.size(0) != self.ensemble_size:\n",
    "            msg = f\"Expected first dim={self.ensemble_size}, got {x.size(0)}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        x = x * self.s.unsqueeze(1)\n",
    "        y = torch.matmul(x, self.weight.t())\n",
    "        y = y * self.r.unsqueeze(1) + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class BatchEnsembleMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10, ensemble_size: int = 4) -> None:\n",
    "        \"\"\"Initialize the BatchEnsemble MLP model with three fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "            ensemble_size (int): Number of ensemble members. Default is 4.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.fc1 = BatchEnsembleLinear(in_dim, hidden, ensemble_size)\n",
    "        self.fc2 = BatchEnsembleLinear(hidden, hidden, ensemble_size)\n",
    "        self.fc3 = BatchEnsembleLinear(hidden, out_dim, ensemble_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the BatchEnsemble MLP.\n",
    "\n",
    "        This mimics the standard MLP forward pass but uses BatchEnsembleLinear layers.\n",
    "        Meaning having two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (ensemble_size, batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c1ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batchensemble(\n",
    "    base_cls: BatchEnsembleMLP,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    model = base_cls().to(device)\n",
    "    ensemble_size = base_cls().ensemble_size\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t0 = time.perf_counter()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            x = xb.to(device).float()\n",
    "            y = yb.to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: [E, B, out_dim]\n",
    "            out = model(x)\n",
    "\n",
    "            # Compute loss per ensemble member\n",
    "            # out: [E, B, out_dim], y: [B]\n",
    "            loss = 0.0\n",
    "            for e in range(ensemble_size):\n",
    "                loss += loss_fn(out[e], y)\n",
    "            loss = loss / ensemble_size\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {t1 - t0:.2f}s\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d68dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 3 * 32 * 32\n",
    "hidden = 128\n",
    "ensemble_size = 5\n",
    "epochs = 20\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "363844fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ensemble member 1/5\n",
      "Epoch 1/20, Loss: 1.6492, Time: 41.37s\n",
      "Epoch 2/20, Loss: 1.4371, Time: 33.59s\n",
      "Epoch 3/20, Loss: 1.3390, Time: 33.17s\n",
      "Epoch 4/20, Loss: 1.2656, Time: 33.15s\n",
      "Epoch 5/20, Loss: 1.2022, Time: 33.42s\n",
      "Epoch 6/20, Loss: 1.1451, Time: 33.27s\n",
      "Epoch 7/20, Loss: 1.0908, Time: 33.25s\n",
      "Epoch 8/20, Loss: 1.0445, Time: 35.72s\n",
      "Epoch 9/20, Loss: 1.0010, Time: 35.03s\n",
      "Epoch 10/20, Loss: 0.9609, Time: 35.97s\n",
      "Epoch 11/20, Loss: 0.9262, Time: 35.80s\n",
      "Epoch 12/20, Loss: 0.8893, Time: 36.40s\n",
      "Epoch 13/20, Loss: 0.8508, Time: 36.47s\n",
      "Epoch 14/20, Loss: 0.8169, Time: 36.46s\n",
      "Epoch 15/20, Loss: 0.7888, Time: 35.95s\n",
      "Epoch 16/20, Loss: 0.7547, Time: 36.60s\n",
      "Epoch 17/20, Loss: 0.7266, Time: 36.02s\n",
      "Epoch 18/20, Loss: 0.7035, Time: 36.18s\n",
      "Epoch 19/20, Loss: 0.6710, Time: 35.96s\n",
      "Epoch 20/20, Loss: 0.6545, Time: 35.97s\n",
      "\n",
      "Training ensemble member 2/5\n",
      "Epoch 1/20, Loss: 1.6517, Time: 35.90s\n",
      "Epoch 2/20, Loss: 1.4401, Time: 36.27s\n",
      "Epoch 3/20, Loss: 1.3407, Time: 36.62s\n",
      "Epoch 4/20, Loss: 1.2668, Time: 34.68s\n",
      "Epoch 5/20, Loss: 1.2022, Time: 20.86s\n",
      "Epoch 6/20, Loss: 1.1452, Time: 17.84s\n",
      "Epoch 7/20, Loss: 1.0924, Time: 17.55s\n",
      "Epoch 8/20, Loss: 1.0452, Time: 17.89s\n",
      "Epoch 9/20, Loss: 1.0034, Time: 16.79s\n",
      "Epoch 10/20, Loss: 0.9694, Time: 16.70s\n",
      "Epoch 11/20, Loss: 0.9234, Time: 16.30s\n",
      "Epoch 12/20, Loss: 0.8873, Time: 16.33s\n",
      "Epoch 13/20, Loss: 0.8494, Time: 16.32s\n",
      "Epoch 14/20, Loss: 0.8138, Time: 16.40s\n",
      "Epoch 15/20, Loss: 0.7874, Time: 16.60s\n",
      "Epoch 16/20, Loss: 0.7531, Time: 16.48s\n",
      "Epoch 17/20, Loss: 0.7249, Time: 16.49s\n",
      "Epoch 18/20, Loss: 0.6943, Time: 16.37s\n",
      "Epoch 19/20, Loss: 0.6793, Time: 16.52s\n",
      "Epoch 20/20, Loss: 0.6431, Time: 16.28s\n",
      "\n",
      "Training ensemble member 3/5\n",
      "Epoch 1/20, Loss: 1.6514, Time: 16.30s\n",
      "Epoch 2/20, Loss: 1.4418, Time: 16.87s\n",
      "Epoch 3/20, Loss: 1.3378, Time: 17.67s\n",
      "Epoch 4/20, Loss: 1.2596, Time: 21.61s\n",
      "Epoch 5/20, Loss: 1.1930, Time: 26.07s\n",
      "Epoch 6/20, Loss: 1.1395, Time: 29.44s\n",
      "Epoch 7/20, Loss: 1.0918, Time: 27.66s\n",
      "Epoch 8/20, Loss: 1.0405, Time: 25.05s\n",
      "Epoch 9/20, Loss: 1.0030, Time: 20.89s\n",
      "Epoch 10/20, Loss: 0.9599, Time: 19.20s\n",
      "Epoch 11/20, Loss: 0.9235, Time: 19.04s\n",
      "Epoch 12/20, Loss: 0.8841, Time: 18.86s\n",
      "Epoch 13/20, Loss: 0.8470, Time: 19.37s\n",
      "Epoch 14/20, Loss: 0.8208, Time: 19.35s\n",
      "Epoch 15/20, Loss: 0.7868, Time: 18.77s\n",
      "Epoch 16/20, Loss: 0.7589, Time: 18.75s\n",
      "Epoch 17/20, Loss: 0.7270, Time: 18.67s\n",
      "Epoch 18/20, Loss: 0.6952, Time: 18.46s\n",
      "Epoch 19/20, Loss: 0.6822, Time: 18.36s\n",
      "Epoch 20/20, Loss: 0.6510, Time: 18.38s\n",
      "\n",
      "Training ensemble member 4/5\n",
      "Epoch 1/20, Loss: 1.6437, Time: 18.50s\n",
      "Epoch 2/20, Loss: 1.4369, Time: 18.48s\n",
      "Epoch 3/20, Loss: 1.3416, Time: 18.37s\n",
      "Epoch 4/20, Loss: 1.2670, Time: 18.41s\n",
      "Epoch 5/20, Loss: 1.1992, Time: 18.31s\n",
      "Epoch 6/20, Loss: 1.1443, Time: 18.55s\n",
      "Epoch 7/20, Loss: 1.0982, Time: 18.42s\n",
      "Epoch 8/20, Loss: 1.0488, Time: 18.38s\n",
      "Epoch 9/20, Loss: 1.0031, Time: 18.37s\n",
      "Epoch 10/20, Loss: 0.9638, Time: 18.52s\n",
      "Epoch 11/20, Loss: 0.9272, Time: 18.69s\n",
      "Epoch 12/20, Loss: 0.8902, Time: 18.73s\n",
      "Epoch 13/20, Loss: 0.8599, Time: 23.01s\n",
      "Epoch 14/20, Loss: 0.8261, Time: 19.32s\n",
      "Epoch 15/20, Loss: 0.7928, Time: 18.63s\n",
      "Epoch 16/20, Loss: 0.7644, Time: 18.99s\n",
      "Epoch 17/20, Loss: 0.7378, Time: 18.96s\n",
      "Epoch 18/20, Loss: 0.7109, Time: 18.45s\n",
      "Epoch 19/20, Loss: 0.6822, Time: 18.52s\n",
      "Epoch 20/20, Loss: 0.6556, Time: 18.46s\n",
      "\n",
      "Training ensemble member 5/5\n",
      "Epoch 1/20, Loss: 1.6460, Time: 18.74s\n",
      "Epoch 2/20, Loss: 1.4395, Time: 18.63s\n",
      "Epoch 3/20, Loss: 1.3376, Time: 18.71s\n",
      "Epoch 4/20, Loss: 1.2598, Time: 18.84s\n",
      "Epoch 5/20, Loss: 1.1932, Time: 18.82s\n",
      "Epoch 6/20, Loss: 1.1359, Time: 18.64s\n",
      "Epoch 7/20, Loss: 1.0904, Time: 18.95s\n",
      "Epoch 8/20, Loss: 1.0416, Time: 18.57s\n",
      "Epoch 9/20, Loss: 0.9957, Time: 18.55s\n",
      "Epoch 10/20, Loss: 0.9579, Time: 18.47s\n",
      "Epoch 11/20, Loss: 0.9231, Time: 18.62s\n",
      "Epoch 12/20, Loss: 0.8860, Time: 19.01s\n",
      "Epoch 13/20, Loss: 0.8509, Time: 18.89s\n",
      "Epoch 14/20, Loss: 0.8183, Time: 18.32s\n",
      "Epoch 15/20, Loss: 0.7870, Time: 18.46s\n",
      "Epoch 16/20, Loss: 0.7581, Time: 18.55s\n",
      "Epoch 17/20, Loss: 0.7329, Time: 18.48s\n",
      "Epoch 18/20, Loss: 0.7025, Time: 18.68s\n",
      "Epoch 19/20, Loss: 0.6826, Time: 18.49s\n",
      "Epoch 20/20, Loss: 0.6538, Time: 18.47s\n",
      "\n",
      "Trained classical ensemble of size 5 in 2282.77s\n"
     ]
    }
   ],
   "source": [
    "t0_e = time.perf_counter()\n",
    "trained_ensemble = train_ensemble(\n",
    "    base_cls=lambda: MLP(in_dim=in_dim, hidden=hidden, out_dim=10),\n",
    "    k=ensemble_size,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_e = time.perf_counter()\n",
    "print(f\"\\nTrained classical ensemble of size {ensemble_size} in {t1_e - t0_e:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b0c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.6542, Time: 22.28s\n",
      "Epoch 2/20, Loss: 1.4417, Time: 22.58s\n",
      "Epoch 3/20, Loss: 1.3403, Time: 22.56s\n",
      "Epoch 4/20, Loss: 1.2650, Time: 22.76s\n",
      "Epoch 5/20, Loss: 1.1972, Time: 22.67s\n",
      "Epoch 6/20, Loss: 1.1395, Time: 22.87s\n",
      "Epoch 7/20, Loss: 1.0898, Time: 22.55s\n",
      "Epoch 8/20, Loss: 1.0481, Time: 22.23s\n",
      "Epoch 9/20, Loss: 1.0029, Time: 22.52s\n",
      "Epoch 10/20, Loss: 0.9623, Time: 22.66s\n",
      "Epoch 11/20, Loss: 0.9255, Time: 22.63s\n",
      "Epoch 12/20, Loss: 0.8850, Time: 22.84s\n",
      "Epoch 13/20, Loss: 0.8553, Time: 22.77s\n",
      "Epoch 14/20, Loss: 0.8216, Time: 23.05s\n",
      "Epoch 15/20, Loss: 0.7891, Time: 22.46s\n",
      "Epoch 16/20, Loss: 0.7590, Time: 22.36s\n",
      "Epoch 17/20, Loss: 0.7283, Time: 24.52s\n",
      "Epoch 18/20, Loss: 0.7030, Time: 24.87s\n",
      "Epoch 19/20, Loss: 0.6788, Time: 22.97s\n",
      "Epoch 20/20, Loss: 0.6597, Time: 22.79s\n",
      "\n",
      "Trained BatchEnsemble model of size 5 in 457.00s\n"
     ]
    }
   ],
   "source": [
    "t0_be = time.perf_counter()\n",
    "trained_be_model = train_batchensemble(\n",
    "    base_cls=lambda: BatchEnsembleMLP(in_dim=in_dim, hidden=hidden, out_dim=10, ensemble_size=ensemble_size),\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_be = time.perf_counter()\n",
    "print(f\"\\nTrained BatchEnsemble model of size {ensemble_size} in {t1_be - t0_be:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a4259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, data_loader: torch.utils.data.DataLoader, device: str) -> None:\n",
    "        \"\"\"Initialize the Evaluator with a data loader and device.\n",
    "\n",
    "        Args:\n",
    "            data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
    "            device (str): Device to run the evaluation on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        self.member_predictions = []\n",
    "\n",
    "    def evaluate_batchensemble(self, model: BatchEnsembleMLP) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a BatchEnsemble model.\"\"\"\n",
    "        self._setup()\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                out = model(x)  # [E, B, out_dim]\n",
    "                preds = torch.argmax(out, dim=2)  # [E, B]\n",
    "\n",
    "                self.correct += (preds == y.unsqueeze(0)).sum().item()\n",
    "                self.total += y.size(0) * model.ensemble_size\n",
    "                self.member_predictions.append(preds.cpu())\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "\n",
    "        return accuracy, all_member_preds\n",
    "\n",
    "    def evaluate_classical_ensemble(self, models: list[MLP]) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a classical ensemble of models.\"\"\"\n",
    "        self._setup()\n",
    "        for m in models:\n",
    "            m.to(self.device)\n",
    "            m.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                batch_member_preds = []\n",
    "                for m in models:\n",
    "                    out = m(x)  # [B, out_dim]\n",
    "                    preds = torch.argmax(out, dim=1)  # [B]\n",
    "                    batch_member_preds.append(preds.cpu().unsqueeze(0))  # [1, B]\n",
    "\n",
    "                batch_member_preds = torch.cat(batch_member_preds, dim=0)  # [E, B]\n",
    "                self.correct += (batch_member_preds == y.unsqueeze(0).cpu()).sum().item()\n",
    "                self.total += y.size(0) * len(models)\n",
    "                self.member_predictions.append(batch_member_preds)\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "        return accuracy, all_member_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3590b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEnsemble Accuracy: 0.5126\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BatchEnsemble\n",
    "evaluator = Evaluator(val_loader, device)\n",
    "be_acc, be_member_preds = evaluator.evaluate_batchensemble(trained_be_model)\n",
    "print(f\"BatchEnsemble Accuracy: {be_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcebb7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Ensemble Accuracy: 0.5196\n"
     ]
    }
   ],
   "source": [
    "for m in trained_ensemble:\n",
    "    m.to(device)\n",
    "ce_acc, ce_member_preds = evaluator.evaluate_classical_ensemble(trained_ensemble)\n",
    "print(f\"Classical Ensemble Accuracy: {ce_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
