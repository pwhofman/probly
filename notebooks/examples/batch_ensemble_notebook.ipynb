{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# **Batch Ensemble Networks**\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "A) Explain the idea behind **Batch Ensembles**.\n",
    "\n",
    "B) Create a small Multi-Layer-Perceptron (MLP).\n",
    "\n",
    "D) Train the **MLP** as an Ensemble and BatchEnsemble network on CIFAR10.\n",
    "\n",
    "E) Compare **accuracy** and **speed**.\n",
    "\n",
    "\n",
    "## A) Introduction: What are Batch Ensembles?\n",
    "\n",
    "**Batch Ensembles** are a way to efficiently approximate an ensemble of neural networks. Traditional ensembles require training and storing multiple independent networks, which is memory and computation expensive.\n",
    "\n",
    "Key ideas:\n",
    "- Use **shared base weights (and biases)** for all ensemble members.\n",
    "- Introduce **rank-1 multiplicative factors** for each member.\n",
    "- Much **faster and memory-efficient** than classic ensembles.\n",
    "\n",
    "Mathematically the classic forward of\n",
    "\n",
    "$$y_i = W \\circ x + b$$\n",
    "\n",
    "transforms to\n",
    "\n",
    "$$y_i = (W \\circ (x \\circ s_i^T)) \\circ r_i+ b$$\n",
    "\n",
    "Where $r_i, s_i$ are the rank-1 vectors for ensemble member $i$, and $\\circ$ denotes element-wise multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90adbcb4d71591",
   "metadata": {},
   "source": [
    "## What does the transformation do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b21e1137c3a3f",
   "metadata": {},
   "source": [
    "### The parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751cb1c3f18f5ae",
   "metadata": {},
   "source": [
    "- num_members: The number of ensemble members to create.\n",
    "- s_mean: The mean used to initialize the input modulation factor s.\n",
    "- s_std: The standard deviation used to initialize the input modulation factor s.\n",
    "- r_mean: The mean used to initialize the output modulation factor r.\n",
    "- r_std: The standard deviation used to initialize the output modulation factor r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7948c61fbc4b11",
   "metadata": {},
   "source": [
    "### The layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed866cbba2906f03",
   "metadata": {},
   "source": [
    "With these parameters we can transform:\n",
    "- Linear layer into BatchEnsembleLinear layer\n",
    "- Conv2d layer into BatchEnsembleConv2d layer\n",
    "\n",
    "The transformation keeps the dimensions and base weights, while adding rank-1 factors:\n",
    "- **s:** scales input-dimension per member\n",
    "- **r:** scales output-dimension per member\n",
    "\n",
    "The base weights (weight) and bias (bias) are shared across all members, keeping memory usage minimal. The differences between members arise solely from their individual scaling factorys **s** and **r**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## B) Setup of the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f22fb",
   "metadata": {},
   "source": [
    "**Standard Imports and Pytorch Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "**Import CIFAR10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # add more transforms if desired\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "**The MLP Class**\n",
    "\n",
    "We create a MLP inherting basic functionality from the nn.Module parent class. The MLP has to hidden layers utilizing the ReLU activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10) -> None:\n",
    "        \"\"\"Initialize the MLP model with two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the MLP model.\n",
    "\n",
    "        Before passing the input through the network, it flattens the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032e410740d3",
   "metadata": {},
   "source": [
    "**The Ensemble MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb69f475fecb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.transformation.ensemble import ensemble\n",
    "\n",
    "in_dim = 3 * 32 * 32\n",
    "hidden = 128\n",
    "num_members = 5\n",
    "\n",
    "ensemble_mlp = ensemble(\n",
    "    base=MLP(in_dim=in_dim, hidden=hidden, out_dim=10),\n",
    "    num_members=num_members,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d012165452be4e",
   "metadata": {},
   "source": [
    "**The BatchEnsemble MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73462de610dd1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.transformation import batchensemble\n",
    "\n",
    "batch_ensemble_mlp = batchensemble(\n",
    "    base=MLP(in_dim=in_dim, hidden=hidden, out_dim=10),\n",
    "    num_members=num_members,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add6463574a760c",
   "metadata": {},
   "source": [
    "Let's compare the different models now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67721828123126c7",
   "metadata": {},
   "source": [
    "We start with a comparison of the base MLP and the BatchEnsemble MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a1f350a3341911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base MLP:\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "BatchEnsemble MLP:\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): BatchEnsembleLinear()\n",
      "    (1): ReLU()\n",
      "    (2): BatchEnsembleLinear()\n",
      "    (3): ReLU()\n",
      "    (4): BatchEnsembleLinear()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base MLP:\\n{MLP(in_dim=in_dim, hidden=hidden, out_dim=10)}\\n\")\n",
    "print(f\"BatchEnsemble MLP:\\n{batch_ensemble_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cb3da7c149d70",
   "metadata": {},
   "source": [
    "Then we compare the Ensemble MLP and the BatchEnsemble MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3362930c08d1b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble MLP:\n",
      "ModuleList(\n",
      "  (0-4): 5 x MLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "BatchEnsemble MLP:\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): BatchEnsembleLinear()\n",
      "    (1): ReLU()\n",
      "    (2): BatchEnsembleLinear()\n",
      "    (3): ReLU()\n",
      "    (4): BatchEnsembleLinear()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ensemble MLP:\\n{ensemble_mlp}\\n\")\n",
    "print(f\"BatchEnsemble MLP:\\n{batch_ensemble_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279beedf",
   "metadata": {},
   "source": [
    "## C) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51357d7",
   "metadata": {},
   "source": [
    "**Training Methods**\n",
    "\n",
    "While there is currently no training functionality implemented in *probly* we define the training methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcc5d1",
   "metadata": {},
   "source": [
    "**Base Training Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e577f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: nn.CrossEntropyLoss,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    num_members: int | None = None,\n",
    ") -> nn.Module:\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            x = xb.to(device).float()\n",
    "            y = yb.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "\n",
    "            if isinstance(num_members, int) and num_members > 0:\n",
    "                loss = 0.0\n",
    "                for e in range(num_members):\n",
    "                    loss += loss_function(out[e], y)\n",
    "                loss = loss / num_members\n",
    "            else:\n",
    "                # fallback to standard loss computation\n",
    "                loss = loss_function(out, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} trained in {t1 - t0} seconds.\")\n",
    "        print(f\"> Loss: {avg_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f584d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "    ensemble: MLP,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.ModuleList:\n",
    "    model = nn.ModuleList()\n",
    "\n",
    "    for i, member in enumerate(ensemble):\n",
    "        print(f\"\\nTraining ensemble member {i + 1}/{len(ensemble)}\")\n",
    "        member_i = member.to(device)\n",
    "        optimizer = optim.Adam(member_i.parameters(), lr=lr)\n",
    "        train_model(\n",
    "            member_i,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=nn.CrossEntropyLoss(),\n",
    "            train_loader=train_loader,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        model.append(member_i)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_batchensemble(\n",
    "    base_cls: MLP,\n",
    "    num_members: int,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    model = base_cls.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        train_loader=train_loader,\n",
    "        epochs=epochs,\n",
    "        num_members=num_members,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "\n",
    "t0_batch_ensemble = time.perf_counter()\n",
    "trained_batch_ensemble = train_batchensemble(\n",
    "    base_cls=batch_ensemble_mlp,\n",
    "    num_members=num_members,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_batch_ensemble = time.perf_counter()\n",
    "print(f\"\\nTrained BatchEnsemble model of size {num_members} in {t1_batch_ensemble - t0_batch_ensemble:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_ensemble = time.perf_counter()\n",
    "trained_ensemble = train_ensemble(\n",
    "    ensemble=ensemble_mlp,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_ensemble = time.perf_counter()\n",
    "print(f\"\\nTrained classical ensemble of size {num_members} in {t1_ensemble - t0_ensemble:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, data_loader: torch.utils.data.DataLoader, device: str) -> None:\n",
    "        \"\"\"Initialize the Evaluator with a data loader and device.\n",
    "\n",
    "        Args:\n",
    "            data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
    "            device (str): Device to run the evaluation on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        self.member_predictions = []\n",
    "\n",
    "    def evaluate_batchensemble(self, model: nn.Module, num_members: int) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a BatchEnsemble model.\"\"\"\n",
    "        self._setup()\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                out = model(x)  # [E, B, out_dim]\n",
    "                preds = torch.argmax(out, dim=2)  # [E, B]\n",
    "\n",
    "                self.correct += (preds == y.unsqueeze(0)).sum().item()\n",
    "                self.total += y.size(0) * num_members\n",
    "                self.member_predictions.append(preds.cpu())\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "\n",
    "        return accuracy, all_member_preds\n",
    "\n",
    "    def evaluate_classical_ensemble(self, models: nn.ModuleList) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a classical ensemble of models.\"\"\"\n",
    "        self._setup()\n",
    "        for m in models:\n",
    "            m.to(self.device)\n",
    "            m.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                batch_member_preds = []\n",
    "                for m in models:\n",
    "                    out = m(x)  # [B, out_dim]\n",
    "                    preds = torch.argmax(out, dim=1)  # [B]\n",
    "                    batch_member_preds.append(preds.cpu().unsqueeze(0))  # [1, B]\n",
    "\n",
    "                batch_member_preds = torch.cat(batch_member_preds, dim=0)  # [E, B]\n",
    "                self.correct += (batch_member_preds == y.unsqueeze(0).cpu()).sum().item()\n",
    "                self.total += y.size(0) * len(models)\n",
    "                self.member_predictions.append(batch_member_preds)\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "        return accuracy, all_member_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3590b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BatchEnsemble\n",
    "evaluator = Evaluator(val_loader, device)\n",
    "be_acc, be_member_preds = evaluator.evaluate_batchensemble(trained_batch_ensemble, num_members)\n",
    "print(f\"BatchEnsemble Accuracy: {be_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in trained_ensemble:\n",
    "    m.to(device)\n",
    "ce_acc, ce_member_preds = evaluator.evaluate_classical_ensemble(trained_ensemble)\n",
    "print(f\"Classical Ensemble Accuracy: {ce_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
