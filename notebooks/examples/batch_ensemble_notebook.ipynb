{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# **Batch Ensemble Networks**\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "A) Explain the idea behind **Batch Ensembles**.\n",
    "\n",
    "B) Create a small Multi-Layer-Perceptron (MLP).\n",
    "\n",
    "D) Train the **MLP** as an Ensemble and BatchEnsemble network on CIFAR10.\n",
    "\n",
    "E) Compare **accuracy** and **speed**.\n",
    "\n",
    "\n",
    "## A) Introduction: What are Batch Ensembles?\n",
    "\n",
    "**Batch Ensembles** are a way to efficiently approximate an ensemble of neural networks. Traditional ensembles require training and storing multiple independent networks, which is memory and computation expensive.\n",
    "\n",
    "Key ideas:\n",
    "- Use **shared base weights (and biases)** for all ensemble members.\n",
    "- Introduce **rank-1 multiplicative factors** for each member.\n",
    "- Much **faster and memory-efficient** than classic ensembles.\n",
    "\n",
    "Mathematically the classic forward of\n",
    "\n",
    "$$y_i = W \\circ x + b$$\n",
    "\n",
    "transforms to\n",
    "\n",
    "$$y_i = (W \\circ (x \\circ s_i^T)) \\circ r_i+ b$$\n",
    "\n",
    "Where $r_i, s_i$ are the rank-1 vectors for ensemble member $i$, and $\\circ$ denotes element-wise multiplication.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## B) Setup of the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f22fb",
   "metadata": {},
   "source": [
    "**Standard Imports and Pytorch Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from typing import Any\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "**Import CIFAR10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        # add more transforms if desired\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "**The MLP Class**\n",
    "\n",
    "We create a MLP inherting basic functionality from the nn.Module parent class. The MLP has to hidden layers utilizing the ReLU activation function.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 3072, hidden: int = 128, out_dim: int = 10) -> None:\n",
    "        \"\"\"Initialize the MLP model with two hidden layers.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Dimension of the input features. Default is 3072 (32x32x3 for CIFAR-10).\n",
    "            hidden (int): Number of neurons in the hidden layers. Default is 128.\n",
    "            out_dim (int): Dimension of the output features. Default is 10 (number of classes in CIFAR-10).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the MLP model.\n",
    "\n",
    "        Before passing the input through the network, it flattens the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_dim).\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279beedf",
   "metadata": {},
   "source": [
    "## C) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51357d7",
   "metadata": {},
   "source": [
    "**Training Methods**\n",
    "\n",
    "While there is currently no training functionality implemented in *probly* we define the training methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcc5d1",
   "metadata": {},
   "source": [
    "**Base Training Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e577f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "              optimizer: Any,\n",
    "              loss_function: Any,\n",
    "              train_loader: DataLoader,\n",
    "              epochs: int=10,\n",
    "              num_members : int = None,\n",
    ") -> list[nn.Module]:\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.perf_counter()\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            x = xb.to(device).float()\n",
    "            y = yb.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "\n",
    "            if isinstance(num_members,int) and num_members > 0:\n",
    "                loss = 0.0\n",
    "                for e in range(num_members):\n",
    "                    loss += loss_function(out[e], y)\n",
    "                loss = loss / num_members\n",
    "            else:\n",
    "                # fallback to standard loss computation\n",
    "                loss = loss_function(out, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} trained in {t1-t0} seconds.\")\n",
    "        print(f\"> Loss: {avg_loss}\")\n",
    "    return model        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f584d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "    ensemble : list[nn.Module],\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> list[nn.Module]:\n",
    "    models = []\n",
    "\n",
    "    for i,member in enumerate(ensemble):\n",
    "        print(f\"\\nTraining ensemble member {i + 1}/{len(ensemble)}\")\n",
    "        model_i = member.to(device)\n",
    "        optimizer = optim.Adam(model_i.parameters(), lr=lr)\n",
    "        trained_model_i = train_model(model_i, optimizer=optimizer,\n",
    "                                      loss_function=nn.CrossEntropyLoss(),\n",
    "                              train_loader=train_loader,epochs=epochs)\n",
    "        models.append(trained_model_i)\n",
    "    return models\n",
    "\n",
    "def train_batchensemble(\n",
    "    base_cls: MLP,\n",
    "    num_members: int,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    \n",
    "    model = base_cls.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model = train_model(model, optimizer=optimizer, \n",
    "                        loss_function=nn.CrossEntropyLoss(),\n",
    "                        train_loader=train_loader,epochs=epochs,num_members=num_members)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42083d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.transformation import batchensemble\n",
    "in_dim = 3 * 32 * 32\n",
    "hidden = 128\n",
    "num_members = 5\n",
    "epochs = 20\n",
    "lr = 1e-3\n",
    "ensemble = [MLP(in_dim=in_dim, hidden=hidden, out_dim=10) for _ in range(num_members)]\n",
    "batch_ensemble_mlp = batchensemble(MLP(in_dim=in_dim, hidden=hidden, out_dim=10),num_members=num_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2bd6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 trained in 40.554668200202286 seconds.\n",
      "> Loss: 1.8352828882324794\n",
      "Epoch 2/20 trained in 30.627204100135714 seconds.\n",
      "> Loss: 1.5650636937781472\n",
      "Epoch 3/20 trained in 29.23773320019245 seconds.\n",
      "> Loss: 1.4680251480674256\n",
      "Epoch 4/20 trained in 26.651254000142217 seconds.\n",
      "> Loss: 1.4031920902483446\n",
      "Epoch 5/20 trained in 26.01576830027625 seconds.\n",
      "> Loss: 1.354238022655077\n",
      "Epoch 6/20 trained in 26.190833599772304 seconds.\n",
      "> Loss: 1.3148494001694848\n",
      "Epoch 7/20 trained in 26.76800880022347 seconds.\n",
      "> Loss: 1.2799603526819539\n",
      "Epoch 8/20 trained in 29.592886600177735 seconds.\n",
      "> Loss: 1.2490024723384294\n",
      "Epoch 9/20 trained in 26.600823500193655 seconds.\n",
      "> Loss: 1.2213217937938692\n",
      "Epoch 10/20 trained in 30.731435799971223 seconds.\n",
      "> Loss: 1.197284526422248\n",
      "Epoch 11/20 trained in 28.348196799866855 seconds.\n",
      "> Loss: 1.1747562160723803\n",
      "Epoch 12/20 trained in 30.84166119992733 seconds.\n",
      "> Loss: 1.1535143873970706\n",
      "Epoch 13/20 trained in 28.81197979999706 seconds.\n",
      "> Loss: 1.134618471504707\n",
      "Epoch 14/20 trained in 27.534864000044763 seconds.\n",
      "> Loss: 1.1160684212308165\n",
      "Epoch 15/20 trained in 27.448727800045162 seconds.\n",
      "> Loss: 1.096784615432766\n",
      "Epoch 16/20 trained in 27.28740580007434 seconds.\n",
      "> Loss: 1.0812040967050456\n",
      "Epoch 17/20 trained in 27.30820220010355 seconds.\n",
      "> Loss: 1.0649115295648117\n",
      "Epoch 18/20 trained in 27.385240700095892 seconds.\n",
      "> Loss: 1.0494363949761052\n",
      "Epoch 19/20 trained in 27.684434399940073 seconds.\n",
      "> Loss: 1.0363758488564787\n",
      "Epoch 20/20 trained in 27.30964200012386 seconds.\n",
      "> Loss: 1.0196998739806948\n",
      "\n",
      "Trained BatchEnsemble model of size 5 in 572.95s\n"
     ]
    }
   ],
   "source": [
    "t0_batch_ensemble = time.perf_counter()\n",
    "trained_batch_ensemble = train_batchensemble(\n",
    "    base_cls=batch_ensemble_mlp,\n",
    "    num_members=num_members,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_batch_ensemble = time.perf_counter()\n",
    "print(f\"\\nTrained BatchEnsemble model of size {num_members} in {t1_batch_ensemble - t0_batch_ensemble:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363844fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ensemble member 1/5\n",
      "Epoch 1/20 trained in 20.00968790007755 seconds.\n",
      "> Loss: 1.6475496249403316\n",
      "Epoch 2/20 trained in 20.433384000323713 seconds.\n",
      "> Loss: 1.4513696497324102\n",
      "Epoch 3/20 trained in 19.71125359972939 seconds.\n",
      "> Loss: 1.3573608495299814\n",
      "Epoch 4/20 trained in 20.114626599941403 seconds.\n",
      "> Loss: 1.2816591315824712\n",
      "Epoch 5/20 trained in 20.998445800039917 seconds.\n",
      "> Loss: 1.2200282713730826\n",
      "Epoch 6/20 trained in 21.642976799979806 seconds.\n",
      "> Loss: 1.1680420410991554\n",
      "Epoch 7/20 trained in 21.16135359974578 seconds.\n",
      "> Loss: 1.1187594093272721\n",
      "Epoch 8/20 trained in 21.403368500061333 seconds.\n",
      "> Loss: 1.0765911825787768\n",
      "Epoch 9/20 trained in 20.987796700093895 seconds.\n",
      "> Loss: 1.0347559028760942\n",
      "Epoch 10/20 trained in 21.356161499861628 seconds.\n",
      "> Loss: 0.9983648232596087\n",
      "Epoch 11/20 trained in 21.00062010018155 seconds.\n",
      "> Loss: 0.9627541714536785\n",
      "Epoch 12/20 trained in 21.23691130010411 seconds.\n",
      "> Loss: 0.932914604857726\n",
      "Epoch 13/20 trained in 25.427497000433505 seconds.\n",
      "> Loss: 0.9046951045382885\n",
      "Epoch 14/20 trained in 23.622624200303108 seconds.\n",
      "> Loss: 0.8698483388430022\n",
      "Epoch 15/20 trained in 21.08180859964341 seconds.\n",
      "> Loss: 0.844978573893555\n",
      "Epoch 16/20 trained in 21.274892299901694 seconds.\n",
      "> Loss: 0.8158540088476008\n",
      "Epoch 17/20 trained in 21.989467200357467 seconds.\n",
      "> Loss: 0.7945375218081764\n",
      "Epoch 18/20 trained in 21.26761779980734 seconds.\n",
      "> Loss: 0.7732265684098215\n",
      "Epoch 19/20 trained in 21.133168700151145 seconds.\n",
      "> Loss: 0.7530815901660187\n",
      "Epoch 20/20 trained in 21.00267399987206 seconds.\n",
      "> Loss: 0.7256978874452894\n",
      "\n",
      "Training ensemble member 2/5\n",
      "Epoch 1/20 trained in 21.20358659978956 seconds.\n",
      "> Loss: 1.6527004843710023\n",
      "Epoch 2/20 trained in 21.00378099968657 seconds.\n",
      "> Loss: 1.4521372893721494\n",
      "Epoch 3/20 trained in 20.965327099896967 seconds.\n",
      "> Loss: 1.3604437650508479\n",
      "Epoch 4/20 trained in 20.884264799766243 seconds.\n",
      "> Loss: 1.2836019573727253\n",
      "Epoch 5/20 trained in 21.094905300065875 seconds.\n",
      "> Loss: 1.2213161809659507\n",
      "Epoch 6/20 trained in 21.850827200338244 seconds.\n",
      "> Loss: 1.167170792150711\n",
      "Epoch 7/20 trained in 21.034514599945396 seconds.\n",
      "> Loss: 1.1209270405372747\n",
      "Epoch 8/20 trained in 20.804478299804032 seconds.\n",
      "> Loss: 1.0770060151948886\n",
      "Epoch 9/20 trained in 20.990873999893665 seconds.\n",
      "> Loss: 1.0389332742509518\n",
      "Epoch 10/20 trained in 21.014742099680007 seconds.\n",
      "> Loss: 0.9968280974329853\n",
      "Epoch 11/20 trained in 20.89560279995203 seconds.\n",
      "> Loss: 0.9589608669547949\n",
      "Epoch 12/20 trained in 20.917805999983102 seconds.\n",
      "> Loss: 0.9239583756400467\n",
      "Epoch 13/20 trained in 20.823468999937177 seconds.\n",
      "> Loss: 0.8959511516380981\n",
      "Epoch 14/20 trained in 20.885117099620402 seconds.\n",
      "> Loss: 0.864392388747887\n",
      "Epoch 15/20 trained in 21.003882400225848 seconds.\n",
      "> Loss: 0.8329515278110577\n",
      "Epoch 16/20 trained in 21.111913400236517 seconds.\n",
      "> Loss: 0.8059728597496384\n",
      "Epoch 17/20 trained in 21.10517349978909 seconds.\n",
      "> Loss: 0.7811383805782919\n",
      "Epoch 18/20 trained in 21.007485500071198 seconds.\n",
      "> Loss: 0.7529854548526588\n",
      "Epoch 19/20 trained in 21.05255199968815 seconds.\n",
      "> Loss: 0.7343943012271718\n",
      "Epoch 20/20 trained in 20.95121440012008 seconds.\n",
      "> Loss: 0.7107185974040248\n",
      "\n",
      "Training ensemble member 3/5\n",
      "Epoch 1/20 trained in 20.854019300080836 seconds.\n",
      "> Loss: 1.6400036918605967\n",
      "Epoch 2/20 trained in 21.161461299750954 seconds.\n",
      "> Loss: 1.448656389138215\n",
      "Epoch 3/20 trained in 21.0814132001251 seconds.\n",
      "> Loss: 1.351315500640137\n",
      "Epoch 4/20 trained in 21.333040500059724 seconds.\n",
      "> Loss: 1.275436546050503\n",
      "Epoch 5/20 trained in 20.86926899990067 seconds.\n",
      "> Loss: 1.2170643166937434\n",
      "Epoch 6/20 trained in 21.075494800228626 seconds.\n",
      "> Loss: 1.1641537148984518\n",
      "Epoch 7/20 trained in 21.15520259970799 seconds.\n",
      "> Loss: 1.1163206732356998\n",
      "Epoch 8/20 trained in 20.76848899992183 seconds.\n",
      "> Loss: 1.0714475830167207\n",
      "Epoch 9/20 trained in 21.03401040006429 seconds.\n",
      "> Loss: 1.0324393553529423\n",
      "Epoch 10/20 trained in 21.00383900012821 seconds.\n",
      "> Loss: 0.9916979043588986\n",
      "Epoch 11/20 trained in 21.022067300044 seconds.\n",
      "> Loss: 0.9635979241273834\n",
      "Epoch 12/20 trained in 20.898787399753928 seconds.\n",
      "> Loss: 0.9267327205271425\n",
      "Epoch 13/20 trained in 20.95615560002625 seconds.\n",
      "> Loss: 0.8949061830075826\n",
      "Epoch 14/20 trained in 20.96766269998625 seconds.\n",
      "> Loss: 0.8621887406781172\n",
      "Epoch 15/20 trained in 21.00644990010187 seconds.\n",
      "> Loss: 0.8346798868989304\n",
      "Epoch 16/20 trained in 21.123290200252086 seconds.\n",
      "> Loss: 0.8104486750892852\n",
      "Epoch 17/20 trained in 20.90961889969185 seconds.\n",
      "> Loss: 0.7867543365012661\n",
      "Epoch 18/20 trained in 20.964201900176704 seconds.\n",
      "> Loss: 0.763178781428096\n",
      "Epoch 19/20 trained in 20.946166700217873 seconds.\n",
      "> Loss: 0.7401686364347478\n",
      "Epoch 20/20 trained in 20.92501419968903 seconds.\n",
      "> Loss: 0.7188013335374098\n",
      "\n",
      "Training ensemble member 4/5\n",
      "Epoch 1/20 trained in 22.39586649974808 seconds.\n",
      "> Loss: 1.64682386826035\n",
      "Epoch 2/20 trained in 22.84664630005136 seconds.\n",
      "> Loss: 1.4509556294250243\n",
      "Epoch 3/20 trained in 20.80600700015202 seconds.\n",
      "> Loss: 1.3567787164003515\n",
      "Epoch 4/20 trained in 21.021126500330865 seconds.\n",
      "> Loss: 1.2865671655419386\n",
      "Epoch 5/20 trained in 21.084840999916196 seconds.\n",
      "> Loss: 1.226602837777031\n",
      "Epoch 6/20 trained in 20.94313109992072 seconds.\n",
      "> Loss: 1.1709800925074825\n",
      "Epoch 7/20 trained in 20.801876300014555 seconds.\n",
      "> Loss: 1.1248161208530458\n",
      "Epoch 8/20 trained in 20.93894420005381 seconds.\n",
      "> Loss: 1.0804925278753938\n",
      "Epoch 9/20 trained in 20.82627149997279 seconds.\n",
      "> Loss: 1.0401135622425408\n",
      "Epoch 10/20 trained in 20.936324500013143 seconds.\n",
      "> Loss: 1.0022568278067094\n",
      "Epoch 11/20 trained in 20.847652200143784 seconds.\n",
      "> Loss: 0.967581113789681\n",
      "Epoch 12/20 trained in 20.628212600015104 seconds.\n",
      "> Loss: 0.9317538588381088\n",
      "Epoch 13/20 trained in 20.903357199858874 seconds.\n",
      "> Loss: 0.9020269368218063\n",
      "Epoch 14/20 trained in 20.825823100283742 seconds.\n",
      "> Loss: 0.8750270497737904\n",
      "Epoch 15/20 trained in 20.915150800254196 seconds.\n",
      "> Loss: 0.8471266440451336\n",
      "Epoch 16/20 trained in 21.222943699918687 seconds.\n",
      "> Loss: 0.8180296323426015\n",
      "Epoch 17/20 trained in 20.886415800079703 seconds.\n",
      "> Loss: 0.792336987003789\n",
      "Epoch 18/20 trained in 21.18237360008061 seconds.\n",
      "> Loss: 0.7674193502387707\n",
      "Epoch 19/20 trained in 20.825341699644923 seconds.\n",
      "> Loss: 0.7468195289316195\n",
      "Epoch 20/20 trained in 21.097963500302285 seconds.\n",
      "> Loss: 0.7284524961209649\n",
      "\n",
      "Training ensemble member 5/5\n",
      "Epoch 1/20 trained in 21.036780499853194 seconds.\n",
      "> Loss: 1.6437645725004961\n",
      "Epoch 2/20 trained in 20.857323300093412 seconds.\n",
      "> Loss: 1.445600970769188\n",
      "Epoch 3/20 trained in 20.943091600202024 seconds.\n",
      "> Loss: 1.3534862816676985\n",
      "Epoch 4/20 trained in 21.17177430028096 seconds.\n",
      "> Loss: 1.2808283063668284\n",
      "Epoch 5/20 trained in 20.85830010008067 seconds.\n",
      "> Loss: 1.2204749199418174\n",
      "Epoch 6/20 trained in 20.931758500169963 seconds.\n",
      "> Loss: 1.167788354883725\n",
      "Epoch 7/20 trained in 21.338086099829525 seconds.\n",
      "> Loss: 1.1201159355355164\n",
      "Epoch 8/20 trained in 20.857138400431722 seconds.\n",
      "> Loss: 1.0775726125054228\n",
      "Epoch 9/20 trained in 21.01375290006399 seconds.\n",
      "> Loss: 1.0345828403895738\n",
      "Epoch 10/20 trained in 21.01664840010926 seconds.\n",
      "> Loss: 0.9975392721588613\n",
      "Epoch 11/20 trained in 21.196625899989158 seconds.\n",
      "> Loss: 0.9599893695066468\n",
      "Epoch 12/20 trained in 20.939498300198466 seconds.\n",
      "> Loss: 0.9278894950774565\n",
      "Epoch 13/20 trained in 21.041721999645233 seconds.\n",
      "> Loss: 0.8933244392190007\n",
      "Epoch 14/20 trained in 20.902470599859953 seconds.\n",
      "> Loss: 0.8695545914229565\n",
      "Epoch 15/20 trained in 21.121822100132704 seconds.\n",
      "> Loss: 0.8403103543410908\n",
      "Epoch 16/20 trained in 21.125498099718243 seconds.\n",
      "> Loss: 0.8128893370629883\n",
      "Epoch 17/20 trained in 20.848514900077134 seconds.\n",
      "> Loss: 0.7876554594463976\n",
      "Epoch 18/20 trained in 20.928004800342023 seconds.\n",
      "> Loss: 0.7590155389090798\n",
      "Epoch 19/20 trained in 20.976285899989307 seconds.\n",
      "> Loss: 0.7422792157605147\n",
      "Epoch 20/20 trained in 20.890541000291705 seconds.\n",
      "> Loss: 0.7166683243221758\n",
      "\n",
      "Trained classical ensemble of size 5 in 2109.50s\n"
     ]
    }
   ],
   "source": [
    "t0_ensemble = time.perf_counter()\n",
    "trained_ensemble = train_ensemble(\n",
    "    ensemble,\n",
    "    train_loader=train_loader,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    ")\n",
    "t1_ensemble = time.perf_counter()\n",
    "print(f\"\\nTrained classical ensemble of size {num_members} in {t1_ensemble - t0_ensemble:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a4259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, data_loader: torch.utils.data.DataLoader, device: str) -> None:\n",
    "        \"\"\"Initialize the Evaluator with a data loader and device.\n",
    "\n",
    "        Args:\n",
    "            data_loader (torch.utils.data.DataLoader): DataLoader for evaluation data.\n",
    "            device (str): Device to run the evaluation on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        self.member_predictions = []\n",
    "\n",
    "    def evaluate_batchensemble(self, model: MLP, num_members) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a BatchEnsemble model.\"\"\"\n",
    "        self._setup()\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                out = model(x)  # [E, B, out_dim]\n",
    "                preds = torch.argmax(out, dim=2)  # [E, B]\n",
    "\n",
    "                self.correct += (preds == y.unsqueeze(0)).sum().item()\n",
    "                self.total += y.size(0) * num_members\n",
    "                self.member_predictions.append(preds.cpu())\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "\n",
    "        return accuracy, all_member_preds\n",
    "\n",
    "    def evaluate_classical_ensemble(self, models: list[MLP]) -> tuple[float, torch.Tensor]:\n",
    "        \"\"\"Evaluate a classical ensemble of models.\"\"\"\n",
    "        self._setup()\n",
    "        for m in models:\n",
    "            m.to(self.device)\n",
    "            m.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.data_loader:\n",
    "                x = xb.to(self.device).float()\n",
    "                y = yb.to(self.device).long()\n",
    "\n",
    "                batch_member_preds = []\n",
    "                for m in models:\n",
    "                    out = m(x)  # [B, out_dim]\n",
    "                    preds = torch.argmax(out, dim=1)  # [B]\n",
    "                    batch_member_preds.append(preds.cpu().unsqueeze(0))  # [1, B]\n",
    "\n",
    "                batch_member_preds = torch.cat(batch_member_preds, dim=0)  # [E, B]\n",
    "                self.correct += (batch_member_preds == y.unsqueeze(0).cpu()).sum().item()\n",
    "                self.total += y.size(0) * len(models)\n",
    "                self.member_predictions.append(batch_member_preds)\n",
    "\n",
    "        accuracy = self.correct / self.total\n",
    "        all_member_preds = torch.cat(self.member_predictions, dim=1)\n",
    "        return accuracy, all_member_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3590b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEnsemble Accuracy: 0.5110\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BatchEnsemble\n",
    "evaluator = Evaluator(val_loader, device)\n",
    "be_acc, be_member_preds = evaluator.evaluate_batchensemble(trained_batch_ensemble, num_members)\n",
    "print(f\"BatchEnsemble Accuracy: {be_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcebb7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Ensemble Accuracy: 0.5081\n"
     ]
    }
   ],
   "source": [
    "for m in trained_ensemble:\n",
    "    m.to(device)\n",
    "ce_acc, ce_member_preds = evaluator.evaluate_classical_ensemble(trained_ensemble)\n",
    "print(f\"Classical Ensemble Accuracy: {ce_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
