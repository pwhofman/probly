{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A tutorial for clustervisualiser.py\n",
    "\n",
    "This notebook shows a small demonstration for `clustervisualizer.py`, one of the five types we can currently visualize.\n",
    "It visualizes the magin for two 2D-Clusters with the Support-Vector-Machine (SVM) and three different kernels `linear`, `rbf` and `sigmoid`.\n",
    "We are creating an example cluster, plot several datapoints and then lay a heatmap over it.\n",
    "Overview:\n",
    "1. Imports\n",
    "2. A small demonstration\n",
    "3. But what is SVM?\n",
    "4. Code breakdown: What happens step by step?\n",
    "5. All about flags\n",
    "6. Further reading\n",
    "NOTE: Please load imports and demo first so that the notebook works properly."
   ],
   "id": "d1d8f7ab03d8ac6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Imports:\n",
    "At first, we need to import several packages."
   ],
   "id": "ec8a4fd1f3347ed4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "from probly.visualization.clustermargin.clustervisualizer import plot_uncertainty\n",
    "\n",
    "print(\"Imports successfully loaded.\")"
   ],
   "id": "deafcc97d4832b8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. A small demonstration:\n",
    "We generate two reproducible 2D clusters using a fixed random seed, which is set to `value = 42` in this example for consistent results.\n",
    "The cluster centers are controlled via `loc` and their spread and overlap via `scale`.\n",
    "\n",
    "We then call `plot_uncertainty()` to visualize the samples and a heatmap derived from an SVM decision boundary."
   ],
   "id": "34543b8002431444"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "value = 42\n",
    "rng = np.random.default_rng(value)\n",
    "\n",
    "n1, n2 = 120, 120\n",
    "cluster1 = rng.normal(loc=(0.0, 0.0), scale=0.7, size=(n1, 2))\n",
    "cluster2 = rng.normal(loc=(2.0, 1.5), scale=0.7, size=(n2, 2))\n",
    "\n",
    "plot_uncertainty(\n",
    "    cluster1,\n",
    "    cluster2,\n",
    "    title=\"Quick demo\",\n",
    "    kernel=\"rbf\",\n",
    "    C=0.5,\n",
    "    gamma=\"scale\",\n",
    "    show=True,\n",
    ")\n",
    "print(f\"Demo loaded with seed {value}.\")"
   ],
   "id": "148ee35cf8591b15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. But what is SVM?\n",
    "\n",
    "A Support Vector Machine (SVM) is a supervised classifier that separates classes by learning a **decision boundary**.\n",
    "In the linear case, this boundary is a straight line (in 2D); with nonlinear kernels (e.g. RBF), it can become curved.\n",
    "\n",
    "We use an SVM in this demo because it provides a natural notion of **distance to the boundary** via the *decision function*.\n",
    "Points close to the boundary are typically harder to classify, so we treat small margins as **high uncertainty** and large margins as **low uncertainty**.\n",
    "That distance-based signal is what we visualize as the margin heatmap."
   ],
   "id": "e15a1d61edfa6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Code breakdown: What happens step-by-step\n",
    "### 4.1 Overview\n",
    "\n",
    "This module visualizes margin between two 2D clusters using a Support Vector Machine (SVM).<br>\n",
    "The plot consists of:\n",
    "- the two labeled point clouds (scatter)\n",
    "- a background heatmap showing margin between those two scatters\n",
    "\n",
    "**Helper functions:**<br>\n",
    "\n",
    "`_check_shape(input_data: np.ndarray) -> np.ndarray`:<br>\n",
    "Purpose: Ensure input arrays have the expected format: a non-empty NumPy array with shape (n_samples, 2).\n",
    "\n",
    "`_2_cluster_to_y(cluster1, cluster2) -> np.ndarray`:<br>\n",
    "Purpose: Build the label vector y for SVM by validating both clusters with `_check_shape`\n",
    "\n",
    "`_2_cluster_to_x(cluster1, cluster2) -> np.ndarray`:<br>\n",
    "Purpose: Stack both clusters into a single feature matrix X ; validates both clusters and uses `np.vstack` to produce one array of shape (n1 + n2, 2)\n",
    "\n",
    "`_plot_svm_beam(ax, clf, X, cmap) -> None`:<br>\n",
    "Purpose: Compute and draw the margin heatmap based on the SVM.\n",
    "\n",
    "\n",
    "**Main function:**<br>\n",
    "\n",
    "`plot_uncertainty() -> plt.Axes`:<br>\n",
    "Visualizes how *uncertain* a Support Vector Machine (SVM) is when separating two **2D clusters**.<br>\n",
    "The output combines:\n",
    "- **Scatter points** for both clusters (the raw data)\n",
    "- A **background heatmap** that is brightest near the SVM decision boundary and fades away as points become easier to classify.<br>\n",
    "\n",
    "*What it does:*\n",
    "- Creation of axes (optional), makes the function usable both standalone and in multi-plot notebooks.\n",
    "- Build training data `X` and labels `y`, then combined into clustes with their corresponding lables.\n",
    "- Input an parameter validation before fitting: `X`, `gamma` and `C` are checked for right values and `len(X) == len(y)` checks if every sample has a label.\n",
    "- `labels` and `title` are annotated with the given (or default) parameters.\n",
    "- Each class is plotted with `ax.scatter()`.\n",
    "- Result is a decision function that can be evaluated anywhere in the 2D plane.\n",
    "- Heatmap is computed with `_plot_svm_beam()` and drawn.\n",
    "- If `show=True`, it calls `plt.show()` so that the plot is shown.\n",
    "\n",
    "**How to read the result:**<br>\n",
    "- Bright regions (high values) indicate **low margin**, points close to the decision boundary -> **high uncertainty**.\n",
    "- Darker regions indicate **high margin**, points far from the boundary -> **low uncertainty**.\n",
    "- Tuning `kernel`, `C`, and `gamma` changes the boundary shape and therefore the width/shape of the margin band."
   ],
   "id": "bee145ec552e8992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Difference between kernels\n",
    "\n",
    "A `kernel` defines the shape of the decision boundary by implicitly mapping the data into another feature space. <br>\n",
    "\n",
    "**kernel=\"linear\":** <br>\n",
    "Decision boundary is a **straight line** in 2D (a hyperplane in higher dimensions).\n",
    "Works best when classes are approximately linearly separable.<br>\n",
    "**Pros:** fast, simple, stable, fewer hyperparameters. <br>\n",
    "**Cons:** cannot model curved boundaries.<br>\n",
    "In the plot: expect a mostly straight separating region and margin band.\n",
    "\n",
    "**kernel=\"rbf\" (Radial Basis Function):**<br>\n",
    "Produces **nonlinear** boundaries.\n",
    "Great default when you expect complex shapes or overlapping clusters.\n",
    "Sensitive to `gamma` and `C`.<br>\n",
    "In the plot: can create curved separation with an margin band that adapts to the data geometry.\n",
    "\n",
    "**kernel=\"sigmoid\":**<br>\n",
    "Nonlinear kernel inspired by **neural networks**.\n",
    "Can work, but is often less reliable than `rbf` unless tuned carefully.\n",
    "Sensitive to scaling and `gamma`.<br>\n",
    "In the plot: can look similar to linear in some settings, or behave oddly if parameters are not well tuned."
   ],
   "id": "64e975b62c8525cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for kernel in [\"linear\", \"rbf\", \"sigmoid\"]:\n",
    "    plot_uncertainty(\n",
    "        cluster1,\n",
    "        cluster2,\n",
    "        title=\"Kernel comparison\",\n",
    "        kernel=kernel,\n",
    "        C=0.5,\n",
    "        gamma=\"scale\",\n",
    "        show=True,\n",
    "    )"
   ],
   "id": "fd9d5ac662d2354a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 How C influences the outcome.\n",
    "`C` controls the trade-off between fitting data and keeping the decision boundary smooth.\n",
    "\n",
    "**High C:** <br>\n",
    "The model penalizes misclassifications strongly, it tries hard to classify every training point correctly.<br>\n",
    "Result: tighter boundary, less tolerance for outliers, **higher risk of overfitting**.\n",
    "\n",
    "**Low C:** <br>\n",
    "The model allows **more classification errors**, it prefers a simpler boundary. <br>\n",
    "Result: smoother boundary, more tolerance for noise/outliers, often better generalization.\n",
    "\n",
    "In the clustermargin plot, changing `C` often changes how “tight” the transition region between the two clusters looks."
   ],
   "id": "54ccbe8dda24184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for C in [0.1, 0.5, 2.0]:\n",
    "    plot_uncertainty(\n",
    "        cluster1,\n",
    "        cluster2,\n",
    "        title=\"Effect of C\",\n",
    "        kernel=\"rbf\",\n",
    "        C=C,\n",
    "        gamma=\"scale\",\n",
    "        show=True,\n",
    "    )"
   ],
   "id": "6135d6a4a06d842b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4 How gamma influences the outcome.\n",
    "`gamma` mostly matters for nonlinear kernels (`rbf` and `sigmoid`). It controls how far the influence of a single training point reaches.<br>\n",
    "\n",
    "**High gamma:**<br>\n",
    "Each point influences only a very **small neighborhood**.<br>\n",
    "Result: the decision boundary becomes very local and wiggly, potentially overfitting.\n",
    "\n",
    "**Low gamma:**<br>\n",
    "Each point influences a **larger region**.<br>\n",
    "Result: the boundary becomes smoother and more global, potentially underfitting.\n",
    "\n",
    "**Special options:**<br>\n",
    "`gamma=\"scale\"`: uses 1 / (n_features * X.var())<br>\n",
    "`gamma=\"auto\"`: uses 1 / n_features\n",
    "\n",
    "In `clustermargin`, `gamma` strongly affects how “narrow” or “wide” the uncertainty band is around the boundary."
   ],
   "id": "eb4bfa649f0aa849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for gamma in [\"scale\", 0.2, 1.0, 5.0]:\n",
    "    plot_uncertainty(\n",
    "        cluster1,\n",
    "        cluster2,\n",
    "        title=f\"Effect of gamma = {gamma}\",\n",
    "        kernel=\"rbf\",\n",
    "        C=0.5,\n",
    "        gamma=gamma,\n",
    "        show=True,\n",
    "    )"
   ],
   "id": "994d5b12d45e0bb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. All about flags:<br>\n",
    "To use clustermargin the most flexible way, there are several flags you can use when calling `plot_uncertainty()`.<br>\n",
    "\n",
    "`title`: Add a custom title for your plot. Defaults to *Uncertainty*.<br>\n",
    "`x-label`: Add a custom name for the x-axis. Defaults to *Feature 1*.<br>\n",
    "`y-label`: Add a custom name for the y-axis. Defaults to *Feature 2*.<br>\n",
    "`class_labels`: List of names for classes. Defaults to to Class [i], where i is number of class.<br>\n",
    "`kernel`: You can change the kernel as mentioned above. You can choose between `rbf` (default), `linear` and `sigmoid`.<br>\n",
    "`C`: Regularization parameter, the lower, the more tolerant to outliers. Cannot be below 0.0. , defaults to 0.5.<br>\n",
    "`gamma`: Kernel coefficient controlling the influence radius of samples. Higher values lead to more local decision boundaries. Can either be a float, `auto` or `scale` (default).<br>\n",
    "`show`: Boolean, can either be `True` (dafault) or `False`.<br>"
   ],
   "id": "b483cae89169176c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Further reading:\n",
    "[Scikit SVM] https://scikit-learn.org/stable/modules/svm.html\n"
   ],
   "id": "8511fa4367533891"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
