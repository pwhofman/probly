{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49dc21c",
   "metadata": {},
   "source": [
    "# Unified Evidential Training Example\n",
    "\n",
    "To add training functionality to the probly package, we want to add a Unified Evidential Training function, that enables evidential models to be easily trained.\n",
    "\n",
    "This notebook demonstrates how a Unified Evidential Training Function works.\n",
    "It uses the `MNIST-dataset` and the `EvidentialCELoss` loss-function, as introduced by _Sensoy et al. (2018)_.\n",
    "The function `unified_evidential_train()` simulates, how the routine is going to look later on.\n",
    "\n",
    "This notebook can be divided into 5 sections:\n",
    "\n",
    "1. Imports & Setup\n",
    "2. Data Preparation\n",
    "3. Model Definition\n",
    "4. Unified Evidential Training Function\n",
    "5. Starting Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccea3a",
   "metadata": {},
   "source": [
    "### 1. Imports & Setup\n",
    "- **torch** → building neural networks\n",
    "- **torchvision** → used to convert images to tensors and normalize them\n",
    "- **losses** → pre-defined loss-functions\n",
    "- **models** → pre-defined models\n",
    "- **unified_evidential_train_class** → our pre-defined unified-evidential-training function\n",
    "\n",
    "This imports everything your Unified Evidential Training Function will also depend on:\n",
    "datasets, losses, and the PyTorch core tools.\n",
    "Right now we do it manually but the unified function will handle this internally later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3600554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import probly.layers.evidential.torch as models\n",
    "import probly.losses.evidential.torch as losses\n",
    "from probly.train.evidential import unified_evidential_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5980a43",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "This simulates how the function will handle datasets. It'll prepare them by e.g. converting images to tensors and normalizing pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c251d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded (ID).\n",
      "Loaded datasets with 60000 samples.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# In-distribution data\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"MNIST loaded (ID).\")\n",
    "\n",
    "# Out-of-distribution data\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Loaded datasets with {len(train_data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcc22d",
   "metadata": {},
   "source": [
    "### 3. Model Definition\n",
    "This is an example of a small Convolutional Neural Network (CNN), that produces evidence values instead of softmax probabilities. Later on we can also use models from `probly.layers` if we want to.\n",
    "Our unified function will be able to train such a model with the corresponding evidential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184af3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D102\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softplus(self.fc2(x))  # use of softplus so that our output is always positive\n",
    "\n",
    "\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7629",
   "metadata": {},
   "source": [
    "### 4. Unified Evidential Training Function\n",
    "In this part, we create the heart of our notebook...the Unified Evidential Train Function.\n",
    "Its takes a model and a bunch of other parameters in, that the user can costumize before running.\n",
    "After starting, it creates a training loop for evidential deep learning in PyTorch based on our given parameters (e.g. model, loss-function, epochs...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_evidential_train_showcase(model, dataloader, loss_fn, epochs=5, lr=1e-3, device=\"cpu\") -> None:  # noqa: ANN001\n",
    "    \"\"\"Demonstration of a unified evidential training function.\"\"\"\n",
    "    model = model.to(device)  # moves the model to the correct device (GPU or CPU)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # repeats the training function for a defined number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # call of train important for models like dropout\n",
    "        total_loss = 0.0  # track total_loss to calculate average loss per epoch\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            # handle both cases: distributions (CIFAR10H original) or integer labels (fallback)\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "\n",
    "            y = torch.tensor(y, device=device) if not torch.is_tensor(y) else y.to(device)  # noqa: PLW2901\n",
    "\n",
    "            optimizer.zero_grad()  # clears old gradients\n",
    "            outputs = model(x)  # computes model-outputs\n",
    "            loss = loss_fn(outputs, y)  # calculate the evidential loss based on given loss-function\n",
    "            loss.backward()  # backpropagation\n",
    "            optimizer.step()  # updates model-parameters\n",
    "\n",
    "            total_loss += loss.item()  # add-up the loss of this epoch ontop of our total loss till then\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)  # calculate average loss per epoch across all batches\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a408a",
   "metadata": {},
   "source": [
    "### 5. Starting Training Loop\n",
    "In this part, we are executing the training funtion and starting the training loop, after we imported and initiated the model and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9245363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 1.1457\n",
      "Epoch [2/5] - Loss: 0.6863\n",
      "Epoch [3/5] - Loss: 0.5780\n",
      "Epoch [4/5] - Loss: 0.5172\n",
      "Epoch [5/5] - Loss: 0.4753\n"
     ]
    }
   ],
   "source": [
    "model = models.SimpleCNN(num_classes=10)  # model that gets trained\n",
    "loss = losses.evidential_ce_loss  # loss function that will be customizable later on\n",
    "\n",
    "unified_evidential_train(\n",
    "    mode=\"EDL\",\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eef06b",
   "metadata": {},
   "source": [
    "### 6. Test with OOD-Data\n",
    "Here we want to load the earlier defined Out-Of-Distribution-Data in our unified-evidential-training-function and compute the Uncertaintys of the two different datasets. We expect the Uncertainty for the OOD-Data to be certainly higher than the ID-Data's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c86fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha0_edl(model: torch.Tensor, loader: DataLoader, device: torch.device) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    out = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "            outputs = model(x)\n",
    "            evidence = torch.relu(outputs)\n",
    "            alpha = evidence + 1.0\n",
    "            alpha0 = alpha.sum(dim=1)\n",
    "            out.append(alpha0.cpu())\n",
    "\n",
    "    return torch.cat(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_alpha0 = compute_alpha0_edl(model, train_loader, \"cpu\")\n",
    "ood_alpha0 = compute_alpha0_edl(model, ood_loader, \"cpu\")\n",
    "\n",
    "print(\"Mean ID α₀:\", id_alpha0.mean().item())\n",
    "print(\"Mean OOD α₀:\", ood_alpha0.mean().item())\n",
    "\n",
    "\n",
    "K = 10  # oder num_classes\n",
    "\n",
    "id_unc = K / id_alpha0\n",
    "ood_unc = K / ood_alpha0\n",
    "\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "\n",
    "plt.hist(id_unc.numpy(), bins=50, alpha=0.6, label=\"ID\")\n",
    "plt.hist(ood_unc.numpy(), bins=50, alpha=0.6, label=\"OOD\")\n",
    "plt.xlabel(\"Uncertainty (K / α₀)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Predictive Uncertainty (EDL)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
