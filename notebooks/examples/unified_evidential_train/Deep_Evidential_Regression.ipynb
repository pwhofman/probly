{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf91a2d-9a1d-41ab-aa15-d5cfff492221",
   "metadata": {},
   "source": [
    "## Deep Evidential Regression (Amini et al., 2020) - Summary\n",
    "\n",
    "### 1.  Introduction: Why do we need uncertainty estimation in regression?\n",
    "\n",
    "Neural Networks are increasingly being used in places where mistakes can be dangerous or expensive, such as autonomous driving or medical tasks. \n",
    "\n",
    "In these situations, it’s not enough to only receive a prediction. In order to minimize mistakes as much as possible, the model should not only output a prediction, but also say how certain it is about said prediction.\n",
    "\n",
    "Having reliable uncertainty estimates helps prevent wrong decisions, detect unusual data, and improve the safety of machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a25da-c1d1-4343-8b55-f50ef352cb7a",
   "metadata": {},
   "source": [
    "### 2. What is Deep Evidential Regression? \n",
    "\n",
    "Deep Evidential Regression is a method where a neural network predicts the parameters of a Normal-Inverse-Gamma distribution instead of predicting a single output. \n",
    "\n",
    "This higher-order distribution represents “evidence” for the prediction and allows the model to estimate both aleatoric and epistemic uncertainty in one forward pass, without sampling or ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bf096-6df2-4633-a879-8e3fdcc205e1",
   "metadata": {},
   "source": [
    "### 3. Two types of uncertainties in regression models \n",
    "\n",
    "**Aleatoric Uncertainty:** noise in the data; it is caused by randomness or measurement noise that even a perfect model can’t eliminate. An example for that would be a noisy sensor reading distance. \n",
    "\n",
    "**Epistemic uncertainty:** the model simply doesn’t know enough; it is caused by a lack of knowledge, which decreases with more data. This uncertainty is high when the model sees something unfamiliar or out-of-distribution. An example would be training the model on indoor-only images and then suddenly showing it a snowy mountain. \n",
    "\n",
    "These two uncertainties behave differently and matter in different ways, so a good system should be able to estimate both. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268f9a5-f0cf-48de-8a64-8fdfe703c63e",
   "metadata": {},
   "source": [
    "### 4. Proposal of Amini et al. \n",
    "\n",
    "The main idea of the paper is to let a neural network predict not only a single output value, but the parameters of a Normal-Inverse-Gamma (NIG) distribution.\n",
    "This distribution is a higher-order distribution that represents uncertainty about the parameters of a Gaussian likelihood (its mean and variance). By learning this distribution, the model can estimate both aleatoric and epistemic uncertainty in a principled way.\n",
    "\n",
    "For each regression target, the network outputs four values:\n",
    "\n",
    "- **γ**: predicted mean\n",
    "- **υ**: strength of belief about the mean\n",
    "- **α**: evidence related to the variance\n",
    "- **β**: scale parameter for the variance\n",
    "\n",
    "Using these parameters, the model can compute:\n",
    "\n",
    "- the final prediction\n",
    "- the aleatoric uncertainty\n",
    "- the epistemic uncertainty\n",
    "\n",
    "The key idea is that the NIG distribution captures uncertainty about the likelihood parameters themselves, not just about the data.\n",
    "This allows the model to express how confident it is about its own prediction without using sampling, dropout, or model ensembles.\n",
    "\n",
    "Because the model learns how much evidence it has for its predictions, it can increase uncertainty when it makes mistakes or when it sees out-of-distribution data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2b3dd-4229-4b6f-a324-a2d2ddc8ab10",
   "metadata": {},
   "source": [
    "### 5. Loss Functions\n",
    "\n",
    "As explained earlier, in Deep Evidential Regression, the neural network not only outputs a single prediction, but also the parameters of a Normal-Inverse-Gamma distribution. Because of this, the loss function needs to do two things at the same time: make the prediction fit the data + control how much evidence or uncertainty the model produces. \n",
    "\n",
    "Because the model outputs a Normal-Inverse-Gamma (NIG) distribution, the resulting likelihood over the target becomes a Student-t distribution. The Student-t distribution is similar to a Gaussian but has heavier tails, which makes it better at handling uncertainty and outliers. This is why the loss function uses the Student-t Negative Log-Likelihood (NLL).\n",
    "\n",
    "The paper combines two different loss components into one: \n",
    "\n",
    "**1. Student-t Negative Log-Likelihood (NLL)**  \n",
    "\n",
    "This measures how well the predicted distribution matches the true target value.\n",
    "Because the NIG distribution induces a Student-t likelihood, this term naturally models the data noise and is responsible for learning aleatoric uncertainty.\n",
    "\n",
    "**2. Evidence Regularizer**  \n",
    "This penalizes the model when it assigns a large amount of evidence (high confidence) to a prediction that is far from the true value.\n",
    "It encourages the network to reduce evidence and increase epistemic uncertainty whenever it encounters unfamiliar or difficult inputs.\n",
    "\n",
    "The total loss is simply:\n",
    "The final loss combines both parts:\n",
    "\n",
    "$$\n",
    "L = L_{\\text{NLL}} + \\lambda \\cdot L_R\n",
    "$$\n",
    "\n",
    "where the regularizer is:\n",
    "\n",
    "$$\n",
    "L_R = |y - \\gamma| \\cdot (2\\upsilon + \\alpha)\n",
    "$$\n",
    "\n",
    "This loss encourages the model to make accurate predictions while also expressing meaningful uncertainty.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bf9dae-461a-4f7a-ab23-930199a6bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc82423-b3d3-42d3-a325-851a67b2585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evidential_loss(\n",
    "    y: torch.Tensor,\n",
    "    gamma: torch.Tensor,\n",
    "    v: torch.Tensor,\n",
    "    alpha: torch.Tensor,\n",
    "    beta: torch.Tensor,\n",
    "    lam: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Evidential Regression Loss (Amini et al., 2020).\n",
    "\n",
    "    L = LNLL + λ * LR\n",
    "\n",
    "    Args:\n",
    "        y: Ground-truth targets.\n",
    "        gamma: Predicted mean.\n",
    "        v: Evidence for the mean (> 0).\n",
    "        alpha: Evidence for variance (> 1).\n",
    "        beta: Scale parameter (> 0).\n",
    "        lam: Regularization weight (λ).\n",
    "    \"\"\"\n",
    "    # Avoid log(0)\n",
    "    eps = 1e-8\n",
    "\n",
    "    # --- 1. Student-t Negative Log Likelihood (NLL) ---\n",
    "    two_bv = 2.0 * beta * (1.0 + v) + eps\n",
    "\n",
    "    lnll = (\n",
    "        0.5 * torch.log(torch.pi / (v + eps))\n",
    "        - alpha * torch.log(two_bv)\n",
    "        + (alpha + 0.5) * torch.log(v * (y - gamma).pow(2) + two_bv)\n",
    "        + torch.lgamma(alpha)\n",
    "        - torch.lgamma(alpha + 0.5)\n",
    "    )\n",
    "\n",
    "    # --- 2. Evidence Regularizer (LR) ---\n",
    "    evidence = 2.0 * v + alpha\n",
    "    lr = torch.abs(y - gamma) * evidence\n",
    "\n",
    "    # --- 3. Final Loss ---\n",
    "    return (lnll + lam * lr).mean()  # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11e4a3-9bbd-4cea-8977-5884c7d79942",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. Comparison with Sensoy et al. (2018)\n",
    "\n",
    "The Sensoy et al. paper introduces evidential learning for classification using a Dirichlet distribution and a KL-based evidence regularized. Amini et al. extends the evidential idea to regression, using Normal-Inverse-Gamma distribution and a new evidence penalty to capture both aleatoric and epistemic uncertainty. \n",
    "\n",
    "Together, these two papers form the foundation of modern evidential deep learning for both classification and regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416682be-b66c-4017-ac8a-da151d59ba09",
   "metadata": {},
   "source": [
    "### 7. Implementation Example (PyTorch)\n",
    "\n",
    "Below is a minimal PyTorch example (without the unified evidential training function) showing how the Deep Evidential Regression loss can be used to train a simple model on a toy regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2535171-130c-4fcc-aae8-d894abc11f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny toy dataset\n",
    "x = torch.linspace(-3.0, 3.0, 200).unsqueeze(1)\n",
    "y = x**3 + 0.3 * torch.randn_like(x)\n",
    "\n",
    "\n",
    "# minimal model\n",
    "class EvidentialNet(nn.Module):\n",
    "    def __init__(self) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1, 4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, ...]:  # noqa: D102\n",
    "        out = self.fc(x)\n",
    "        gamma = out[:, 0:1]\n",
    "        v = F.softplus(out[:, 1:2])\n",
    "        alpha = F.softplus(out[:, 2:3]) + 1.0\n",
    "        beta = F.softplus(out[:, 3:4])\n",
    "        return gamma, v, alpha, beta\n",
    "\n",
    "\n",
    "model = EvidentialNet()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# short training loop\n",
    "for _ in range(1500):\n",
    "    opt.zero_grad()\n",
    "    gamma, v, alpha, beta = model(x)\n",
    "    loss = evidential_loss(y, gamma, v, alpha, beta, lam=1e-2)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# visualize result\n",
    "with torch.no_grad():\n",
    "    preds = model(x)[0]\n",
    "\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, preds, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039a40c-dad8-4380-b801-a9c23baea44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: OOD epistemic uncertainty\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# OOD input range far from training domain\n",
    "ood_x: torch.Tensor = torch.linspace(5.0, 8.0, 200).unsqueeze(1)\n",
    "\n",
    "# Predict evidential parameters (model must already be trained)\n",
    "with torch.no_grad():\n",
    "    gamma_ood, v_ood, a_ood, b_ood = model(ood_x)\n",
    "\n",
    "# Epistemic uncertainty: sqrt(beta / (v * (alpha - 1)))\n",
    "epistemic_ood: torch.Tensor = (b_ood / (v_ood * (a_ood - 1.0))).sqrt()\n",
    "\n",
    "# Plot epistemic uncertainty curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(ood_x.squeeze(), epistemic_ood.squeeze(), label=\"Epistemic (OOD)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Uncertainty\")\n",
    "plt.title(\"Sanity Check: OOD Epistemic Uncertainty Should Be HIGH\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d78cb7-ef2f-446c-8152-d7a67414af3b",
   "metadata": {},
   "source": [
    "### Application of the Unified Evidential Function \n",
    "In this segment, we will be using the unified evidential function created for classifiction and regression models alike, which will significantly reduce the length of the code and make it more practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f220f170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded (ID).\n",
      "Loaded datasets with 60000 samples.\n"
     ]
    }
   ],
   "source": [
    "# test with the unified function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNIST1DRegression(Dataset):\n",
    "    def __init__(self, mnist_dataset: Dataset) -> None:  # noqa: D107\n",
    "        self.mnist = mnist_dataset\n",
    "\n",
    "    def __len__(self) -> int:  # noqa: D105\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: D105\n",
    "        x, y = self.mnist[idx]\n",
    "\n",
    "        # 1D input\n",
    "        x_1d = x.mean().view(1)\n",
    "\n",
    "        # convert int -> tensor\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # regression target\n",
    "        y_reg = y / 9.0\n",
    "\n",
    "        return x_1d, y_reg.view(1)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# In-distribution data\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_data = MNIST1DRegression(train_data)\n",
    "test_data = MNIST1DRegression(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"MNIST loaded (ID).\")\n",
    "\n",
    "# Out-of-distribution data\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "ood_data = MNIST1DRegression(ood_data)\n",
    "\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Loaded datasets with {len(train_data)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33635859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 0.6970\n",
      "Epoch [2/5] - Loss: 0.3456\n",
      "Epoch [3/5] - Loss: 0.3332\n",
      "Epoch [4/5] - Loss: 0.3254\n",
      "Epoch [5/5] - Loss: 0.3187\n"
     ]
    }
   ],
   "source": [
    "from probly.losses.evidential.torch import der_loss\n",
    "from probly.models.evidential.torch import EvidentialRegressionModel\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    \"\"\"Simple MLP encoder used to transform inputs into feature embeddings.\n",
    "\n",
    "    This module contains no evidential logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 1, hidden_dim: int = 64, latent_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Size of input features.\n",
    "            hidden_dim: Number of neurons in hidden layers.\n",
    "            latent_dim: Dimension of the output feature representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute feature embedding.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Feature tensor of shape (N, feature_dim).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "enc = MLPEncoder()\n",
    "model = EvidentialRegressionModel(encoder=enc)\n",
    "loss = der_loss\n",
    "unified_evidential_train(mode=\"DER\", model=model, dataloader=ood_loader, loss_fn=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a025b9-1847-4a36-b013-2afb34129fdc",
   "metadata": {},
   "source": [
    "### 8. Project Insights \n",
    "\n",
    "Amini et al. (2020) provide the theoretical and practical foundation for evidential regression in probly. Their NIG-based uncertainty model, loss function, and evidence regularizer are exactly what our project needs to use to implement fast, sampling-free uncertainty estimation for continuous outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6e14d-4026-409f-81ff-b43964fc2fba",
   "metadata": {},
   "source": [
    "### 9. Summary\n",
    "\n",
    "Deep Evidential Regression gives neural networks a way to predict both a value and how certain they are about it. By predicting the parameters of a Normal-Inverse-Gamma distribution, the model learns aleatoric and epistemic uncertainty in a single forward pass. The combination of the Student-T likelihood and the evidence regularizer ensures that the model becomes confident only when it should. Overall, this method provides a simple and efficient way to add uncertainty estimation to regression models without relying on sampling or ensembles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
