{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49dc21c",
   "metadata": {},
   "source": [
    "# Unified Evidential Training Example\n",
    "\n",
    "To add training functionality to the probly package, we want to add a Unified Evidential Training function, that enables evidential models to be easily trained.\n",
    "\n",
    "This notebook demonstrates how a Unified Evidential Training Function works.\n",
    "It uses the `MNIST-dataset` and the `EvidentialCELoss` loss-function, as introduced by _Sensoy et al. (2018)_.\n",
    "The function `unified_evidential_train()` simulates, how the routine is going to look later on.\n",
    "\n",
    "This notebook can be divided into 5 sections:\n",
    "\n",
    "1. Imports & Setup\n",
    "2. Data Preparation\n",
    "3. Model Definition\n",
    "4. Unified Evidential Training Function\n",
    "5. Starting Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccea3a",
   "metadata": {},
   "source": [
    "### 1. Imports & Setup\n",
    "- **torch** → building neural networks\n",
    "- **torchvision** → used to convert images to tensors and normalize them\n",
    "- **losses** → pre-defined loss-functions\n",
    "- **models** → pre-defined models\n",
    "- **unified_evidential_train** → our pre-defined unified-evidential-training function\n",
    "\n",
    "This imports everything your Unified Evidential Training Function will also depend on:\n",
    "datasets, losses, and the PyTorch core tools.\n",
    "Right now we do it manually but the unified function will handle this internally later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3600554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import probly.losses.evidential.torch as losses\n",
    "import probly.models.evidential.torch as t\n",
    "from probly.train.evidential.torch import unified_evidential_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5980a43",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "This simulates how the function will handle datasets. It'll prepare them by e.g. converting images to tensors and normalizing pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c251d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded (ID).\n",
      "Loaded datasets with 60000 samples.\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# In-distribution data\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"MNIST loaded (ID).\")\n",
    "\n",
    "# Out-of-distribution data\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Loaded datasets with {len(train_data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcc22d",
   "metadata": {},
   "source": [
    "### 3. Model Definition\n",
    "This section defines a small Neural Network based on Sensoy et al. (2018), which produces alpha values. The model consists of:\n",
    "\n",
    "- **EncoderMnist**: Flattens the input and maps it through two linear layers to a latent representation\n",
    "- **EDLHead**: Takes the latent representation and outputs alpha values\n",
    "- **EDLModel**: Combines the encoder and head into a complete model\n",
    "\n",
    "Later on, we can also use models from `probly.models` if we want to. Our unified function will be able to train such a model with the corresponding evidential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184af3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMnist(nn.Module):  # Encoder for MNIST-dataset\n",
    "    def __init__(self, latent_dim: int = 32) -> None:\n",
    "        \"\"\"Initialize the EncoderMnist for MNIST dataset.\n",
    "\n",
    "        Args:\n",
    "            latent_dim: Dimension of the latent space. Defaults to 32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Latent representation.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EDLHead(nn.Module):\n",
    "    \"\"\"outputs Dirichlet concentration parameters (alpha).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int, num_classes: int = 10, hidden_dim: int = 128) -> None:\n",
    "        \"\"\"Initialize the EDLHead.\n",
    "\n",
    "        Args:\n",
    "            latent_dim: Dimension of the input latent vector.\n",
    "            num_classes: Number of output classes. Defaults to 10.\n",
    "            hidden_dim: Dimension of the hidden layer. Defaults to 128.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass to compute Dirichlet concentration parameters (alpha).\n",
    "\n",
    "        Args:\n",
    "            z: Input latent tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Dirichlet concentration parameters (alpha).\n",
    "        \"\"\"\n",
    "        alpha = F.softplus(self.net(z)) + 1.0\n",
    "\n",
    "        return alpha\n",
    "\n",
    "\n",
    "class EDLModel(nn.Module):\n",
    "    \"\"\"Simple model for EDL classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module | None = None,\n",
    "        head: nn.Module | None = None,\n",
    "        latent_dim: int = 32,\n",
    "        num_classes: int = 10,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the EDLModel for evidential classification.\n",
    "\n",
    "        Args:\n",
    "            encoder: Encoder module mapping inputs to latent space.\n",
    "            head: Head module for evidential output (defaults to EDLHead).\n",
    "            latent_dim: Dimension of the latent space.\n",
    "            num_classes: Number of output classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if head is None:\n",
    "            head = t.EDLHead(latent_dim=latent_dim, num_classes=num_classes)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through encoder and head.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor compatible with the encoder.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor from the head module.\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z)\n",
    "\n",
    "\n",
    "model = EDLModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7629",
   "metadata": {},
   "source": [
    "### 4. Unified Evidential Training Function\n",
    "In this part, we create the heart of our notebook...the Unified Evidential Train Function.\n",
    "Its takes a model and a bunch of other parameters in, that the user can costumize before running.\n",
    "After starting, it creates a training loop for evidential deep learning in PyTorch based on our given parameters (e.g. model, loss-function, epochs...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_evidential_train_showcase(model, dataloader, loss_fn, epochs=5, lr=1e-3, device=\"cpu\") -> None:  # noqa: ANN001\n",
    "    \"\"\"Demonstration of a unified evidential training function.\"\"\"\n",
    "    model = model.to(device)  # moves the model to the correct device (GPU or CPU)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # repeats the training function for a defined number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # call of train important for models like dropout\n",
    "        total_loss = 0.0  # track total_loss to calculate average loss per epoch\n",
    "\n",
    "        for x_raw, y_raw in dataloader:\n",
    "            # handle both cases: distributions (CIFAR10H original) or integer labels (fallback)\n",
    "            x = x_raw.to(device)\n",
    "\n",
    "            y = torch.tensor(y_raw, device=device) if not torch.is_tensor(y_raw) else y_raw.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # clears old gradients\n",
    "            outputs = model(x)  # computes model-outputs\n",
    "            loss = loss_fn(outputs, y)  # calculate the evidential loss based on given loss-function\n",
    "            loss.backward()  # backpropagation\n",
    "            optimizer.step()  # updates model-parameters\n",
    "\n",
    "            total_loss += loss.item()  # add-up the loss of this epoch ontop of our total loss till then\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)  # calculate average loss per epoch across all batches\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a408a",
   "metadata": {},
   "source": [
    "### 5. Starting Training Loop\n",
    "In this part, we are executing the training funtion and starting the training loop, after we imported and initiated the model and the loss function. So this demonstrates the use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderMnist()  # definition of own encoder\n",
    "model = t.EDLModel(encoder=enc)  # model that gets trained\n",
    "loss = losses.evidential_ce_loss  # initialize loss function\n",
    "\n",
    "unified_evidential_train(\n",
    "    mode=\"EDL\",\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=loss,\n",
    ")  # call of uet with given parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eef06b",
   "metadata": {},
   "source": [
    "### 6. Test with OOD-Data\n",
    "Here we want to load the earlier defined Out-Of-Distribution-Data in our unified-evidential-training-function and compute the Uncertaintys of the two different datasets. We expect the Uncertainty for the OOD-Data to be certainly higher than the ID-Data's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c86fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha0_edl(model: torch.Tensor, loader: DataLoader, device: torch.device) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    out = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_raw, _ in loader:\n",
    "            x = x_raw.to(device)\n",
    "            alpha = model(x)\n",
    "            alpha0 = alpha.sum(dim=1)\n",
    "            out.append(alpha0.cpu())\n",
    "\n",
    "    return torch.cat(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_alpha0 = compute_alpha0_edl(model, train_loader, \"cpu\")\n",
    "ood_alpha0 = compute_alpha0_edl(model, ood_loader, \"cpu\")\n",
    "\n",
    "print(\"Mean ID α₀:\", id_alpha0.mean().item())\n",
    "print(\"Mean OOD α₀:\", ood_alpha0.mean().item())\n",
    "\n",
    "\n",
    "K = 10  # oder num_classes\n",
    "\n",
    "id_unc = K / id_alpha0\n",
    "ood_unc = K / ood_alpha0\n",
    "\n",
    "plt.hist(id_unc.numpy(), bins=50, alpha=0.6, label=\"ID\")\n",
    "plt.hist(ood_unc.numpy(), bins=50, alpha=0.6, label=\"OOD\")\n",
    "plt.xlabel(\"Uncertainty (K / α₀)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Predictive Uncertainty (EDL)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
