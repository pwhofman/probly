{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b81968",
   "metadata": {},
   "source": [
    "# Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts\n",
    "\n",
    "Posterior Networks (PostNet) extend the idea of Evidential Deep Learning by deriving Dirichlet evidence from class-conditional density estimates in a learned latent space. This enables uncertainty to increase for inputs that lie outside the learned data distribution, eliminating the need for explicit out-of-distribution (OOD) samples during training.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Define a Posterior Network model consisting of an encoder and a density model\n",
    "- Convert densities into evidence (Dirichlet pseudo-counts) and train with a unified evidential trainer\n",
    "- Evaluate accuracy and plot epistemic evidence for in-distribution (ID) and OOD data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fd6fa",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d22d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions import Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from probly.layers.evidential import torch as t\n",
    "from probly.losses.evidential.torch import postnet_loss\n",
    "import probly.models.evidential.torch as m\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d91721",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Posterior Networks require:\n",
    "- an ID dataset used for training and standard evaluation\n",
    "- an OOD dataset used only for testing epistemic uncertainty\n",
    "\n",
    "Here, we use **MNIST** as the ID dataset and **FashionMNIST** as the OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bbf28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST (ID) and FashionMNIST (OOD)\n"
     ]
    }
   ],
   "source": [
    "transform = T.transforms.Compose(\n",
    "    [\n",
    "        T.transforms.ToTensor(),\n",
    "        T.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Loaded MNIST (ID) and FashionMNIST (OOD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c437fd8",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "### Posterior Networks are composed of:\n",
    "\n",
    "#### 1. Encoder\n",
    "The encoder maps each input to a low-dimensional latent vector. A simple MLP-based encoder that flattens MNIST images is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4886e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Encoder using MNIST inputs\n",
    "class FlattenMLPEncoder(nn.Module):\n",
    "    \"\"\"Flatten + MLP encoder that maps an input tensor to a latent vector z.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 784, hidden_dim: int = 256, latent_dim: int = 6) -> None:\n",
    "        \"\"\"Initialize the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_dim: number of input features after flattening.\n",
    "            hidden_dim: width of the hidden layer.\n",
    "            latent_dim: Dimension of the output latent representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute latent vector z.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, ...) where B is batch size\n",
    "\n",
    "        Returns:\n",
    "            Latent tensor of shape (B, latent_dim).\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efb59c",
   "metadata": {},
   "source": [
    "#### 2. Class-conditional density model\n",
    "A radial-flow-based density estimator that computes class-conditional densities P(z|c) in the latent space. These density estimates are used to derive Dirichlet evidence for uncertainty-aware predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2649bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialFlowLayer(nn.Module):\n",
    "    \"\"\"Single radial flow transformation shared across all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int) -> None:\n",
    "        \"\"\"Initialize parameters for a radial flow transform.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.x0 = nn.Parameter(torch.zeros(self.c, self.dim))\n",
    "        self.alpha_prime = nn.Parameter(torch.zeros(self.c))\n",
    "        self.beta_prime = nn.Parameter(torch.zeros(self.c))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset learnable parameters with a small uniform init.\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.dim)\n",
    "        self.x0.data.uniform_(-stdv, stdv)\n",
    "        self.alpha_prime.data.uniform_(-stdv, stdv)\n",
    "        self.beta_prime.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, zc) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Apply the radial flow to latent inputs zc.\"\"\"\n",
    "        alpha = torch.nn.functional.softplus(self.alpha_prime)\n",
    "        beta = -alpha + torch.nn.functional.softplus(self.beta_prime)\n",
    "\n",
    "        x0 = self.x0.unsqueeze(1)\n",
    "        diff = zc - x0\n",
    "        r = diff.norm(dim=-1)\n",
    "\n",
    "        h = 1.0 / (alpha.unsqueeze(1) + r)\n",
    "        h_prime = -h * h\n",
    "        beta_h = beta.unsqueeze(1) * h\n",
    "\n",
    "        z_new = zc + beta_h.unsqueeze(-1) * diff\n",
    "\n",
    "        term1 = (self.dim - 1) * torch.log1p(beta_h)\n",
    "        term2 = torch.log1p(beta_h + beta.unsqueeze(1) * h_prime * r)\n",
    "        log_abs_det = term1 + term2\n",
    "\n",
    "        return z_new, log_abs_det\n",
    "\n",
    "\n",
    "class BatchedRadialFlowDensity(nn.Module):\n",
    "    \"\"\"Radial-flow density estimator that computes P(z|c) for all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int, flow_length: int = 6) -> None:\n",
    "        \"\"\"Create a sequence of radial flow layers and base distribution.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [RadialFlowLayer(num_classes, dim) for _ in range(flow_length)],\n",
    "        )\n",
    "\n",
    "        self.log_base_const = -0.5 * self.dim * math.log(2 * math.pi)\n",
    "\n",
    "    def forward(self, x) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Expand input x for all classes and apply flow layers.\"\"\"\n",
    "        B = x.size(0)  # noqa: N806\n",
    "        zc = x.unsqueeze(0).expand(self.c, B, self.dim)\n",
    "        sum_log_jac = torch.zeros(self.c, B, device=x.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            zc, log_j = layer(zc)\n",
    "            sum_log_jac = sum_log_jac + log_j\n",
    "\n",
    "        return zc, sum_log_jac\n",
    "\n",
    "    def log_prob(self, x) -> torch.Tensor:  # noqa: ANN001\n",
    "        \"\"\"Return class-conditional log densities log P(x|c).\"\"\"\n",
    "        zc, sum_log_jac = self.forward(x)  # zc: [C,B,D]\n",
    "\n",
    "        base_logp = self.log_base_const - 0.5 * (zc**2).sum(dim=-1)\n",
    "        logp = base_logp + sum_log_jac  # [C,B]\n",
    "\n",
    "        return logp.transpose(0, 1)  # [B,C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdc895",
   "metadata": {},
   "source": [
    "## Full Posterior Network Model\n",
    "\n",
    "The full Posterior Network combines the encoder and class-conditional density model to construct a Dirichlet posterior in a single forward pass.\n",
    "\n",
    "Given an input x, the model:\n",
    "1.\tEncodes x into a latent representation z,\n",
    "2.\tComputes class-conditional densities p(z \\mid y),\n",
    "3.\tConverts densities into Dirichlet concentration parameters,\n",
    "4.\tReturns both the Dirichlet parameters and the predictive mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07814ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostNetModel(nn.Module):\n",
    "    \"\"\"Posterior Network model containing encoder and class-conditional flows.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module | None = None,\n",
    "        num_classes: int = 10,\n",
    "        latent_dim: int = 6,\n",
    "        flow: t.BatchedRadialFlowDensity | None = None,\n",
    "        class_counts: torch.Tensor | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a Posterior Network model.\n",
    "\n",
    "        Args:\n",
    "            encoder: Encoder mapping inputs to a latent space.\n",
    "            num_classes: Number of output classes.\n",
    "            latent_dim: Dimensionality of the latent space.\n",
    "            flow: Class-conditional normalizing flow. If None, a default flow is used.\n",
    "            class_counts: Empirical class counts used as a prior. If None, assumes a uniform prior.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        if flow is None:\n",
    "            flow = t.BatchedRadialFlowDensity(num_classes=num_classes, dim=latent_dim, flow_length=6)\n",
    "        self.flow = flow\n",
    "\n",
    "        if class_counts is None:\n",
    "            class_counts = torch.ones(num_classes)\n",
    "        self.register_buffer(\"class_counts\", class_counts)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the Posterior Network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, ...) compatible with the encoder.\n",
    "\n",
    "        Returns:\n",
    "            alpha: Dirichlet concentration parameters of shape.\n",
    "            p_mean: Predictive mean of the Dirichlet distribution.\n",
    "            z: Latent representation of the input.\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        log_dens = self.flow.log_prob(z)\n",
    "        dens = log_dens.exp()\n",
    "\n",
    "        beta = dens * self.class_counts.unsqueeze(0)\n",
    "        alpha = beta + 1.0\n",
    "        alpha0 = alpha.sum(dim=1, keepdim=True)\n",
    "        p_mean = alpha / alpha0\n",
    "\n",
    "        return alpha, p_mean, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45df54f",
   "metadata": {},
   "source": [
    "## PostNet Loss Function\n",
    "\n",
    "Posterior Networks are trained using a Bayesian evidential loss that optimizes a Dirichlet distribution over class probabilities.\n",
    "The loss consists of two components:\n",
    "1. an expected cross-entropy term, which encourages correct class predictions\n",
    "2. an entropy regularization term, which promotes well-calibrated uncertainty estimates\n",
    "\n",
    "The Dirichlet parameters are constructed from class-conditional density estimates in latent space and are scaled by the number of training samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.special import digamma\n",
    "\n",
    "\n",
    "def postnet_loss_notebook(\n",
    "    alpha: Tensor,\n",
    "    y: Tensor,\n",
    "    entropy_weight: float = 1e-5,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Posterior Networks (PostNet) loss.\"\"\"\n",
    "    alpha0 = alpha.sum(dim=1)\n",
    "    batch_idx = torch.arange(len(y), device=y.device)\n",
    "    expected_ce = digamma(alpha0) - digamma(alpha[batch_idx, y])\n",
    "\n",
    "    entropy = Dirichlet(alpha).entropy()\n",
    "\n",
    "    loss = (expected_ce - entropy_weight * entropy).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79bb6d",
   "metadata": {},
   "source": [
    "## Training loop using Unified Evidential Train\n",
    "\n",
    "To train our model, we first compute the number of training samples per class.\n",
    "These class counts are used to scale the class-conditional density estimates and sure that the resulting Dirichlet parameters reflect the empirical class frequencies.\n",
    "\n",
    "We then instantiate the encoder and the Posterior Network model and train it using the unified evidential training function with the PostNet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 1.9861\n",
      "Epoch [2/5] - Loss: 0.3729\n",
      "Epoch [3/5] - Loss: 0.2563\n",
      "Epoch [4/5] - Loss: 0.2093\n",
      "Epoch [5/5] - Loss: 0.1838\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "class_counts = torch.zeros(num_classes, device=device)\n",
    "\n",
    "# Count how many samples belong to each class in the training set\n",
    "for _, y in train_loader:\n",
    "    y_device = y.to(device)\n",
    "    class_counts += torch.bincount(y_device, minlength=num_classes)\n",
    "\n",
    "enc = FlattenMLPEncoder(input_dim=784, hidden_dim=256, latent_dim=6).to(device)\n",
    "model = m.PostNetModel(encoder=enc)\n",
    "loss = postnet_loss\n",
    "\n",
    "unified_evidential_train(\n",
    "    \"PostNet\",\n",
    "    model,\n",
    "    train_loader,\n",
    "    loss,\n",
    "    class_count=class_counts,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a82ee",
   "metadata": {},
   "source": [
    "## ID Data Evaluation\n",
    "\n",
    "We evaluate classification accuracy on in-distribution test data using the mean of the predicted Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: m.PostNetModel, loader: torch.Tensor) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)  # noqa: PLW2901\n",
    "\n",
    "            _, probs, _ = model(x)\n",
    "\n",
    "            preds = probs.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "evaluate(model=model, loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6207d",
   "metadata": {},
   "source": [
    "## OOD Data Evaluation\n",
    "\n",
    "We evaluate epistemic uncertainty by comparing the total Dirichlet evidence for in-distribution (ID) and out-of-distribution (OOD) inputs.\n",
    "The total evidence is obtained by summing the Dirichlet parameters across classes and serves as a scalar measure of model confidence.\n",
    "\n",
    "In-distribution samples are expected to yield high evidence, while out-of-distribution inputs should result in low evidence, indicating increased epistemic uncertainty.\n",
    "We visualize the distributions of total Dirichlet evidence for ID and OOD data to highlight this separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha0(model: m.PostNetModel, loader: DataLoader) -> torch.Tensor:\n",
    "    model.eval()\n",
    "\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "\n",
    "            alpha, _, _ = model(x)\n",
    "            alpha0 = alpha.sum(dim=1)\n",
    "\n",
    "            out.append(alpha0.cpu())\n",
    "\n",
    "    return torch.cat(out)\n",
    "\n",
    "\n",
    "id_alpha0 = compute_alpha0(model, test_loader)\n",
    "ood_alpha0 = compute_alpha0(model, ood_loader)\n",
    "\n",
    "print(\"Mean ID α₀:\", id_alpha0.mean().item())\n",
    "print(\"Mean OOD α₀:\", ood_alpha0.mean().item())\n",
    "\n",
    "eps = 1e-12\n",
    "id_log = np.log10(id_alpha0.numpy() + eps)\n",
    "ood_log = np.log10(ood_alpha0.numpy() + eps)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(id_log, bins=60, density=True, alpha=0.6, label=\"ID\")\n",
    "plt.hist(ood_log, bins=60, density=True, alpha=0.6, label=\"OOD\")\n",
    "plt.xlabel(r\"$\\log_{10}(\\alpha_0)$ (total evidence)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"PostNet evidence: ID vs OOD\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
