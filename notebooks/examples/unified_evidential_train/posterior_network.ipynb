{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b81968",
   "metadata": {},
   "source": [
    "# Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts\n",
    "\n",
    "Posterior Networks (PostNet) extend the idea of Evidential Deep Learning (EDL) by producing a full Dirichlet distribution over class probabilities for each input. However, instead of evidence being directly predicted by the neural network, PostNet does so by deriving evidence from class-conditional density estimates in a latent space. This assures that out-of-distribution (OOD) samples are not needed during training, as uncertainty increases for inputs that lie outside the learned density.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Build a small encoder to map inputs into a latent space\n",
    "- Train a single batched radial flow that models all class-conditional densities\n",
    "- Convert densities into evidence (Dirichlet pseudo-counts) and train with a unified evidential trainer\n",
    "- Evaluate accuracy and plot epistemic evidence for in-distribution (ID) and OOD data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fd6fa",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d22d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions import Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from probly.layers.evidential import torch as t\n",
    "import probly.layers.evidential.torch as l\n",
    "from probly.losses.evidential.torch import postnet_loss\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d91721",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Posterior Networks require:\n",
    "- an ID dataset used for training and standard evaluation\n",
    "- an OOD dataset used only for testing epistemic uncertainty\n",
    "\n",
    "Here, we use **MNIST** as the ID dataset and **FashionMNIST** as the OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bbf28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST (ID) and FashionMNIST (OOD)\n"
     ]
    }
   ],
   "source": [
    "transform = T.transforms.Compose(\n",
    "    [\n",
    "        T.transforms.ToTensor(),\n",
    "        T.transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Loaded MNIST (ID) and FashionMNIST (OOD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c437fd8",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Posterior Networks are composed of:\n",
    "1. **Encoder**: maps each image to a low-dimensional latent vector.\n",
    "2. **Class-conditional normalizing flows**: instead of training one flow per class, we use a single batched radial-flow model that jointly computes all class-conditional densities P(z|c). These densities provide the evidence used to construct the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4886e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder (x -> z)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=6) -> None:  # noqa: ANN001\n",
    "        \"\"\"Initialize encoder with a small MLP and BatchNorm.\"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(latent_dim)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:  # noqa: ANN001\n",
    "        \"\"\"Encode a batch of images x into latent vectors z.\"\"\"\n",
    "        z = self.net(x)\n",
    "        z = self.bn(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Normalizing flows\n",
    "\n",
    "\n",
    "class RadialFlowLayer(nn.Module):\n",
    "    \"\"\"Single radial flow transformation shared across all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int) -> None:\n",
    "        \"\"\"Initialize parameters for a radial flow transform.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.x0 = nn.Parameter(torch.zeros(self.c, self.dim))\n",
    "        self.alpha_prime = nn.Parameter(torch.zeros(self.c))\n",
    "        self.beta_prime = nn.Parameter(torch.zeros(self.c))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset learnable parameters with a small uniform init.\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.dim)\n",
    "        self.x0.data.uniform_(-stdv, stdv)\n",
    "        self.alpha_prime.data.uniform_(-stdv, stdv)\n",
    "        self.beta_prime.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, zc) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Apply the radial flow to latent inputs zc.\"\"\"\n",
    "        alpha = torch.nn.functional.softplus(self.alpha_prime)\n",
    "        beta = -alpha + torch.nn.functional.softplus(self.beta_prime)\n",
    "\n",
    "        x0 = self.x0.unsqueeze(1)\n",
    "        diff = zc - x0\n",
    "        r = diff.norm(dim=-1)\n",
    "\n",
    "        h = 1.0 / (alpha.unsqueeze(1) + r)\n",
    "        h_prime = -h * h\n",
    "        beta_h = beta.unsqueeze(1) * h\n",
    "\n",
    "        z_new = zc + beta_h.unsqueeze(-1) * diff\n",
    "\n",
    "        term1 = (self.dim - 1) * torch.log1p(beta_h)\n",
    "        term2 = torch.log1p(beta_h + beta.unsqueeze(1) * h_prime * r)\n",
    "        log_abs_det = term1 + term2\n",
    "\n",
    "        return z_new, log_abs_det\n",
    "\n",
    "\n",
    "class BatchedRadialFlowDensity(nn.Module):\n",
    "    \"\"\"Radial-flow density estimator that computes P(z|c) for all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int, flow_length: int = 6) -> None:\n",
    "        \"\"\"Create a sequence of radial flow layers and base distribution.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [RadialFlowLayer(num_classes, dim) for _ in range(flow_length)],\n",
    "        )\n",
    "\n",
    "        self.log_base_const = -0.5 * self.dim * math.log(2 * math.pi)\n",
    "\n",
    "    def forward(self, x) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Expand input x for all classes and apply flow layers.\"\"\"\n",
    "        B = x.size(0)  # noqa: N806\n",
    "        zc = x.unsqueeze(0).expand(self.c, B, self.dim)\n",
    "        sum_log_jac = torch.zeros(self.c, B, device=x.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            zc, log_j = layer(zc)\n",
    "            sum_log_jac = sum_log_jac + log_j\n",
    "\n",
    "        return zc, sum_log_jac\n",
    "\n",
    "    def log_prob(self, x) -> torch.Tensor:  # noqa: ANN001\n",
    "        \"\"\"Return class-conditional log densities log P(x|c).\"\"\"\n",
    "        zc, sum_log_jac = self.forward(x)  # zc: [C,B,D]\n",
    "\n",
    "        base_logp = self.log_base_const - 0.5 * (zc**2).sum(dim=-1)\n",
    "        logp = base_logp + sum_log_jac  # [C,B]\n",
    "\n",
    "        return logp.transpose(0, 1)  # [B,C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45df54f",
   "metadata": {},
   "source": [
    "## PostNet Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937e8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postnet_loss2(\n",
    "    z: Tensor,\n",
    "    y: Tensor,\n",
    "    flow: t.BatchedRadialFlowDensity,\n",
    "    class_counts: Tensor,\n",
    "    entropy_weight: float = 1e-5,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Posterior Networks (PostNet) loss.\"\"\"\n",
    "    log_dens = flow.log_prob(z)  # [B,C]\n",
    "    dens = log_dens.exp()\n",
    "\n",
    "    beta = dens * class_counts.unsqueeze(0)\n",
    "    alpha = beta + 1.0\n",
    "    alpha0 = alpha.sum(dim=1)\n",
    "\n",
    "    digamma = torch.digamma\n",
    "    batch_idx = torch.arange(len(y), device=y.device)\n",
    "    expected_ce = digamma(alpha0) - digamma(alpha[batch_idx, y])\n",
    "\n",
    "    entropy = Dirichlet(alpha).entropy()\n",
    "\n",
    "    loss = (expected_ce - entropy_weight * entropy).mean()\n",
    "    return loss, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79bb6d",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2792a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 1.7287\n",
      "Epoch [2/5] - Loss: 0.7134\n",
      "Epoch [3/5] - Loss: 0.4660\n",
      "Epoch [4/5] - Loss: 0.3872\n",
      "Epoch [5/5] - Loss: 0.3333\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "class_counts = torch.zeros(num_classes, device=device)\n",
    "\n",
    "for _, y in train_loader:\n",
    "    y = y.to(device)  # noqa: PLW2901\n",
    "    for c in range(num_classes):\n",
    "        class_counts[c] += (y == c).sum()\n",
    "\n",
    "latent_dim = 6\n",
    "encoder = Encoder(latent_dim).to(device)\n",
    "loss = postnet_loss\n",
    "flow = l.BatchedRadialFlowDensity(num_classes=num_classes, dim=latent_dim, flow_length=6).to(device)\n",
    "\n",
    "unified_evidential_train(\n",
    "    \"PostNet\",\n",
    "    encoder,\n",
    "    train_loader,\n",
    "    loss,\n",
    "    flow=flow,\n",
    "    class_count=class_counts,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a82ee",
   "metadata": {},
   "source": [
    "## Evaluation: Predictions & Accuracy\n",
    "Compute Dirichlet posteriors and accuracy on the MNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, flow, loader, class_counts) -> float:  # noqa: ANN001\n",
    "    encoder.eval()\n",
    "    flow.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)  # noqa: PLW2901\n",
    "            z = encoder(x)\n",
    "\n",
    "            log_dens = flow.log_prob(z)\n",
    "            dens = log_dens.exp()\n",
    "            beta = dens * class_counts.unsqueeze(0)\n",
    "            alpha = beta + 1.0\n",
    "            alpha0 = alpha.sum(dim=1, keepdim=True)\n",
    "\n",
    "            probs = alpha / alpha0\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "evaluate(encoder, flow, test_loader, class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6207d",
   "metadata": {},
   "source": [
    "## Epistemic Uncertainty Extraction\n",
    "\n",
    "Posterior Networks quantify epistemic uncertainty using the total Dirichlet evidence, defined as the sum of all Dirichlet parameters for each class. High evidence indicates high confidence (in-distribution), while low evidence indicates uncertainty. Here, we compute evidence values for MNIST and FashionMNIST samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha0(encoder, flow, loader, class_counts) -> torch.Tensor:  # noqa: ANN001\n",
    "    encoder.eval()\n",
    "    flow.eval()\n",
    "\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "            z = encoder(x)\n",
    "            log_dens = flow.log_prob(z)\n",
    "            dens = log_dens.exp()\n",
    "            beta = dens * class_counts.unsqueeze(0)\n",
    "            alpha = beta + 1.0\n",
    "            alpha0 = alpha.sum(dim=1)\n",
    "            out.append(alpha0.cpu())\n",
    "    return torch.cat(out)\n",
    "\n",
    "\n",
    "id_alpha0 = compute_alpha0(encoder, flow, test_loader, class_counts)\n",
    "ood_alpha0 = compute_alpha0(encoder, flow, ood_loader, class_counts)\n",
    "\n",
    "print(\"Mean ID α₀:\", id_alpha0.mean().item())\n",
    "print(\"Mean OOD α₀:\", ood_alpha0.mean().item())\n",
    "\n",
    "eps = 1e-12\n",
    "id_log = np.log10(id_alpha0.numpy() + eps)\n",
    "ood_log = np.log10(ood_alpha0.numpy() + eps)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(id_log, bins=60, density=True, alpha=0.6, label=\"ID (MNIST)\")\n",
    "plt.hist(ood_log, bins=60, density=True, alpha=0.6, label=\"OOD (FashionMNIST)\")\n",
    "plt.xlabel(r\"$\\log_{10}(\\alpha_0)$ (total evidence)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"PostNet evidence: ID vs OOD\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
