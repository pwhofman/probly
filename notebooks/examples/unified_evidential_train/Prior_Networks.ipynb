{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bd0907",
   "metadata": {},
   "source": [
    "Dirichlet Prior Networks: Explicit Uncertainty Modeling with In-Distribution and OOD Supervision\n",
    "===============================================================================================\n",
    "\n",
    "This notebook provides a complete implementation of Dirichlet Prior Networks (DPNs) as introduced\n",
    "in the original Prior Networks framework by Malinin & Gales (2018). DPNs model a Dirichlet\n",
    "distribution over class probabilities to explicitly separate:\n",
    "\n",
    "- confident predictions on in-distribution data, and\n",
    "- high uncertainty on out-of-distribution (OOD) data.\n",
    "\n",
    "Training follows the formulation from the Prior Networks paper, using:\n",
    "\n",
    "- sharp Dirichlet targets for in-distribution samples,\n",
    "- flat Dirichlet targets for OOD samples,\n",
    "- KL divergence between target and predicted Dirichlet distributions.\n",
    "\n",
    "The notebook is structured into the following sections:\n",
    "\n",
    "1. Imports and Setup  \n",
    "2. Data Preparation  \n",
    "3. Model Definition  \n",
    "4. Dirichlet Target Construction  \n",
    "5. Unified DPN Training Function  \n",
    "6. Training Loop  \n",
    "7. Evaluation: Accuracy & Predictive Distribution  \n",
    "8. Uncertainty Analysis: Predictive and Differential Entropy  \n",
    "9. Sanity Checks  \n",
    "\n",
    "This notebook implements Dirichlet Prior Networks strictly according to the original formulation:\n",
    "\n",
    "Malinin, A., & Gales, M. (2018).  \n",
    "**“Predictive Uncertainty Estimation via Prior Networks.”**  \n",
    "Advances in Neural Information Processing Systems (NeurIPS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8446b",
   "metadata": {},
   "source": [
    "Imports and Setup\n",
    "-----------------\n",
    "\n",
    "This section loads all required libraries for defining and training a Dirichlet Prior Network.\n",
    "We use PyTorch for model construction, torchvision for dataset handling, and additional numerical\n",
    "utilities for Dirichlet computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115742e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.special import digamma, gammaln\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from probly.losses.evidential.torch import pn_loss\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66baaa",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "----------------\n",
    "\n",
    "Dirichlet Prior Networks require two datasets during training:\n",
    "\n",
    "- an in-distribution (ID) dataset, used to encourage sharp Dirichlet predictions,\n",
    "- an out-of-distribution (OOD) dataset, used to encourage flat Dirichlet predictions.\n",
    "\n",
    "Following the Prior Networks formulation (Malinin & Gales, 2018), we use:\n",
    "\n",
    "- **MNIST** as the in-distribution dataset,\n",
    "- **Fashion-MNIST** as the OOD dataset.\n",
    "\n",
    "Both datasets are normalized in the same manner to ensure consistent feature scaling.  \n",
    "Images are converted to tensors and normalized to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac88c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST (ID) with 60000 training samples.\n",
      "Loaded Fashion-MNIST (OOD) with 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Transformation: convert to tensor and normalize pixel values\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# In-distribution dataset (MNIST)\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Out-of-distribution dataset (Fashion-MNIST)\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Loaded MNIST (ID) with {len(train_data)} training samples.\")\n",
    "print(f\"Loaded Fashion-MNIST (OOD) with {len(ood_data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ada145",
   "metadata": {},
   "source": [
    "Model Definition\n",
    "----------------\n",
    "\n",
    "Following the Prior Networks formulation, a Dirichlet Prior Network produces concentration\n",
    "parameters α ∈ ℝ₊ᴷ for K classes. These parameters define a Dirichlet distribution over the\n",
    "categorical class probabilities.\n",
    "\n",
    "The model used here is a compact convolutional neural network that maps an input image x ∈ ℝ¹ˣ²⁸ˣ²⁸\n",
    "to a vector of K Dirichlet concentration parameters via:\n",
    "\n",
    "1. a convolutional feature extractor,\n",
    "2. a fully-connected classifier head,\n",
    "3. a softplus activation ensuring αₖ > 0 for all classes k.\n",
    "\n",
    "This architecture is sufficient for MNIST-scale experiments and follows the structure typically\n",
    "employed in DPN literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795cc4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvDPN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6272, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvDPN(nn.Module):\n",
    "    \"\"\"Convolutional Dirichlet Prior Network producing concentration parameters (alpha).\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        \"\"\"Initialize the ConvDPN model with the given number of output classes.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Fully-connected classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Return Dirichlet concentration parameters (alpha)(x) > 0.\"\"\"\n",
    "        x = self.features(x)\n",
    "        logits = self.classifier(x)\n",
    "        alpha = F.softplus(logits) + 1e-3\n",
    "        return alpha\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"Generic convolutional encoder mapping images to latent features.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        conv_channels: list[int],\n",
    "        latent_dim: int,\n",
    "        input_shape: tuple[int, int],\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a configurable convolutional encoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        c_in = in_channels\n",
    "\n",
    "        for c_out in conv_channels:\n",
    "            layers.extend(\n",
    "                [\n",
    "                    nn.Conv2d(c_in, c_out, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2),\n",
    "                ]\n",
    "            )\n",
    "            c_in = c_out\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Infer flattened feature dimension dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, *input_shape)\n",
    "            feat = self.features(dummy)\n",
    "            flattened_dim = feat.view(1, -1).size(1)\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode inputs into latent feature representations.\"\"\"\n",
    "        x = self.features(x)\n",
    "        z = self.projection(x)\n",
    "        return z\n",
    "\n",
    "\n",
    "model = ConvDPN(num_classes=10).to(device)\n",
    "model  # display architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b7b10",
   "metadata": {},
   "source": [
    "Dirichlet Target Construction\n",
    "-----------------------------\n",
    "\n",
    "Following the Prior Networks formulation, training requires constructing target Dirichlet\n",
    "distributions for both in-distribution (ID) and out-of-distribution (OOD) inputs.\n",
    "\n",
    "For an input belonging to the true class y, the target Dirichlet distribution should be:\n",
    "\n",
    "- **sharp**, with high precision α₀, and\n",
    "- concentrated around the one-hot class assignment.\n",
    "\n",
    "For OOD inputs, the target distribution should be:\n",
    "\n",
    "- **flat**, with low precision α₀, and\n",
    "- uniform across all classes.\n",
    "\n",
    "These target distributions provide the prior belief used during KL-based training:\n",
    "\n",
    "\\[\n",
    "\\text{KL}(\\,\\mathrm{Dir}(\\alpha^{\\text{target}})\\;||\\;\\mathrm{Dir}(\\alpha^{\\text{pred}})\\,).\n",
    "\\]\n",
    "\n",
    "This section defines helper functions that construct sharp and flat Dirichlet targets given\n",
    "a batch of labels (ID) or batch size (OOD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision values defining sharp (ID) and flat (OOD) Dirichlet distributions\n",
    "alpha0_in = 100.0\n",
    "alpha0_ood = 10.0\n",
    "\n",
    "# Slight label smoothing improves numerical stability\n",
    "label_smoothing = 0.01\n",
    "\n",
    "\n",
    "def make_in_domain_target_alpha(\n",
    "    y: torch.Tensor,\n",
    "    num_classes: int = 10,\n",
    "    alpha0: float = alpha0_in,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Construct sharp Dirichlet targets for in-distribution samples.\"\"\"\n",
    "    batch_size = y.size(0)\n",
    "\n",
    "    # Smoothed one-hot encoding\n",
    "    mu = torch.full(\n",
    "        (batch_size, num_classes),\n",
    "        label_smoothing / (num_classes - 1),\n",
    "        device=y.device,\n",
    "    )\n",
    "    mu[torch.arange(batch_size), y] = 1.0 - label_smoothing\n",
    "\n",
    "    return mu * alpha0\n",
    "\n",
    "\n",
    "def make_ood_target_alpha(\n",
    "    batch_size: int,\n",
    "    num_classes: int = 10,\n",
    "    alpha0: float = alpha0_ood,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Construct flat Dirichlet targets for out-of-distribution samples.\"\"\"\n",
    "    mu = torch.full(\n",
    "        (batch_size, num_classes),\n",
    "        1.0 / num_classes,\n",
    "        device=device,\n",
    "    )\n",
    "    return mu * alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5839d5",
   "metadata": {},
   "source": [
    "Unified DPN Training Function\n",
    "-----------------------------\n",
    "\n",
    "Training a Dirichlet Prior Network requires jointly optimising the model on both\n",
    "in-distribution (ID) and out-of-distribution (OOD) batches. Following the Prior Networks\n",
    "formulation, the loss consists of:\n",
    "\n",
    "1. KL divergence between sharp Dirichlet ID targets and predicted Dirichlets,\n",
    "2. KL divergence between flat Dirichlet OOD targets and predicted Dirichlets,\n",
    "3. an optional cross-entropy term for classification stability.\n",
    "\n",
    "The function below implements a single training epoch that processes paired mini-batches\n",
    "from the MNIST (ID) and Fashion-MNIST (OOD) loaders. It returns the average loss for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_dirichlet(alpha_p: torch.Tensor, alpha_q: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"KL divergence KL( Dir(alpha_p) || Dir(alpha_q) ) computed per batch element.\"\"\"\n",
    "    alpha_p0 = alpha_p.sum(dim=-1, keepdim=True)\n",
    "    alpha_q0 = alpha_q.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    term1 = gammaln(alpha_p0) - gammaln(alpha_q0)\n",
    "    term2 = (gammaln(alpha_q) - gammaln(alpha_p)).sum(dim=-1, keepdim=True)\n",
    "    term3 = ((alpha_p - alpha_q) * (digamma(alpha_p) - digamma(alpha_p0))).sum(\n",
    "        dim=-1,\n",
    "        keepdim=True,\n",
    "    )\n",
    "\n",
    "    return (term1 + term2 + term3).squeeze(-1)\n",
    "\n",
    "\n",
    "def predictive_probs(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return expected categorical probabilities E[p(y|x)] from Dirichlet parameters.\"\"\"\n",
    "    alpha0 = alpha.sum(dim=-1, keepdim=True)\n",
    "    return alpha / alpha0\n",
    "\n",
    "\n",
    "def unified_dpn_train_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    id_loader: DataLoader,\n",
    "    ood_loader: DataLoader,\n",
    ") -> float:\n",
    "    \"\"\"Train the model for one epoch using paired ID and OOD mini-batches.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    ood_iter = iter(ood_loader)\n",
    "\n",
    "    for x_in_raw, y_in_raw in id_loader:\n",
    "        try:\n",
    "            x_ood_raw, _ = next(ood_iter)\n",
    "        except StopIteration:\n",
    "            ood_iter = iter(ood_loader)\n",
    "            x_ood_raw, _ = next(ood_iter)\n",
    "\n",
    "        x_in = x_in_raw.to(device)\n",
    "        y_in = y_in_raw.to(device)\n",
    "        x_ood = x_ood_raw.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # In-distribution forward pass\n",
    "        alpha_in = model(x_in)\n",
    "        alpha_target_in = make_in_domain_target_alpha(y_in)\n",
    "        kl_in = kl_dirichlet(alpha_target_in, alpha_in).mean()\n",
    "\n",
    "        # Optional cross-entropy for classification stability\n",
    "        probs_in = predictive_probs(alpha_in)\n",
    "        ce_term = F.nll_loss(torch.log(probs_in + 1e-8), y_in)\n",
    "\n",
    "        # OOD forward pass\n",
    "        alpha_ood = model(x_ood)\n",
    "        alpha_target_ood = make_ood_target_alpha(x_ood.size(0))\n",
    "        kl_ood = kl_dirichlet(alpha_target_ood, alpha_ood).mean()\n",
    "\n",
    "        # Total loss\n",
    "        loss = kl_in + kl_ood + 0.1 * ce_term\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / total_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d56784",
   "metadata": {},
   "source": [
    "Training Loop\n",
    "-------------\n",
    "\n",
    "The training loop iterates over a fixed number of epochs and applies the unified DPN\n",
    "training function defined above. For each epoch, the model is optimised using paired\n",
    "in-distribution (MNIST) and out-of-distribution (Fashion-MNIST) batches. The loop reports\n",
    "the average loss per epoch, which reflects the combined ID KL, OOD KL, and classification\n",
    "stability terms.\n",
    "\n",
    "This section instantiates the model, defines the optimiser, and performs multi-epoch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48134a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probly.models.evidential.torch as t\n",
    "\n",
    "loss = pn_loss\n",
    "enc = ConvEncoder(\n",
    "    in_channels=1,\n",
    "    conv_channels=[32, 64, 128],\n",
    "    latent_dim=256,\n",
    "    input_shape=(28, 28),\n",
    ")\n",
    "model = t.PrNetModel(encoder=enc)\n",
    "unified_evidential_train(mode=\"PrNet\", model=model, dataloader=train_loader, loss_fn=pn_loss, oodloader=ood_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cd2fa",
   "metadata": {},
   "source": [
    "Evaluation: Accuracy and Predictive Distribution\n",
    "------------------------------------------------\n",
    "\n",
    "Dirichlet Prior Networks produce concentration parameters α(x) for each class. The predictive\n",
    "categorical distribution is obtained from the expected probabilities:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[p(y \\mid x)] = \\frac{\\alpha}{\\alpha_0}, \\qquad \\alpha_0 = \\sum_k \\alpha_k.\n",
    "$$\n",
    "\n",
    "Using this predictive distribution, the network can be evaluated on standard classification\n",
    "metrics such as accuracy. This section computes the ID accuracy on the MNIST test set and returns\n",
    "the corresponding predictive probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate predictive accuracy using expected categorical probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_raw, y_raw in loader:\n",
    "            x = x_raw.to(device)\n",
    "            y = y_raw.to(device)\n",
    "\n",
    "            alpha = model(x)\n",
    "            alpha0 = alpha.sum(dim=-1, keepdim=True)\n",
    "            probs = alpha / alpha0\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Evaluate ID accuracy\n",
    "id_accuracy = evaluate_accuracy(model, test_loader)\n",
    "id_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8b235",
   "metadata": {},
   "source": [
    "Uncertainty Analysis: Predictive and Differential Entropy\n",
    "---------------------------------------------------------\n",
    "\n",
    "Dirichlet Prior Networks provide uncertainty estimates through the structure of the predicted\n",
    "Dirichlet distribution. Two complementary uncertainty measures are commonly used:\n",
    "\n",
    "1. **Predictive entropy**, defined on the expected categorical distribution  \n",
    "   $$\n",
    "   p(y\\mid x) = \\frac{\\alpha}{\\alpha_0},\n",
    "   \\qquad\n",
    "   H_\\text{pred} = -\\sum_k p_k \\log p_k,\n",
    "   $$\n",
    "   capturing total predictive uncertainty.\n",
    "\n",
    "2. **Dirichlet differential entropy**, defined on the full Dirichlet density  \n",
    "   $$\n",
    "   H_\\text{dir} = H\\bigl(\\mathrm{Dir}(\\alpha)\\bigr),\n",
    "   $$\n",
    "   capturing epistemic uncertainty through the concentration of α.\n",
    "\n",
    "ID data (MNIST) should yield low predictive and differential entropy due to high concentration\n",
    "(sharp Dirichlet), whereas OOD data (Fashion-MNIST) should yield high entropy due to low\n",
    "concentration (flat Dirichlet). We compute both measures below for ID and OOD samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute predictive entropy H(pred) = -∑ p log p from Dirichlet parameters.\"\"\"\n",
    "    alpha0 = alpha.sum(dim=-1, keepdim=True)\n",
    "    p = alpha / alpha0\n",
    "    return -(p * torch.log(p + 1e-8)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def dirichlet_differential_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute differential entropy of Dirichlet(alpha).\"\"\"\n",
    "    alpha0 = alpha.sum(dim=-1)\n",
    "    k = alpha.size(-1)\n",
    "\n",
    "    log_b = gammaln(alpha).sum(dim=-1) - gammaln(alpha0)\n",
    "    digamma_alpha = digamma(alpha)\n",
    "    digamma_alpha0 = digamma(alpha0)\n",
    "\n",
    "    return log_b + (alpha0 - k) * digamma_alpha0 - ((alpha - 1) * digamma_alpha).sum(dim=-1)\n",
    "\n",
    "\n",
    "def compute_alpha(model: nn.Module, loader: DataLoader) -> torch.Tensor:\n",
    "    \"\"\"Return (alpha)(x) for all samples in a DataLoader.\"\"\"\n",
    "    model.eval()\n",
    "    out = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_raw, _ in loader:\n",
    "            x = x_raw.to(device)\n",
    "            out.append(model(x).cpu())\n",
    "\n",
    "    return torch.cat(out, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dirichlet parameters for ID and OOD sets\n",
    "alpha_id = compute_alpha(model, test_loader)\n",
    "alpha_ood = compute_alpha(model, ood_loader)\n",
    "\n",
    "# Predictive entropy\n",
    "pred_ent_id = predictive_entropy(alpha_id)\n",
    "pred_ent_ood = predictive_entropy(alpha_ood)\n",
    "\n",
    "# Dirichlet differential entropy\n",
    "diff_ent_id = dirichlet_differential_entropy(alpha_id)\n",
    "diff_ent_ood = dirichlet_differential_entropy(alpha_ood)\n",
    "\n",
    "pred_ent_id.mean().item(), pred_ent_ood.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive entropy histograms\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(pred_ent_id.numpy(), bins=50, alpha=0.6, label=\"ID (MNIST)\")\n",
    "plt.hist(pred_ent_ood.numpy(), bins=50, alpha=0.6, label=\"OOD (Fashion-MNIST)\")\n",
    "plt.xlabel(\"Predictive Entropy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Predictive Entropy: ID vs. OOD\")\n",
    "plt.show()\n",
    "\n",
    "# Dirichlet differential entropy histograms\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(diff_ent_id.numpy(), bins=50, alpha=0.6, label=\"ID (MNIST)\")\n",
    "plt.hist(diff_ent_ood.numpy(), bins=50, alpha=0.6, label=\"OOD (Fashion-MNIST)\")\n",
    "plt.xlabel(\"Dirichlet Differential Entropy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Differential Entropy: ID vs. OOD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612579d8",
   "metadata": {},
   "source": [
    "Sanity Checks\n",
    "-------------\n",
    "\n",
    "Before using the trained Dirichlet Prior Network in downstream evaluation or deployment, several\n",
    "sanity checks ensure that the model behaves according to the Prior Networks formulation.\n",
    "\n",
    "These checks verify that:\n",
    "\n",
    "1. the predicted concentration parameters α(x) are strictly positive,\n",
    "2. in-distribution samples produce higher concentration (α₀) than OOD samples,\n",
    "3. predictive entropy is lower for ID samples than for OOD samples,\n",
    "4. differential entropy is higher for OOD samples, reflecting epistemic uncertainty,\n",
    "5. the predicted class probabilities correspond to the expected categorical mean α/α₀.\n",
    "\n",
    "The following cells compute summary statistics and basic assertions to validate model behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1: (alpha)(x) > 0 for all samples\n",
    "min_alpha_id = alpha_id.min().item()\n",
    "min_alpha_ood = alpha_ood.min().item()\n",
    "\n",
    "# alpha_id must be strictly positive for a valid Dirichlet distribution\n",
    "if min_alpha_id <= 0.0:\n",
    "    msg = \"alpha_id is non-positive\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "# alpha_ood must be strictly positive for a valid Dirichlet distribution\n",
    "if min_alpha_ood <= 0.0:\n",
    "    msg = \"alpha_ood is non-positive\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "print(\"Minimum alpha (ID):\", min_alpha_id)\n",
    "print(\"Minimum alpha (OOD):\", min_alpha_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentration α₀ = Σ_k (alpha)_k\n",
    "alpha0_id = alpha_id.sum(dim=1)\n",
    "alpha0_ood = alpha_ood.sum(dim=1)\n",
    "\n",
    "print(\"Mean α₀ (ID): \", alpha0_id.mean().item())\n",
    "print(\"Mean α₀ (OOD):\", alpha0_ood.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictive entropy (ID, mean): \", pred_ent_id.mean().item())\n",
    "print(\"Predictive entropy (OOD, mean):\", pred_ent_ood.mean().item())\n",
    "\n",
    "print(\"Differential entropy (ID, mean): \", diff_ent_id.mean().item())\n",
    "print(\"Differential entropy (OOD, mean):\", diff_ent_ood.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD detection AUC using predictive entropy\n",
    "y_true = np.concatenate(\n",
    "    [\n",
    "        np.zeros_like(pred_ent_id.numpy()),\n",
    "        np.ones_like(pred_ent_ood.numpy()),\n",
    "    ],\n",
    ")\n",
    "y_scores = np.concatenate(\n",
    "    [\n",
    "        pred_ent_id.numpy(),\n",
    "        pred_ent_ood.numpy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "print(f\"OOD AUC (predictive entropy): {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f5d7c",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "This notebook presented a complete implementation of Dirichlet Prior Networks (DPNs) following\n",
    "the formulation introduced by Malinin & Gales (2018). The model produces Dirichlet\n",
    "concentration parameters for each input, enabling principled uncertainty estimation via:\n",
    "\n",
    "- sharp Dirichlet distributions for in-distribution samples,\n",
    "- flat Dirichlet distributions for out-of-distribution samples,\n",
    "- KL divergence between target and predicted Dirichlet distributions.\n",
    "\n",
    "The experiments demonstrated the characteristic behaviour of DPNs: low predictive and\n",
    "differential entropy for MNIST (ID) and high entropy for Fashion-MNIST (OOD). The included\n",
    "sanity checks verified that the model adheres to the expected properties of Prior Networks.\n",
    "\n",
    "The implementation presented here serves as a reference for applying Dirichlet Prior Networks\n",
    "in settings that require calibrated predictive uncertainty and explicit modelling of\n",
    "out-of-distribution behaviour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
