{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6951c7b9-605e-480d-9844-b35f56edbcef",
   "metadata": {},
   "source": [
    "# Understanding Prior Networks and Regression Prior Networks\n",
    "   ### A Practical and Mathematical Guide to Uncertainty Modeling in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5e408-9234-4b32-92ec-c6318924f296",
   "metadata": {},
   "source": [
    "##  1. Introduction\n",
    "\n",
    "Deep learning models are powerful, but standard neural networks cannot express uncertainty about their predictions. In many applicationsâ€”such as self-driving cars, medical diagnosis, or other safety-critical systemsâ€”it is important to know *how confident* a model is in its output.\n",
    "\n",
    "Traditionally, **neural network ensembles** are used to estimate uncertainty because they capture both:\n",
    "- **Data uncertainty (aleatoric uncertainty)** â†’ noise inherent in the data\n",
    "- **Knowledge uncertainty (epistemic uncertainty)** â†’ model ignorance, OOD inputs\n",
    "\n",
    "However, ensembles are **computationally expensive**, because they require running **many neural networks** at inference time. This makes them impractical for real-world systems that need fast predictions.\n",
    "\n",
    "To solve this, **Prior Networks** were introduced for **classification tasks**. Instead of predicting class probabilities directly (e.g., through a softmax layer), Prior Networks predict the parameters of a **Dirichlet distribution**, allowing a single model to mimic the uncertainty behavior of an entire ensemble.\n",
    "\n",
    "This idea was later extended to continuous outputs in **Regression Prior Networks (Malinin et al., 2020)**, which use a **Normalâ€“Wishart prior** and produce **Student-t predictive distributions**. These models allow single-network uncertainty estimation for regression problems.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Understand why ensembles provide strong uncertainty\n",
    "- Explain why ensembles are expensive at inference\n",
    "- Show how Prior Networks solve the ensemble cost problem\n",
    "- Explain how **classification Prior Networks** work (Dirichlet priors)\n",
    "- Explain how **Regression Prior Networks** work (Normalâ€“Wishart priors â†’ Student-t output)\n",
    "- Show how this relates to **Evidential Deep Learning (Sensoy et al., 2018)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbb708-f3f1-47a7-89c4-b0d81cdf6d1c",
   "metadata": {},
   "source": [
    "## 2. Ensembles and Why They Matter\n",
    "\n",
    "Deep learning ensembles combine several independently trained models:\n",
    "\n",
    "\n",
    "$ f^{(1)}, f^{(2)}, \\dots, f^{(M)}$\n",
    "\n",
    "\n",
    "Ensembles capture two types of uncertainty:\n",
    "\n",
    "### ðŸ”¹ Data Uncertainty (Aleatoric)\n",
    "Noise in the data.\n",
    "\n",
    "### ðŸ”¹ Knowledge Uncertainty (Epistemic)\n",
    "Model uncertainty due to lack of knowledge.\n",
    "\n",
    "Ensembles are powerful but computationally expensive:\n",
    "\n",
    "- $MÃ—$ compute\n",
    "- $MÃ—$ memory\n",
    "- $MÃ—$ inference time\n",
    "\n",
    " but we need something cheaper that still captures uncertainty.\n",
    "\n",
    "This leads to **Prior Networks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807721b-ab58-4cdf-aa1b-3169d8484efe",
   "metadata": {},
   "source": [
    "## 3. Prior Networks for Classification (Dirichlet Prior Networks)\n",
    "\n",
    "Prior Networks were first introduced for **classification tasks** (Malinin & Gales, 2019).\n",
    "Instead of predicting class probabilities directly using softmax, a Prior Network predicts the\n",
    "parameters of a **Dirichlet distribution**:\n",
    "\n",
    "$\n",
    "p(\\mathbf{p} \\mid x) = \\mathrm{Dirichlet}(\\alpha_1(x), \\dots , \\alpha_K(x))\n",
    "$\n",
    "\n",
    "Where:\n",
    "- Each $ \\alpha_k\\ $  represents **evidence** for class \\(k\\).\n",
    "- High $ \\alpha_k\\ $  â†’ strong belief â†’ low epistemic uncertainty\n",
    "- Low $ \\alpha_k\\ $  â†’ weak belief â†’ high epistemic uncertainty\n",
    "\n",
    "## ðŸ”¹ Why Dirichlet?\n",
    "\n",
    "The Dirichlet distribution is the **conjugate prior** of the categorical distribution.\n",
    "This makes it ideal for classification because it models *distributions over class probabilities*.\n",
    "\n",
    "## ðŸ”¹ Predictive Distribution\n",
    "\n",
    "Instead of directly predicting $p(y \\mid x)$, a Prior Network computes:\n",
    "\n",
    "\n",
    "$ p(y \\mid x) = \\int p(y \\mid \\mathbf{p}) \\, p(\\mathbf{p} \\mid x) \\, d\\mathbf{p}$\n",
    "\n",
    "\n",
    "This allows clean decomposition into:\n",
    "\n",
    "### âœ” Total uncertainty\n",
    "### âœ” Data (aleatoric) uncertainty\n",
    "### âœ” Knowledge (epistemic) uncertainty\n",
    "\n",
    "## ðŸ”¹ Why is this useful?\n",
    "\n",
    "âž¡ A single Prior Network can capture **ensemble-like uncertainty**\n",
    "âž¡ Without needing to run an ensemble at inference time\n",
    "âž¡ Saving compute, memory, and latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd4a2-1ee8-4cc9-b3c4-ef40f7e47552",
   "metadata": {},
   "source": [
    "## 4. Transition From Classification to Regression Prior Networks\n",
    "\n",
    "Dirichlet Prior Networks work extremely well for classification.\n",
    "\n",
    "BUT\n",
    "\n",
    "Regression tasks deal with **continuous outputs**, not discrete class probabilities.\n",
    "\n",
    "So we need:\n",
    "- A predictive likelihood: **Normal distribution**\n",
    "- A prior over Normal parameters (mean + precision)\n",
    "\n",
    "The correct conjugate prior to a Normal distribution is:\n",
    "\n",
    "âž¡ **Normalâ€“Wishart distribution**\n",
    "\n",
    "This leads directly to:\n",
    "\n",
    "âœ” Regression Prior Networks\n",
    "âœ” Continuous uncertainty modeling\n",
    "âœ” Student-t predictive distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53133db1-7732-45a2-b110-df09d57abb9c",
   "metadata": {},
   "source": [
    "## 5. Regression Prior Networks (RPNs)\n",
    "\n",
    "Regression Prior Networks generalize Prior Networks to **continuous-valued predictions**.\n",
    "\n",
    "### Step 1 â€” Use a Normal likelihood\n",
    "\n",
    "A normal regression model predicts:\n",
    "\n",
    "\n",
    "$p(y \\mid x, \\mu, \\Lambda) = \\mathcal{N}(y \\mid \\mu, \\Lambda^{-1})$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\mu\\ $ = predicted mean\n",
    "- $ \\Lambda\\ $ = precision (inverse covariance matrix)\n",
    "\n",
    "### Step 2 â€” Predict a *distribution* over (\\mu, \\Lambda)\n",
    "\n",
    "RPNs do NOT predict a single Normal distribution.\n",
    "They predict a **Normalâ€“Wishart prior** over the regression parameters:\n",
    "\n",
    "\n",
    "$(\\mu, \\Lambda) \\sim \\mathrm{NormalWishart}(m, L, \\kappa, \\nu)$\n",
    "\n",
    "Where:\n",
    "- $ m $ = prior mean\n",
    "- $ L $ = prior precision structure\n",
    "- $ \\kappa\\ $ = belief strength in $ m $\n",
    "- $ \\nu\\ $ = belief strength in $ L $\n",
    "\n",
    "These control epistemic uncertainty.\n",
    "\n",
    "### Step 3 â€” Predictive distribution is a Student-t\n",
    "\n",
    "When integrating out the uncertainty in $ \\mu\\ $ and $ \\Lambda\\ $:\n",
    "\n",
    "$\n",
    "p(y \\mid x) = \\mathrm{Student\\text{-}t}(y)\n",
    "$\n",
    "\n",
    "Why Student-t?\n",
    "- Because **Normal likelihood + Normalâ€“Wishart prior = Student-t**.\n",
    "- Student-t has **heavier tails**, which naturally capture uncertainty.\n",
    "\n",
    "### Benefits of Student-t\n",
    "\n",
    "âœ” Detects OOD inputs\n",
    "âœ” Captures both aleatoric + epistemic uncertainty\n",
    "âœ” More robust than a Gaussian\n",
    "\n",
    "## 5.1 The Student-t Distribution\n",
    "\n",
    "When a Regression Prior Network predicts uncertainty, it does not produce a simple Normal\n",
    "distribution. Instead, it produces a **Student-t distribution**, which is the result of:\n",
    "\n",
    "$\n",
    "\\text{Normal likelihood} + \\text{Normalâ€“Wishart prior} \\;\\Rightarrow\\; \\text{Student-t predictive distribution}\n",
    "$\n",
    "\n",
    "This is a fundamental Bayesian identity.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why Student-t instead of Normal?\n",
    "\n",
    "A Normal distribution assumes:\n",
    "- fixed variance\n",
    "- no epistemic uncertainty\n",
    "\n",
    "But in real-world scenarios:\n",
    "- the mean (Î¼) is uncertain\n",
    "- the variance (ÏƒÂ²) is uncertain\n",
    "- the model is uncertain about its parameters\n",
    "\n",
    "A Student-t distribution has *heavier tails*, meaning:\n",
    "\n",
    "- more probability in extreme values\n",
    "- it naturally expresses **model uncertainty**\n",
    "- it becomes wider when the model lacks knowledge\n",
    "- it shrinks toward a Normal distribution when confident\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Shape of a Student-t distribution\n",
    "\n",
    "The parameter **Î½** (nu, degrees of freedom) controls how heavy the tails are:\n",
    "\n",
    "- Small Î½ â†’ *very heavy tails* (uncertain)\n",
    "- Medium Î½ â†’ *moderately heavy* (some uncertainty)\n",
    "- Large Î½ â†’ approaches a Normal distribution (confident)\n",
    "\n",
    "Examples:\n",
    "- Î½ = 1 â†’ Cauchy distribution (extremely heavy-tailed)\n",
    "- Î½ = 3 â†’ high uncertainty\n",
    "- Î½ = 30 â†’ almost Normal\n",
    "- Î½ â†’ âˆž â†’ Normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why is this important for RPNs?\n",
    "\n",
    "Because Student-t naturally captures the behavior of an ensemble:\n",
    "\n",
    "- if models disagree â†’ tail gets heavier â†’ epistemic uncertainty rises\n",
    "- if data is noisy â†’ variance stays large â†’ aleatoric uncertainty\n",
    "- if model is confident â†’ distribution becomes narrow & Gaussian\n",
    "\n",
    "Thus, Student-t is the *perfect* predictive distribution for expressing uncertainty in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c684817-f835-4147-863a-4a63bc9f34d7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "# Values to plot\n",
    "x = np.linspace(-8, 8, 400)\n",
    "\n",
    "# Different degrees of freedom\n",
    "t1 = t.pdf(x, df=1)  # very heavy-tailed\n",
    "t3 = t.pdf(x, df=3)  # moderate uncertainty\n",
    "t30 = t.pdf(x, df=30)  # almost normal\n",
    "\n",
    "normal = norm.pdf(x, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t1, label=\"Student-t (v=1)\", linestyle=\"--\")\n",
    "plt.plot(x, t3, label=\"Student-t (v=3)\", linestyle=\"--\")\n",
    "plt.plot(x, t30, label=\"Student-t (v=30)\", linestyle=\"--\")\n",
    "plt.plot(x, normal, label=\"Normal (mu=0, sigma=1)\", linewidth=2)\n",
    "\n",
    "plt.title(\"Student-t vs Normal Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37d7a-a1a5-49c6-bbd9-f7e1bdbab532",
   "metadata": {},
   "source": [
    "## 6. Ensemble Distribution Distillation (EnDÂ²)\n",
    "\n",
    "Neural network ensembles are great for uncertainty, but expensive.\n",
    "RPNs support **Ensemble Distribution Distillation (EnDÂ²)** to learn ensemble behavior\n",
    "without needing to run an ensemble at inference.\n",
    "\n",
    "### How EnDÂ² works\n",
    "\n",
    "1. Train an ensemble of regression models\n",
    "2. Collect their predicted $ (\\muáµ, \\Lambdaáµ)$  values\n",
    "3. Treat these values as samples from an empirical distribution\n",
    "4. Train ONE Regression Prior Network to *match this distribution*\n",
    "5. Use **temperature annealing**:\n",
    "   - High T â†’ learn the ensemble mean\n",
    "   - Low T â†’ learn full ensemble diversity (variance, disagreement)\n",
    "\n",
    "### Why this is important?\n",
    "\n",
    "âœ” The RPN learns:\n",
    "- Ensemble mean\n",
    "- Ensemble variance\n",
    "- Ensemble disagreement\n",
    "\n",
    "âž¤ But runs as **one single model** at inference time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c72a0-682f-47a9-8e3c-270b20598619",
   "metadata": {},
   "source": [
    "## 7. Implementation pipeline for Regression Prior Networks (RPN) â€” Ensemble-Based Distillation\n",
    "  we will now implements a complete Regression Prior Network (RPN) using an\n",
    "**ensemble of probabilistic regression models** as the teacher.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "1. **Train an ensemble**\n",
    "   Each of the K models outputs a Normal distribution\n",
    "   â†’ Î¼â‚–(x), Ïƒâ‚–Â²(x)\n",
    "\n",
    "2. **Collect ensemble distributions**\n",
    "   Aggregate {Î¼â‚–(x), Ïƒâ‚–Â²(x)} from all models\n",
    "\n",
    "3. **Regression Prior Network (RPN)**\n",
    "   RPN outputs the parameters of a Normal-Wishart distribution:\n",
    "   - m(x)\n",
    "   - L(x)\n",
    "   - Îº(x)\n",
    "   - Î½(x)\n",
    "\n",
    "4. **Distillation Loss**\n",
    "   Match the RPN Normal-Wishart distribution to the ensemble's empirical distribution\n",
    "\n",
    "5. **Predictive Student-t distribution**\n",
    "   - total uncertainty\n",
    "   - aleatoric uncertainty\n",
    "   - epistemic uncertainty\n",
    "\n",
    "We will implement everything step by step, with visual tests at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5696eaf-6a79-4ea7-9791-b20a0261f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d516294-1e6f-476d-8755-b226836bb1c9",
   "metadata": {},
   "source": [
    "### 1. Ensemble Probabilistic Regression Models\n",
    "\n",
    "Each ensemble model predicts a **Normal distribution**:\n",
    "\n",
    "$\n",
    "y | x \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))\n",
    "$\n",
    "\n",
    "For K ensemble members, we obtain:\n",
    "\n",
    "$\n",
    "\\{ (\\mu_k(x), \\sigma_k^2(x)) \\}_{k=1..K}\n",
    "$\n",
    "\n",
    "These Normal distributions will later be distilled into a Normal-Wishart distribution in the RPN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456e302-f2fd-470e-9827-40f0e6355c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    \"\"\"Probabilistic regression model used for building the ensemble.\n",
    "\n",
    "    This model outputs:\n",
    "        - mu(x): predicted mean\n",
    "        - log_var(x): predicted log-variance (to ensure positivity).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the regression model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Size of the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mu_head = nn.Linear(hidden_dim, 1)\n",
    "        self.log_var_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - mu: predicted mean\n",
    "                - log_var: predicted log-variance (var = exp(log_var))\n",
    "        \"\"\"\n",
    "        h = self.feature(x)\n",
    "        mu = self.mu_head(h)\n",
    "        log_var = self.log_var_head(h)\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24277ef8-070a-4186-9385-40e342c2fb42",
   "metadata": {},
   "source": [
    "### 2. Gaussian Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e021a-5ec4-4718-a022-1315218d94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(\n",
    "    mu: torch.Tensor,\n",
    "    log_var: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Gaussian negative log-likelihood.\n",
    "\n",
    "    Computes the NLL for a Gaussian with predicted mean and log-variance:\n",
    "        0.5 * [ log(sigma^2) + (y - mu)^2 / sigma^2 ]\n",
    "    \"\"\"\n",
    "    var = torch.exp(log_var)\n",
    "    return 0.5 * (log_var + (y - mu) ** 2 / var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a635e99-a3e0-47c6-9bc6-6b01929d3c72",
   "metadata": {},
   "source": [
    "### 3. Train a Single Ensemble Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba963fc-e77f-46cf-b9b1-fd2d7af7a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(\n",
    "    model: nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Train a single regression model using Gaussian NLL loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The regression model to train.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in loader:\n",
    "            mu, log_var = model(x)\n",
    "            loss = gaussian_nll(mu, log_var, y).mean()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss={total_loss / len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66032f9-7232-4961-b30e-349f12f5a9fa",
   "metadata": {},
   "source": [
    "### 4. Build & Train the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fbae5-7b61-45b0-8e60-a64686bc8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble(k: int, input_dim: int) -> list[RegressionModel]:\n",
    "    \"\"\"Build an ensemble of probabilistic regression models.\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of ensemble members.\n",
    "        input_dim (int): Number of input features.\n",
    "\n",
    "    Returns:\n",
    "        list[RegressionModel]: List of trained ensemble models.\n",
    "    \"\"\"\n",
    "    return [RegressionModel(input_dim) for _ in range(k)]\n",
    "\n",
    "\n",
    "def train_ensemble(\n",
    "    ensemble: list[RegressionModel],\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 20,\n",
    ") -> list[RegressionModel]:\n",
    "    \"\"\"Train all models in the ensemble.\n",
    "\n",
    "    Args:\n",
    "        ensemble (list[RegressionModel]): List of models to train.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        list[RegressionModel]: List of trained ensemble models.\n",
    "    \"\"\"\n",
    "    trained = []\n",
    "\n",
    "    for i, model in enumerate(ensemble):\n",
    "        print(f\"\\nTraining Ensemble Model {i + 1}/{len(ensemble)}\")\n",
    "        trained_model = train_single_model(model, loader, epochs)\n",
    "        trained.append(trained_model)\n",
    "\n",
    "    return trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf599362-cd42-4974-9dd4-e2608821ea65",
   "metadata": {},
   "source": [
    "### 5. Synthetic Dataset for training the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a5aa4-cd2c-4ec5-97cc-c326618f8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "y = torch.sin(x) + 0.2 * torch.randn_like(x)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d05b03-8f42-44c2-a022-f502851ed91c",
   "metadata": {},
   "source": [
    "### 6. Train the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe7a20-b231-4b52-b4bf-59d74f0c8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = build_ensemble(k=5, input_dim=1)\n",
    "ensemble = train_ensemble(ensemble, loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753aacb-c0f9-49db-8ee2-40541368c14c",
   "metadata": {},
   "source": [
    "### 7. Collect Î¼â‚– and Ïƒâ‚–Â², this are the Ensemble Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab8922-6be0-477b-aaff-fd15f0a05cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_distributions(\n",
    "    ensemble: list[RegressionModel],\n",
    "    x: torch.Tensor,\n",
    ") -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n",
    "    \"\"\"Compute ensemble predictive distributions.\n",
    "\n",
    "    Args:\n",
    "        ensemble (list[RegressionModel]): List of trained ensemble models.\n",
    "        x (torch.Tensor): Input batch to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[torch.Tensor], list[torch.Tensor]]:\n",
    "            - List of predicted means from each ensemble member.\n",
    "            - List of predicted variances from each ensemble member.\n",
    "    \"\"\"\n",
    "    mus: list[torch.Tensor] = []\n",
    "    vars_: list[torch.Tensor] = []\n",
    "\n",
    "    for model in ensemble:\n",
    "        model.eval()\n",
    "        mu, log_var = model(x)\n",
    "        mus.append(mu.detach())\n",
    "        vars_.append(torch.exp(log_var.detach()))\n",
    "\n",
    "    return mus, vars_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab60d-fbe8-48b4-9562-bb28949a9e55",
   "metadata": {},
   "source": [
    "### 8. Regression Prior Network (RPN)\n",
    "\n",
    "The RPN outputs parameters of a **Normal-Wishart distribution**, which is a\n",
    "distribution *over Normal distributions from the Ensemble*:\n",
    "\n",
    "- m(x) : prior mean for Î¼\n",
    "- L(x) : precision parameter\n",
    "- Îº(x) : evidence about Î¼\n",
    "- Î½(x) : evidence about Î£\n",
    "\n",
    "From the Normal-Wishart parameters, the predictive distribution becomes a\n",
    "Student-t distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4d7a9-4635-442f-8531-9c41d96f2ed2",
   "metadata": {},
   "source": [
    "### 9. RPN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14ad2a-3a96-455d-a022-3fa9e477d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionPriorNetwork(nn.Module):\n",
    "    \"\"\"Regression Prior Network: outputs Normal-Wishart parameters.\n",
    "\n",
    "    This implementation uses a univariate Normal-Wishart distribution for\n",
    "    evidential regression. It predicts the four parameters:\n",
    "    - m(x): location parameter (prior mean)\n",
    "    - l_precision(x): precision (must be > 0)\n",
    "    - kappa(x): strength of belief in m\n",
    "    - nu(x): degrees of freedom (> 2, controls heaviness of Student-t tails).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the Regression Prior Network.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Hidden layer width.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Heads for the four Normal-Wishart parameters\n",
    "        self.m_head = nn.Linear(hidden_dim, 1)\n",
    "        self.l_precision_head = nn.Linear(hidden_dim, 1)\n",
    "        self.kappa_head = nn.Linear(hidden_dim, 1)\n",
    "        self.nu_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Forward pass for predicting Normal-Wishart parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - m (torch.Tensor): Prior mean.\n",
    "                - l_precision (torch.Tensor): Precision (> 0).\n",
    "                - kappa (torch.Tensor): Strength (> 0).\n",
    "                - nu (torch.Tensor): Degrees of freedom (> 2).\n",
    "        \"\"\"\n",
    "        h = self.feature(x)\n",
    "\n",
    "        m = self.m_head(h)\n",
    "\n",
    "        # Rename L â†’ l_precision to satisfy Ruff rule N806\n",
    "        l_precision = torch.exp(self.l_precision_head(h))  # must be > 0\n",
    "\n",
    "        # kappa must be strictly positive\n",
    "        kappa = F.softplus(self.kappa_head(h)) + 1e-3\n",
    "\n",
    "        # nu must be > 2 to define a valid Student-t distribution\n",
    "        nu = F.softplus(self.nu_head(h)) + 3.0\n",
    "\n",
    "        return m, l_precision, kappa, nu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a26655-9884-47f8-8acc-62844cb8a07b",
   "metadata": {},
   "source": [
    "### 10. Distillation Loss\n",
    "\n",
    "We distill the ensemble Normal distributions\n",
    "$\n",
    "(\\mu_k, \\sigma_k^2)\n",
    "$\n",
    "into a single Normal-Wishart distribution predicted by the RPN.\n",
    "\n",
    "Loss:\n",
    "$\n",
    "L = -\\frac{1}{K}\\sum_k \\log p_{NW}(\\mu_k, \\sigma_k^2)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e0d6e-7728-4bcd-82ef-76a6f2a0cb95",
   "metadata": {},
   "source": [
    "### 11. Normal-Wishart Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10892afd-2382-438a-962a-ea17aea36626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def normal_wishart_log_prob(\n",
    "    m: Tensor,\n",
    "    l_precision: Tensor,\n",
    "    kappa: Tensor,\n",
    "    nu: Tensor,\n",
    "    mu_k: Tensor,\n",
    "    sigma2_k: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute simplified univariate Normal-Wishart log-likelihood.\n",
    "\n",
    "    Args:\n",
    "        m (Tensor): Prior mean parameter.\n",
    "        l_precision (Tensor): Precision (> 0), formerly `L`.\n",
    "        kappa (Tensor): Strength parameter (> 0).\n",
    "        nu (Tensor): Degrees of freedom (> 2).\n",
    "        mu_k (Tensor): Sample mean from ensemble.\n",
    "        sigma2_k (Tensor): Sample variance from ensemble.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Log-likelihood under the Normal-Wishart model.\n",
    "    \"\"\"\n",
    "    # Likelihood of ensemble mean under Normal prior for mean\n",
    "    log_p_mu = -0.5 * kappa * l_precision * (mu_k - m) ** 2\n",
    "\n",
    "    # Likelihood of variance under Wishart prior on precision\n",
    "    log_p_sigma = 0.5 * (nu - 1) * torch.log(l_precision) - 0.5 * nu * (sigma2_k * l_precision)\n",
    "\n",
    "    return log_p_mu + log_p_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf6f59-6712-400e-abae-237e14336fa0",
   "metadata": {},
   "source": [
    "### 12. Distillation Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0538bc-7b2f-4dae-badb-19600f32668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def rpn_distillation_loss(\n",
    "    rpn_params: tuple[Tensor, Tensor, Tensor, Tensor],\n",
    "    mus: list[Tensor],\n",
    "    variances: list[Tensor],\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute the distillation loss for Regression Prior Networks (RPN).\n",
    "\n",
    "    This loss measures how well the RPN's Normal-Wishart distribution\n",
    "    matches the empirical ensemble distributions (mu_k, var_k).\n",
    "\n",
    "    Args:\n",
    "        rpn_params (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "            The RPN output parameters (m, l_precision, kappa, nu).\n",
    "        mus (list[Tensor]): Ensemble predicted means.\n",
    "        variances (list[Tensor]): Ensemble predicted variances.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss value.\n",
    "    \"\"\"\n",
    "    m, l_precision, kappa, nu = rpn_params  # formerly \"L\"\n",
    "\n",
    "    losses: list[Tensor] = []\n",
    "\n",
    "    for mu_k, var_k in zip(mus, variances, strict=False):\n",
    "        log_prob = normal_wishart_log_prob(\n",
    "            m,\n",
    "            l_precision,\n",
    "            kappa,\n",
    "            nu,\n",
    "            mu_k,\n",
    "            var_k,\n",
    "        )\n",
    "        losses.append(-log_prob.mean())  # negative log-likelihood\n",
    "\n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28360fe5-dd29-47f5-894b-827bff96602f",
   "metadata": {},
   "source": [
    "### 13. RPN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f1d51-1e49-4368-98f6-4a20604349d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_rpn(\n",
    "    rpn: RegressionPriorNetwork,\n",
    "    ensemble: list[RegressionModel],\n",
    "    loader: DataLoader,\n",
    "    epochs: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"Train the Regression Prior Network (RPN) using ensemble distillation.\n",
    "\n",
    "    Args:\n",
    "        rpn (RegressionPriorNetwork): The RPN model to train.\n",
    "        ensemble (list[RegressionModel]): Ensemble of trained regression models.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(rpn.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, _ in loader:\n",
    "            mus, vars_ = get_ensemble_distributions(ensemble, x)\n",
    "\n",
    "            rpn_params = rpn(x)\n",
    "            loss = rpn_distillation_loss(rpn_params, mus, vars_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: RPN Loss={total_loss / len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53433f-d48c-4f23-bece-3a6fc3c49d6c",
   "metadata": {},
   "source": [
    "### 14. Train the RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2305717-c814-4cbc-a4be-ebcff78db338",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = RegressionPriorNetwork(input_dim=1)\n",
    "train_rpn(rpn, ensemble, loader, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32e353-8144-49da-9b9e-8d4d0c672eba",
   "metadata": {},
   "source": [
    "### 15. Student-t Predictive Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1762cd-bbbe-47d5-adc6-c1c59b2a9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def student_t_prediction(\n",
    "    m: Tensor,\n",
    "    l_precision: Tensor,\n",
    "    kappa: Tensor,\n",
    "    nu: Tensor,\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Compute predictive Student-t distribution parameters from Normal-Wishart posterior.\n",
    "\n",
    "    Args:\n",
    "        m (Tensor): Posterior mean parameter.\n",
    "        l_precision (Tensor): Posterior precision parameter (> 0).\n",
    "        kappa (Tensor): Posterior scaling parameter (> 0).\n",
    "        nu (Tensor): Posterior degrees of freedom (> 2).\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor, Tensor]:\n",
    "            - mean of Student-t distribution\n",
    "            - variance of Student-t distribution\n",
    "            - degrees of freedom (df)\n",
    "    \"\"\"\n",
    "    df = nu - 1\n",
    "    var = (kappa + 1) / (kappa * l_precision * df)\n",
    "\n",
    "    return m, var, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d2b09-f267-4d3a-91bf-a55e5dd17352",
   "metadata": {},
   "source": [
    "### 16. RPN Output testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f705f-78b1-4722-a93c-ae34d1408c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "m, L, kappa, nu = rpn(x_test)\n",
    "\n",
    "m_pred, var_pred, df = student_t_prediction(m, L, kappa, nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85672b9f-1058-4517-bb0b-fc7016699127",
   "metadata": {},
   "source": [
    "### 17. Plotten von Mean & Unsicherheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec87cdc-f6b8-4a9c-96f7-26be8f315c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Convert x and y to numpy for plotting\n",
    "plt.plot(\n",
    "    x.detach().cpu().numpy(),\n",
    "    y.detach().cpu().numpy(),\n",
    "    \"k.\",\n",
    "    alpha=0.3,\n",
    "    label=\"Data\",\n",
    ")\n",
    "\n",
    "# RPN mean\n",
    "plt.plot(\n",
    "    x_test.detach().cpu().numpy(),\n",
    "    m_pred.detach().cpu().numpy(),\n",
    "    \"b-\",\n",
    "    label=\"RPN mean\",\n",
    ")\n",
    "\n",
    "# Standard deviation\n",
    "std = var_pred.sqrt()\n",
    "\n",
    "# Fill between (convert EVERYTHING to numpy)\n",
    "plt.fill_between(\n",
    "    x_test.squeeze().detach().cpu().numpy(),\n",
    "    (m_pred - 2 * std).squeeze().detach().cpu().numpy(),\n",
    "    (m_pred + 2 * std).squeeze().detach().cpu().numpy(),\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"Â±2 std RPN\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"RPN Prediction + Uncertainty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95e013-1095-415d-9860-4f8c80e91f7d",
   "metadata": {},
   "source": [
    "## 8. Comparison: Regression Prior Networks vs Evidential Deep Learning\n",
    "\n",
    "| Aspect | Regression Prior Networks (RPN) | Evidential Deep Learning (EDL) |\n",
    "|--------|----------------------------------|--------------------------------|\n",
    "| Task Type | Regression | Classification |\n",
    "| Prior Distribution | Normalâ€“Wishart | Dirichlet |\n",
    "| Predictive Output | Student-t | Categorical |\n",
    "| Evidence Concept | (m, L, Îº, Î½) | Î± (Dirichlet evidence) |\n",
    "| Uncertainty | Aleatoric + Epistemic | Aleatoric + Epistemic |\n",
    "| Independence | No sampling needed | No sampling needed |\n",
    "| Ensemble Behavior | Explicitly distillable (EnDÂ²) | Implicit via evidence |\n",
    "| Conjugate Structure | Strong Bayesian foundation | Subjective logic |\n",
    "\n",
    "### Key insights\n",
    "\n",
    "- Both aim to model **uncertainty** realistically.\n",
    "- Both use **prior distributions** over predictive parameters.\n",
    "- RPN extends this idea to **continuous regression outputs**.\n",
    "- EDL introduced \"evidence\" concepts that inspired RPN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a12da-e5e4-4568-933a-0c36c7d8d622",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "- Ensembles give excellent uncertainty estimates but are computationally expensive.\n",
    "- Prior Networks solve this problem by learning a *prior distribution* over predictions.\n",
    "- Classification Prior Networks use the **Dirichlet distribution**.\n",
    "- Regression Prior Networks generalize this using the **Normalâ€“Wishart prior**.\n",
    "- The resulting predictive distribution is a **Student-t**, ideal for uncertainty modeling.\n",
    "- With **EnDÂ²**, RPNs can learn the full behavior of ensembles with only one model.\n",
    "- These ideas are closely related to Evidential Deep Learning but adapted for regression.\n",
    "\n",
    "This hereby concludes the theoretical explanation of Regression Prior Networks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
