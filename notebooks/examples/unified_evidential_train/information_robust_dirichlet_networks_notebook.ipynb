{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad65fcd7",
   "metadata": {},
   "source": [
    "# Information Robust Dirichlet Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb53a5",
   "metadata": {},
   "source": [
    "In this notebook, we implement the specialized training loss proposed in the paper _Information Robust Dirichlet Networks for Predictive Uncertainty Estimation_ by Tsiligkaridis (2019). The method models predictive uncertainty by having a neural network output Dirichlet concentration parameters ð›¼ instead of just a pointwise softmax.\n",
    "\n",
    "The total loss is composed of three terms:\n",
    "\n",
    "1. Calibration term: implemented in the function  lp_fn\n",
    "2. Regularization term: implemented in the function  regularization_fn\n",
    "3. Adversiarial Entropy penalty: implemented in the function  dirichlet_entropy\n",
    "\n",
    "In the paper and in this notenbook, L_p loss is not directly computed but rather an upper bound for it, denoted by F_i (for sample i)  \n",
    "\n",
    "The regularization term penalizes high alpha values for incorrect classes.  \n",
    "\n",
    "The final term uses the alpha values the model assigns to adversarial inputs.\n",
    "The model is rewarded for outputting a Dirichlet-distribution with high entropy on these inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20a7ec",
   "metadata": {},
   "source": [
    "### 1. The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch.special import digamma\n",
    "\n",
    "\n",
    "def lp_fn(alpha: torch.Tensor, y: torch.Tensor, p: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"Compute the Lp calibration loss (upper bound Fi).\n",
    "\n",
    "    Computes F_i using the expectation-based formulation:\n",
    "        F_i = ( E[(1-p_c)^p] + Î£_{jâ‰ c} E[p_j^p] )^(1/p)\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B, K), must be > 0\n",
    "        y: One-hot encoded labels, shape (B, K)\n",
    "        p: Lp norm exponent (default: 2.0)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss summed over batch\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If alpha contains non-positive values or shapes don't match\n",
    "    \"\"\"\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    B, K = alpha.shape  # noqa: N806\n",
    "\n",
    "    # total concentration alpha0\n",
    "    alpha0 = alpha.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # extract alpha_c (correct class)\n",
    "    alpha_c = (alpha * y).sum(dim=1, keepdim=True)  # (B,1)\n",
    "    alpha0_minus_c = alpha0 - alpha_c  # (B,1)\n",
    "\n",
    "    # log B(a,b) used for expectations: E[X^p] = B(a+p,b)/B(a,b)\n",
    "    def log_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "    # E[(1 - p_c)^p]   where (1 - p_c) ~ Beta( alpha0 - alpha_c , alpha_c )\n",
    "    log_e1 = log_b(alpha0_minus_c + p, alpha_c) - log_b(alpha0_minus_c, alpha_c)\n",
    "    e1 = torch.exp(log_e1)  # (B,1)\n",
    "\n",
    "    # Per-class E[p_j^p] for all j\n",
    "    log_ep = log_b(alpha + p, alpha0 - alpha) - log_b(alpha, alpha0 - alpha)\n",
    "    ep = torch.exp(log_ep)\n",
    "\n",
    "    # zero-out the true class term so we sum only jâ‰ c\n",
    "    ep = ep * (1 - y)\n",
    "\n",
    "    # final expectation sum\n",
    "    e_sum = e1 + ep.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # apply ^(1/p)  # noqa: ERA001\n",
    "    fi = torch.exp(torch.log(e_sum + 1e-8) / p).squeeze(1)  # (B,)\n",
    "\n",
    "    return fi.sum()\n",
    "\n",
    "\n",
    "def regularization_fn(alpha: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the regularization term using trigamma functions.\n",
    "\n",
    "    Penalizes high alpha values for incorrect classes to encourage confident\n",
    "    but calibrated predictions.\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B, K), must be > 0\n",
    "        y: One-hot encoded labels, shape (B, K)\n",
    "\n",
    "    Returns:\n",
    "        Scalar regularization loss\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If shapes don't match\n",
    "    \"\"\"\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Build alpha_tilde by replacing correct-class alpha with 1\n",
    "    alpha_tilde = alpha * (1 - y) + y\n",
    "\n",
    "    # Compute alpha_tilde_0 = 1 + sum over incorrect classes\n",
    "    alpha_tilde_0 = torch.sum(alpha_tilde, dim=1, keepdim=True)\n",
    "\n",
    "    # Polygamma(1, x) = trigamma(x)\n",
    "    trigamma_alpha = torch.polygamma(1, alpha_tilde)\n",
    "    trigamma_alpha0 = torch.polygamma(1, alpha_tilde_0)\n",
    "\n",
    "    # (alpha_tilde - 1)^2 term\n",
    "    diff_sq = (alpha_tilde - 1.0) ** 2\n",
    "\n",
    "    # Penalty only for incorrect classes â†’ mask out true class\n",
    "    mask = 1 - y\n",
    "\n",
    "    # Compute elementwise contribution\n",
    "    term = 0.5 * diff_sq * (trigamma_alpha - trigamma_alpha0) * mask\n",
    "\n",
    "    # Sum over classes and batch\n",
    "    return torch.sum(term)\n",
    "\n",
    "\n",
    "def dirichlet_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Dirichlet entropy.\n",
    "\n",
    "    For adversarial examples, we want to maximize entropy (reward the model for\n",
    "    being uncertain), which appears as a negative term in the loss.\n",
    "\n",
    "    Entropy formula (a stands for alpha):\n",
    "        H(a) = log B(a) + (a_0 - K) * Ïˆ(a_0) - Î£_k (a_k - 1) * Ïˆ(a_k)\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B_a, K), must be > 0\n",
    "\n",
    "    Returns:\n",
    "        Scalar entropy summed over batch\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If alpha contains non-positive values\n",
    "    \"\"\"\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    K = alpha.size(-1)  # noqa: N806\n",
    "    alpha0 = alpha.sum(dim=-1)\n",
    "\n",
    "    log_b = torch.lgamma(alpha).sum(dim=-1) - torch.lgamma(alpha0)\n",
    "\n",
    "    term1 = log_b\n",
    "    term2 = (alpha0 - K) * digamma(alpha0)\n",
    "    term3 = ((alpha - 1) * digamma(alpha)).sum(dim=-1)\n",
    "    entropy = term1 + term2 - term3\n",
    "\n",
    "    return entropy.sum()\n",
    "\n",
    "\n",
    "def loss_IRD(  # noqa: N802\n",
    "    alpha: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    adversarial_alpha: torch.Tensor | None = None,\n",
    "    p: float = 2.0,\n",
    "    lam: float = 1.0,\n",
    "    gamma: float = 1.0,\n",
    "    normalize: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the Loss introduced in paper: Information Robust Dirichlet Networks for Predictive Uncertainty Estimation\n",
    "    Args:\n",
    "        alpha : (B, K) Dirichlet concentration parameters\n",
    "        adversarial_alpha : (B_a, K) adversarial_alpha concentration parameters for adversarial inputs\n",
    "        y     : (B, K) one-hot labels\n",
    "        p     : scalar exponent\n",
    "    Returns:\n",
    "        loss_IRD : the IRD loss comprised of all three terms, summed over all input examples.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Input validation\n",
    "    if alpha.dim() != 2 or y.dim() != 2:\n",
    "        msg = f\"alpha and y must be 2D, got {alpha.dim()}, {y.dim()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute Loss Components\n",
    "    lp_term = lp_fn(alpha, y, p)\n",
    "    reg_term = regularization_fn(alpha, y)\n",
    "\n",
    "    if adversarial_alpha is not None:\n",
    "        if adversarial_alpha.dim() != 2:\n",
    "            msg = f\"adversarial_alpha must be 2D, got {adversarial_alpha.dim()}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        if adversarial_alpha.shape[1] != alpha.shape[1]:\n",
    "            msg = (\n",
    "                f\"adversarial_alpha must have same number of classes as alpha: \"\n",
    "                f\"{adversarial_alpha.shape[1]} vs {alpha.shape[1]}\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                msg,\n",
    "            )\n",
    "\n",
    "        entropy_term = dirichlet_entropy(adversarial_alpha)\n",
    "    else:\n",
    "        entropy_term = 0.0\n",
    "\n",
    "    # Normalize by batch sizes for stable training across different batch sizes\n",
    "    if normalize:\n",
    "        B = alpha.shape[0]  # noqa: N806\n",
    "        K = alpha.shape[1]  # noqa: N806\n",
    "        lp_term = lp_term / B\n",
    "        reg_term = reg_term / (B * K)\n",
    "\n",
    "        if adversarial_alpha is not None and isinstance(entropy_term, torch.Tensor):\n",
    "            B_a = adversarial_alpha.shape[0]  # noqa: N806\n",
    "            entropy_term = entropy_term / B_a\n",
    "\n",
    "    loss = lp_term + lam * reg_term - gamma * entropy_term\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae451a",
   "metadata": {},
   "source": [
    "### 2. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfabb1",
   "metadata": {},
   "source": [
    "LetÂ´s also test our loss by implementing a simple Evidential CNN for grayscale MNIST images. Here is the setup:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup training pipeline with MNIST, ResNet18, and AdamW optimizer.\"\"\"\n",
    "from __future__ import annotations  # noqa: E402, F404\n",
    "\n",
    "import torch  # noqa: E402, F811\n",
    "from torch import nn  # noqa: E402\n",
    "import torch.nn.functional as F  # noqa: E402, F401\n",
    "from torch.utils.data import DataLoader  # noqa: E402\n",
    "from torchvision import datasets, transforms  # noqa: E402\n",
    "from tqdm import tqdm  # noqa: E402\n",
    "\n",
    "\n",
    "def get_mnist_dataloaders(\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 0,\n",
    "    data_dir: str = \"./data\",\n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Load MNIST dataset and create training and validation dataloaders.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size for dataloaders (default: 32)\n",
    "        num_workers: Number of workers for data loading (default: 0)\n",
    "        data_dir: Directory to store MNIST data (default: \"./data\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    # Define transforms for MNIST - keep grayscale (1 channel)\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.1307],  # MNIST mean\n",
    "                std=[0.3081],  # MNIST std\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    transform_val = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.1307],\n",
    "                std=[0.3081],\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Load training dataset\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "\n",
    "    # Load validation dataset (using test set as validation)\n",
    "    val_dataset = datasets.MNIST(\n",
    "        root=data_dir,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_val,\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0cd0d",
   "metadata": {},
   "source": [
    "The CNN is composed of three layers, whereas the outputs of these layers are being transformed into alpha values (applying ReLU and then adding 1 instead of softmax) for evidential learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayscaleMNISTCNN(nn.Module):\n",
    "    \"\"\"Simple Evidential CNN for grayscale MNIST images.\n",
    "    Returns Dirichlet parameters (alpha) as output.\n",
    "    \"\"\"  # noqa: D205\n",
    "\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        \"\"\"Initialize the CNN.\"\"\"\n",
    "        super().__init__()\n",
    "        # (batch, 1, 28, 28) -> (batch, 32, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # After 3 pooling layers: 28 -> 14 -> 7 -> 3 (with padding)\n",
    "        # Actual: 28 -> 14 -> 7 -> 3\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Conv block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 28 -> 14\n",
    "\n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 14 -> 7\n",
    "\n",
    "        # Conv block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)  # 7 -> 3\n",
    "\n",
    "        # Flatten and fully connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Turn outputs into alpha values for evidential learning\n",
    "        x = self.relu(x)\n",
    "        x = x + torch.ones_like(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf1233",
   "metadata": {},
   "source": [
    "We train the model using different parameters like the model we are using, the loss function we are using (criterion), the variable  bda oder the number of training epochs, etc. \n",
    "\n",
    "We also add different uncertainty metrics, the predictive entropy (aleatoric & epistemic uncertainty) and mutual information (epistemic uncertainty). Using these metrics, we can further estimate the correctness and uncertainty of our model. The value of the entropy for the uniform distribution over 10 classes is around 2.3. This is the highest uncertainty the model can assign, so the values of the predictive entropy should be around 2.3, never higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, val_loader, device, n_epochs=5) -> None:  # noqa: ANN001, D417\n",
    "    \"\"\"Train a model, validate after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model: any evidential model\n",
    "        criterion: this is your loss function, has to take inputs alpha and y as shape (B,)\n",
    "          (float, optional):  bda parameter for the regularization term. Defaults to 0.15.\n",
    "        n_epochs (int, optional): Number of training epochs. Defaults to 5.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for x, y in pbar:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "            # Convert labels to one-hot encoding\n",
    "            y = nn.functional.one_hot(y, num_classes=10).float()  # noqa: PLW2901\n",
    "            y = y.to(device)  # noqa: PLW2901\n",
    "\n",
    "            alpha = model(x)\n",
    "            loss = criterion(alpha, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        validate(model, val_loader, criterion, device)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device) -> None:  # noqa: ANN001, ARG001\n",
    "    \"\"\"Validation loop.\"\"\"\n",
    "    model.eval()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    all_pe_id = []\n",
    "    all_mi_id = []\n",
    "    all_pe_ood = []\n",
    "    all_mi_ood = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:  # noqa: B007\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "\n",
    "            # In-distribution\n",
    "            alpha_id = model(x)\n",
    "            pe_id, mi_id = dirichlet_mi(alpha_id)\n",
    "            all_pe_id.append(pe_id)\n",
    "            all_mi_id.append(mi_id)\n",
    "\n",
    "            # OOD: Permuted MNIST\n",
    "            B = x.shape[0]  # noqa: N806\n",
    "            # random permutation of pixels\n",
    "            perm = torch.randperm(28 * 28, device=device)\n",
    "            x_permuted = x.reshape(B, 1, -1)[:, :, perm]\n",
    "            x_permuted = x_permuted.reshape(B, 1, 28, 28)\n",
    "\n",
    "            alpha_ood = model(x_permuted)\n",
    "            pe_ood, mi_ood = dirichlet_mi(alpha_ood)\n",
    "            all_pe_ood.append(pe_ood)\n",
    "            all_mi_ood.append(mi_ood)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        pe_id = torch.cat(all_pe_id)\n",
    "        mi_id = torch.cat(all_mi_id)\n",
    "        pe_ood = torch.cat(all_pe_ood)\n",
    "        mi_ood = torch.cat(all_mi_ood)\n",
    "\n",
    "        print(\"=== Uncertainty Summary ===\")\n",
    "        print(f\"ID  â€” Predictive Entropy: mean {pe_id.mean().item():.4f}, std {pe_id.std().item():.4f}\")\n",
    "        print(f\"ID  â€” Mutual Information: mean {mi_id.mean().item():.4f}, std {mi_id.std().item():.4f}\")\n",
    "        print(f\"OOD â€” Predictive Entropy: mean {pe_ood.mean().item():.4f}, std {pe_ood.std().item():.4f}\")\n",
    "        print(f\"OOD â€” Mutual Information: mean {mi_ood.mean().item():.4f}, std {mi_ood.std().item():.4f}\")\n",
    "        return {\n",
    "            \"pe_id\": pe_id,\n",
    "            \"mi_id\": mi_id,\n",
    "            \"pe_ood\": pe_ood,\n",
    "            \"mi_ood\": mi_ood,\n",
    "        }\n",
    "\n",
    "\n",
    "def dirichlet_mi(alpha: torch.Tensor):  # noqa: ANN201\n",
    "    \"\"\"Computes predictive entropy and mutual information for a Dirichlet prior.\n",
    "\n",
    "    Args:\n",
    "        alpha: (B, K) Dirichlet concentration\n",
    "\n",
    "    Returns:\n",
    "        predictive_entropy: (B,)\n",
    "        mutual_information: (B,)\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    alpha = torch.clamp(alpha, min=1e-6)\n",
    "    alpha0 = alpha.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # Predictive probabilities\n",
    "    p = alpha / alpha0\n",
    "\n",
    "    # Predictive entropy H[Y]\n",
    "    predictive_entropy = -(p * torch.log(p + eps)).sum(dim=1)\n",
    "\n",
    "    # Expected conditional entropy E_p[H[Y|p]]\n",
    "    digamma_alpha = torch.digamma(alpha + 1.0)\n",
    "    digamma_alpha0 = torch.digamma(alpha0 + 1.0)  # (B,1)\n",
    "\n",
    "    expected_cond_entropy = -torch.sum(\n",
    "        (alpha / alpha0) * (digamma_alpha - digamma_alpha0),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    # Mutual information = H[pred] - E[cond]\n",
    "    mutual_information = predictive_entropy - expected_cond_entropy\n",
    "\n",
    "    return predictive_entropy, mutual_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204b2cf",
   "metadata": {},
   "source": [
    "Now, we implement the function responsible for calculating the loss, accuracy and confidence for in-distribution samples and the confidence for out-of-distribution samples. The confidence for the OOD-samples should be lower than the confidence for the ID-samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c780bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for OOD inputs\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, data_loader, device) -> None:  # noqa: ANN001\n",
    "    \"\"\"Evaluate model on given data_loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss_id = 0.0\n",
    "    correct_id = 0\n",
    "    confidence_id = 0.0\n",
    "    confidence_ood = 0.0\n",
    "    length = len(data_loader.dataset)  # Number of samples\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)  # noqa: PLW2901\n",
    "        y = nn.functional.one_hot(y, num_classes=10).float()  # noqa: PLW2901\n",
    "        y = y.to(device)  # noqa: PLW2901\n",
    "\n",
    "        # OOD Noise inputs\n",
    "        noise = torch.randn_like(x).to(device)\n",
    "        alpha_noise = model(noise)\n",
    "\n",
    "        # Calculate loss for in-distribution\n",
    "        alpha = model(x)  # (B, num_classes)\n",
    "        total_loss_id += criterion(alpha, y)\n",
    "\n",
    "        # Calculate accuracy for in-distribution inputs\n",
    "        pred = torch.max(alpha, -1).indices\n",
    "        y_labels = torch.argmax(y, -1)\n",
    "        correct_id += (pred == y_labels).sum().item()\n",
    "\n",
    "        # Calculate confidence for in-distribution and OOD inputs\n",
    "        confidence_id += (torch.max(alpha, -1).values / torch.sum(alpha, -1)).sum().item()\n",
    "        confidence_ood += (torch.max(alpha_noise, -1).values / torch.sum(alpha_noise, -1)).sum().item()\n",
    "\n",
    "    total_loss_id /= length\n",
    "    accuracy_id = correct_id / length\n",
    "    confidence_ood /= length\n",
    "    confidence_id /= length\n",
    "\n",
    "    print(\"Loss In-Distribution: \", total_loss_id.item())\n",
    "    print(\"Confidence In-Distribution: \", confidence_id)\n",
    "    print(\"Confidence OOD: \", confidence_ood)  # Ideally should be low\n",
    "    print(f\"Evaluation In-Distribution Accuracy: {accuracy_id:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd6df6",
   "metadata": {},
   "source": [
    "In the following code, we will be plotting a histogram which should visualize our uncertainty for ID and OOD-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_uncertainty(pe_id, pe_ood, mi_id, mi_ood) -> None:  # noqa: ANN001\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Predictive Entropy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(\n",
    "        pe_id.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"ID\",\n",
    "        color=\"#4C72B0\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        pe_ood.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"OOD\",\n",
    "        color=\"#DC1489\",\n",
    "    )\n",
    "    plt.xlabel(\"Predictive Entropy\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Predictive Entropy: ID vs OOD\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Mutual Information\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(\n",
    "        mi_id.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"ID\",\n",
    "        color=\"#4C72B0\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        mi_ood.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"OOD\",\n",
    "        color=\"#DC1489\",\n",
    "    )\n",
    "    plt.xlabel(\"Mutual Information\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Mutual Information: ID vs OOD\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a019191",
   "metadata": {},
   "source": [
    "Now we implement the main function. We set the parameters and define the criterion we are using. In my case, that is the loss proposed in the paper _Information Robust Dirichlet Networks_ by Tsiligkaridis (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"This code presumes that the loss function takes in alpha and y with shape (B, 10).\n",
    "    Currently loss_IRD takes only one-hot encoded y\n",
    "    criterion = loss_IRD.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # --------------- Standard setup --------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    print(\"\\nLoading MNIST dataset...\")\n",
    "\n",
    "    # Use large batch to reduce gradient noise\n",
    "    # Gradients will be noisy if regularization term is used\n",
    "    train_loader, val_loader = get_mnist_dataloaders(batch_size=128)\n",
    "\n",
    "    print(\"\\nInitialize model...\")\n",
    "    model = GrayscaleMNISTCNN(num_classes=10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    # Import your loss function with hyperparameters\n",
    "    from src.probly.train.evidential.torch import IRDLoss  # noqa: PLC0415\n",
    "\n",
    "    criterion = IRDLoss(p=2.0, lam=0.01, gamma=0.01)\n",
    "\n",
    "    # Train for a few epochs\n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    train(model, optimizer, criterion, train_loader, val_loader, device, n_epochs=10)\n",
    "    evaluate(model, criterion, val_loader, device)\n",
    "\n",
    "    stats = validate(model, val_loader, criterion, device)\n",
    "    plot_uncertainty(\n",
    "        stats[\"pe_id\"],\n",
    "        stats[\"pe_ood\"],\n",
    "        stats[\"mi_id\"],\n",
    "        stats[\"mi_ood\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10ec4f",
   "metadata": {},
   "source": [
    "To enhance code reusability and maintain a clean notebook structure, we can get access to the Grayscale-CNN by importing it from the dedicated module.\n",
    "\n",
    "The same goes for the IRD loss. As seen below, we implement it from train.evidential.torch, which contains all the different losses introduced in the papers.\n",
    "\n",
    "- **Model Architecture:** Import `GrayscaleMNISTCNN` from `probly.layers.evidential.torch`\n",
    "- **Loss Function:** Import `IRDLoss` from `probly.train.evidential.torch`  \n",
    "- **Training Interface:** Use `unified_evidential_train_class` for standardized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Advanced: Unified Evidential Training Interface (Optional)\n",
    "# ============================================================================\n",
    "# NOTE: Use main() above instead. This is for advanced testing only.\n",
    "# Uncomment to test the unified training interface:\n",
    "\n",
    "\"\"\"\"\n",
    "from probly.layers.evidential.torch import GrayscaleMNISTCNN as LibraryGrayscaleMNISTCNN\n",
    "from probly.train.evidential.common import unified_evidential_train\n",
    "from probly.train.evidential.torch import ird_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, val_loader = get_mnist_dataloaders(batch_size=128)\n",
    "model = LibraryGrayscaleMNISTCNN(num_classes=10).to(device)\n",
    "criterion = ird_loss\n",
    "\n",
    "# Call unified_evidential_train_class with correct parameters\n",
    "unified_evidential_train(\n",
    "    mode=\"IRD\",\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=criterion,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
