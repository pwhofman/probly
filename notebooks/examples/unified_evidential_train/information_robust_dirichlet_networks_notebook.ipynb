{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad65fcd7",
   "metadata": {},
   "source": [
    "# Information Robust Dirichlet Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb53a5",
   "metadata": {},
   "source": [
    "In this notebook, we implement the specialized training loss proposed in the paper _Information Robust Dirichlet Networks for Predictive Uncertainty Estimation_ by Tsiligkaridis (2019). The method models predictive uncertainty by having a neural network output Dirichlet concentration parameters ð›¼ instead of just a pointwise softmax.\n",
    "\n",
    "The total loss is composed of three terms:\n",
    "\n",
    "1. Calibration term: implemented in the function  lp_fn\n",
    "2. Regularization term: implemented in the function  regularization_fn\n",
    "3. Adversiarial Entropy penalty: implemented in the function  dirichlet_entropy\n",
    "\n",
    "In the paper and in this notenbook, L_p loss is not directly computed but rather an upper bound for it, denoted by F_i (for sample i)  \n",
    "\n",
    "The regularization term penalizes high alpha values for incorrect classes.  \n",
    "\n",
    "The final term uses the alpha values the model assigns to adversarial inputs.\n",
    "The model is rewarded for outputting a Dirichlet-distribution with high entropy on these inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20a7ec",
   "metadata": {},
   "source": [
    "### 1. The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c812c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch.special import digamma\n",
    "\n",
    "\n",
    "def lp_fn(alpha: torch.Tensor, y: torch.Tensor, p: float = 2.0) -> torch.Tensor:\n",
    "    \"\"\"Compute the Lp calibration loss (upper bound Fi).\n",
    "\n",
    "    Computes F_i using the expectation-based formulation:\n",
    "        F_i = ( E[(1-p_c)^p] + Î£_{jâ‰ c} E[p_j^p] )^(1/p)\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B, K), must be > 0\n",
    "        y: One-hot encoded labels, shape (B, K)\n",
    "        p: Lp norm exponent (default: 2.0)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss summed over batch\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If alpha contains non-positive values or shapes don't match\n",
    "    \"\"\"\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    _B, _K = alpha.shape  # noqa: N806\n",
    "\n",
    "    # total concentration alpha0\n",
    "    alpha0 = alpha.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # extract alpha_c (correct class)\n",
    "    alpha_c = (alpha * y).sum(dim=1, keepdim=True)  # (B,1)\n",
    "    alpha0_minus_c = alpha0 - alpha_c  # (B,1)\n",
    "\n",
    "    # log B(a,b) used for expectations: E[X^p] = B(a+p,b)/B(a,b)\n",
    "    def log_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "    # E[(1 - p_c)^p]   where (1 - p_c) ~ Beta( alpha0 - alpha_c , alpha_c )\n",
    "    log_e1 = log_b(alpha0_minus_c + p, alpha_c) - log_b(alpha0_minus_c, alpha_c)\n",
    "    e1 = torch.exp(log_e1)  # (B,1)\n",
    "\n",
    "    # Per-class E[p_j^p] for all j\n",
    "    log_ep = log_b(alpha + p, alpha0 - alpha) - log_b(alpha, alpha0 - alpha)\n",
    "    ep = torch.exp(log_ep)\n",
    "\n",
    "    # zero-out the true class term so we sum only jâ‰ c\n",
    "    ep = ep * (1 - y)\n",
    "\n",
    "    # final expectation sum\n",
    "    e_sum = e1 + ep.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # apply ^(1/p)  # noqa: ERA001\n",
    "    fi = torch.exp(torch.log(e_sum + 1e-8) / p).squeeze(1)  # (B,)\n",
    "\n",
    "    return fi.sum()\n",
    "\n",
    "\n",
    "def regularization_fn(alpha: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the regularization term using trigamma functions.\n",
    "\n",
    "    Penalizes high alpha values for incorrect classes to encourage confident\n",
    "    but calibrated predictions.\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B, K), must be > 0\n",
    "        y: One-hot encoded labels, shape (B, K)\n",
    "\n",
    "    Returns:\n",
    "        Scalar regularization loss\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If shapes don't match\n",
    "    \"\"\"\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Build alpha_tilde by replacing correct-class alpha with 1\n",
    "    alpha_tilde = alpha * (1 - y) + y\n",
    "\n",
    "    # Compute alpha_tilde_0 = 1 + sum over incorrect classes\n",
    "    alpha_tilde_0 = torch.sum(alpha_tilde, dim=1, keepdim=True)\n",
    "\n",
    "    # Polygamma(1, x) = trigamma(x)\n",
    "    trigamma_alpha = torch.polygamma(1, alpha_tilde)\n",
    "    trigamma_alpha0 = torch.polygamma(1, alpha_tilde_0)\n",
    "\n",
    "    # (alpha_tilde - 1)^2 term\n",
    "    diff_sq = (alpha_tilde - 1.0) ** 2\n",
    "\n",
    "    # Penalty only for incorrect classes â†’ mask out true class\n",
    "    mask = 1 - y\n",
    "\n",
    "    # Compute elementwise contribution\n",
    "    term = 0.5 * diff_sq * (trigamma_alpha - trigamma_alpha0) * mask\n",
    "\n",
    "    # Sum over classes and batch\n",
    "    return torch.sum(term)\n",
    "\n",
    "\n",
    "def dirichlet_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute Dirichlet entropy.\n",
    "\n",
    "    For adversarial examples, we want to maximize entropy (reward the model for\n",
    "    being uncertain), which appears as a negative term in the loss.\n",
    "\n",
    "    Entropy formula (a stands for alpha):\n",
    "        H(a) = log B(a) + (a_0 - K) * Ïˆ(a_0) - Î£_k (a_k - 1) * Ïˆ(a_k)\n",
    "\n",
    "    Args:\n",
    "        alpha: Dirichlet concentration parameters, shape (B_a, K), must be > 0\n",
    "\n",
    "    Returns:\n",
    "        Scalar entropy summed over batch\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If alpha contains non-positive values\n",
    "    \"\"\"\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    K = alpha.size(-1)  # noqa: N806\n",
    "    alpha0 = alpha.sum(dim=-1)\n",
    "\n",
    "    log_b = torch.lgamma(alpha).sum(dim=-1) - torch.lgamma(alpha0)\n",
    "\n",
    "    term1 = log_b\n",
    "    term2 = (alpha0 - K) * digamma(alpha0)\n",
    "    term3 = ((alpha - 1) * digamma(alpha)).sum(dim=-1)\n",
    "    entropy = term1 + term2 - term3\n",
    "\n",
    "    return entropy.sum()\n",
    "\n",
    "\n",
    "def loss_IRD(  # noqa: N802\n",
    "    alpha: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    adversarial_alpha: torch.Tensor | None = None,\n",
    "    p: float = 2.0,\n",
    "    lam: float = 1.0,\n",
    "    gamma: float = 1.0,\n",
    "    normalize: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the Loss introduced in paper: Information Robust Dirichlet Networks for Predictive Uncertainty Estimation\n",
    "    Args:\n",
    "        alpha : (B, K) Dirichlet concentration parameters\n",
    "        adversarial_alpha : (B_a, K) adversarial_alpha concentration parameters for adversarial inputs\n",
    "        y     : (B, K) one-hot labels\n",
    "        p     : scalar exponent\n",
    "    Returns:\n",
    "        loss_IRD : the IRD loss comprised of all three terms, summed over all input examples.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Input validation\n",
    "    if alpha.dim() != 2 or y.dim() != 2:\n",
    "        msg = f\"alpha and y must be 2D, got {alpha.dim()}, {y.dim()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if alpha.shape != y.shape:\n",
    "        msg = f\"alpha and y shape mismatch: {alpha.shape} vs {y.shape}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if not torch.all(alpha > 0):\n",
    "        msg = f\"All alpha values must be > 0, got min={alpha.min().item()}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Compute Loss Components\n",
    "    lp_term = lp_fn(alpha, y, p)\n",
    "    reg_term = regularization_fn(alpha, y)\n",
    "\n",
    "    if adversarial_alpha is not None:\n",
    "        if adversarial_alpha.dim() != 2:\n",
    "            msg = f\"adversarial_alpha must be 2D, got {adversarial_alpha.dim()}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        if adversarial_alpha.shape[1] != alpha.shape[1]:\n",
    "            msg = (\n",
    "                f\"adversarial_alpha must have same number of classes as alpha: \"\n",
    "                f\"{adversarial_alpha.shape[1]} vs {alpha.shape[1]}\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                msg,\n",
    "            )\n",
    "\n",
    "        entropy_term = dirichlet_entropy(adversarial_alpha)\n",
    "    else:\n",
    "        entropy_term = 0.0\n",
    "\n",
    "    # Normalize by batch sizes for stable training across different batch sizes\n",
    "    if normalize:\n",
    "        B = alpha.shape[0]  # noqa: N806\n",
    "        K = alpha.shape[1]  # noqa: N806\n",
    "        lp_term = lp_term / B\n",
    "        reg_term = reg_term / (B * K)\n",
    "\n",
    "        if adversarial_alpha is not None and isinstance(entropy_term, torch.Tensor):\n",
    "            B_a = adversarial_alpha.shape[0]  # noqa: N806\n",
    "            entropy_term = entropy_term / B_a\n",
    "\n",
    "    loss = lp_term + lam * reg_term - gamma * entropy_term\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb6163",
   "metadata": {},
   "source": [
    "### 2. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78007ac6",
   "metadata": {},
   "source": [
    "LetÂ´s also test our loss by implementing a simple Evidential CNN for images. Here is the setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf03556",
   "metadata": {},
   "source": [
    "#### LetÂ´s assume the user passes a dataset to us\n",
    "In this example we use CIFAR10 as an example, but the code works for any torch.util.data.Dataset class.\n",
    "\n",
    "If the user has a dataset, which he passes to us, we can automatically adapt the models input channels and output dimension (num_classes), based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02af417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F  # noqa: F401\n",
    "from torch.utils.data import DataLoader  # noqa: F401\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# wrap into a DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35356e4",
   "metadata": {},
   "source": [
    "#### Determine the input channels and number of class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c97b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 32, 32), Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "def get_num_classes(dataset: torch.utils.data.Dataset) -> int:\n",
    "    \"\"\"Determine number of classes by checking in order:\n",
    "    1) dataset.classes\n",
    "    2) dataset.targets\n",
    "    3) full dataset scan (max label + 1).\n",
    "    \"\"\"  # noqa: D205\n",
    "    # Try Torchvision-style classes attribute\n",
    "    if hasattr(dataset, \"classes\"):\n",
    "        try:\n",
    "            return len(dataset.classes)\n",
    "        except Exception:  # noqa: BLE001, S110\n",
    "            pass\n",
    "\n",
    "    # Else try Torchvision-style targets tensor\n",
    "    if hasattr(dataset, \"targets\"):\n",
    "        targets = dataset.targets\n",
    "        if isinstance(targets, torch.Tensor):\n",
    "            return int(targets.max().item()) + 1\n",
    "        else:  # noqa: RET505\n",
    "            return max(targets) + 1\n",
    "\n",
    "    # Fallback: scan entire dataset\n",
    "    max_label = -1\n",
    "    for _, y in dataset:\n",
    "        if isinstance(y, torch.Tensor):\n",
    "            y = y.item()  # noqa: PLW2901\n",
    "        max_label = max(max_label, int(y))\n",
    "\n",
    "    return max_label + 1\n",
    "\n",
    "\n",
    "# to get the number of input channels, take the first image from the dataset and check its shape\n",
    "c_in, H, W = dataset[0][0].shape  # assuming dataset returns (image, label) tuples\n",
    "\n",
    "# to get the number of classes, use the get_num_classes function\n",
    "num_classes = get_num_classes(dataset)\n",
    "\n",
    "print(f\"Input shape: ({c_in}, {H}, {W}), Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1da84",
   "metadata": {},
   "source": [
    "#### Initialize a CNN model with correct input channels, feedforward dimension and output dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d460fb1",
   "metadata": {},
   "source": [
    "The CNN is composed of three layers, whereas the outputs of these layers are being transformed into alpha values (applying ReLU and then adding 1 instead of softmax) for evidential learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93ec0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"Simple Evidential CNN for images.\n",
    "    Returns Dirichlet parameters (alpha) as output.\n",
    "    \"\"\"  # noqa: D205\n",
    "\n",
    "    def __init__(self, c_in: int, num_classes: int) -> None:\n",
    "        \"\"\"Initialize the CNN.\"\"\"\n",
    "        super().__init__()\n",
    "        # (B, C_in, H, W) -> (B, 32, H_out, W_out)\n",
    "        self.conv1 = nn.Conv2d(in_channels=c_in, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Dummy forward pass through the convolutions to infer feature dimension of ffwd layer automatically\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c_in, H, W)\n",
    "            dummy = self._forward_conv(dummy)\n",
    "            feature_dim = dummy.view(1, -1).size(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(feature_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _forward_conv(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Convolutions\n",
    "        x = self._forward_conv(x)\n",
    "\n",
    "        # Flatten and fully connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Turn outputs into alpha values for evidential learning\n",
    "        x = self.relu(x)\n",
    "        x = x + torch.ones_like(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34557",
   "metadata": {},
   "source": [
    "#### Simple forward pass test to verify model dimensions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0608017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output shape: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# simple forward pass test\n",
    "model = CNN(c_in=c_in, num_classes=num_classes)\n",
    "sample_input = torch.randn(8, c_in, 32, 32)  # batch of 8 images\n",
    "sample_output = model(sample_input)\n",
    "print(\"Sample output shape:\", sample_output.shape)  # should be (8, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf1233",
   "metadata": {},
   "source": [
    "We train the model using different parameters like the model we are using, the loss function we are using (criterion), or the number of training epochs, etc. \n",
    "\n",
    "We also add different uncertainty metrics, the predictive entropy (aleatoric & epistemic uncertainty) and mutual information (epistemic uncertainty). Using these metrics, we can further estimate the correctness and uncertainty of our model. The value of the entropy for the uniform distribution over 10 classes is around 2.3. This is the highest uncertainty the model can assign, so the values of the predictive entropy should be around 2.3, never higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abdbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, val_loader, device, n_epochs=5) -> None:  # noqa: ANN001, D417\n",
    "    \"\"\"Train a model, validate after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model: any evidential model\n",
    "        criterion: this is your loss function, has to take inputs alpha and y as shape (B,)\n",
    "          (float, optional):  bda parameter for the regularization term. Defaults to 0.15.\n",
    "        n_epochs (int, optional): Number of training epochs. Defaults to 5.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for x, y in pbar:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "            # Convert labels to one-hot encoding\n",
    "            y = nn.functional.one_hot(y, num_classes=10).float()  # noqa: PLW2901\n",
    "            y = y.to(device)  # noqa: PLW2901\n",
    "\n",
    "            alpha = model(x)\n",
    "            loss = criterion(alpha, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        validate(model, val_loader, criterion, device)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device) -> None:  # noqa: ANN001, ARG001\n",
    "    \"\"\"Validation loop.\"\"\"\n",
    "    model.eval()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    all_pe_id = []\n",
    "    all_mi_id = []\n",
    "    all_pe_ood = []\n",
    "    all_mi_ood = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:  # noqa: B007\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "\n",
    "            # In-distribution\n",
    "            alpha_id = model(x)\n",
    "            pe_id, mi_id = dirichlet_mi(alpha_id)\n",
    "            all_pe_id.append(pe_id)\n",
    "            all_mi_id.append(mi_id)\n",
    "\n",
    "            # OOD: Permuted MNIST\n",
    "            B = x.shape[0]  # noqa: N806\n",
    "            # random permutation of pixels\n",
    "            perm = torch.randperm(28 * 28, device=device)\n",
    "            x_permuted = x.reshape(B, 1, -1)[:, :, perm]\n",
    "            x_permuted = x_permuted.reshape(B, 1, 28, 28)\n",
    "\n",
    "            alpha_ood = model(x_permuted)\n",
    "            pe_ood, mi_ood = dirichlet_mi(alpha_ood)\n",
    "            all_pe_ood.append(pe_ood)\n",
    "            all_mi_ood.append(mi_ood)\n",
    "\n",
    "        # Concatenate tensors\n",
    "        pe_id = torch.cat(all_pe_id)\n",
    "        mi_id = torch.cat(all_mi_id)\n",
    "        pe_ood = torch.cat(all_pe_ood)\n",
    "        mi_ood = torch.cat(all_mi_ood)\n",
    "\n",
    "        print(\"=== Uncertainty Summary ===\")\n",
    "        print(f\"ID  â€” Predictive Entropy: mean {pe_id.mean().item():.4f}, std {pe_id.std().item():.4f}\")\n",
    "        print(f\"ID  â€” Mutual Information: mean {mi_id.mean().item():.4f}, std {mi_id.std().item():.4f}\")\n",
    "        print(f\"OOD â€” Predictive Entropy: mean {pe_ood.mean().item():.4f}, std {pe_ood.std().item():.4f}\")\n",
    "        print(f\"OOD â€” Mutual Information: mean {mi_ood.mean().item():.4f}, std {mi_ood.std().item():.4f}\")\n",
    "        return {\n",
    "            \"pe_id\": pe_id,\n",
    "            \"mi_id\": mi_id,\n",
    "            \"pe_ood\": pe_ood,\n",
    "            \"mi_ood\": mi_ood,\n",
    "        }\n",
    "\n",
    "\n",
    "def dirichlet_mi(alpha: torch.Tensor):  # noqa: ANN201\n",
    "    \"\"\"Computes predictive entropy and mutual information for a Dirichlet prior.\n",
    "\n",
    "    Args:\n",
    "        alpha: (B, K) Dirichlet concentration\n",
    "\n",
    "    Returns:\n",
    "        predictive_entropy: (B,)\n",
    "        mutual_information: (B,)\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    alpha = torch.clamp(alpha, min=1e-6)\n",
    "    alpha0 = alpha.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # Predictive probabilities\n",
    "    p = alpha / alpha0\n",
    "\n",
    "    # Predictive entropy H[Y]\n",
    "    predictive_entropy = -(p * torch.log(p + eps)).sum(dim=1)\n",
    "\n",
    "    # Expected conditional entropy E_p[H[Y|p]]\n",
    "    digamma_alpha = torch.digamma(alpha + 1.0)\n",
    "    digamma_alpha0 = torch.digamma(alpha0 + 1.0)  # (B,1)\n",
    "\n",
    "    expected_cond_entropy = -torch.sum(\n",
    "        (alpha / alpha0) * (digamma_alpha - digamma_alpha0),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    # Mutual information = H[pred] - E[cond]\n",
    "    mutual_information = predictive_entropy - expected_cond_entropy\n",
    "\n",
    "    return predictive_entropy, mutual_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204b2cf",
   "metadata": {},
   "source": [
    "Now, we implement the function responsible for calculating the loss, accuracy and confidence for in-distribution samples and the confidence for out-of-distribution samples. The confidence for the OOD-samples should be lower than the confidence for the ID-samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c780bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for OOD inputs\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, data_loader, device) -> None:  # noqa: ANN001\n",
    "    \"\"\"Evaluate model on given data_loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss_id = 0.0\n",
    "    correct_id = 0\n",
    "    confidence_id = 0.0\n",
    "    confidence_ood = 0.0\n",
    "    length = len(data_loader.dataset)  # Number of samples\n",
    "\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)  # noqa: PLW2901\n",
    "        y = nn.functional.one_hot(y, num_classes=10).float()  # noqa: PLW2901\n",
    "        y = y.to(device)  # noqa: PLW2901\n",
    "\n",
    "        # OOD Noise inputs\n",
    "        noise = torch.randn_like(x).to(device)\n",
    "        alpha_noise = model(noise)\n",
    "\n",
    "        # Calculate loss for in-distribution\n",
    "        alpha = model(x)  # (B, num_classes)\n",
    "        total_loss_id += criterion(alpha, y)\n",
    "\n",
    "        # Calculate accuracy for in-distribution inputs\n",
    "        pred = torch.max(alpha, -1).indices\n",
    "        y_labels = torch.argmax(y, -1)\n",
    "        correct_id += (pred == y_labels).sum().item()\n",
    "\n",
    "        # Calculate confidence for in-distribution and OOD inputs\n",
    "        confidence_id += (torch.max(alpha, -1).values / torch.sum(alpha, -1)).sum().item()\n",
    "        confidence_ood += (torch.max(alpha_noise, -1).values / torch.sum(alpha_noise, -1)).sum().item()\n",
    "\n",
    "    total_loss_id /= length\n",
    "    accuracy_id = correct_id / length\n",
    "    confidence_ood /= length\n",
    "    confidence_id /= length\n",
    "\n",
    "    print(\"Loss In-Distribution: \", total_loss_id.item())\n",
    "    print(\"Confidence In-Distribution: \", confidence_id)\n",
    "    print(\"Confidence OOD: \", confidence_ood)  # Ideally should be low\n",
    "    print(f\"Evaluation In-Distribution Accuracy: {accuracy_id:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd6df6",
   "metadata": {},
   "source": [
    "In the following code, we will be plotting a histogram which should visualize our uncertainty for ID and OOD-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_uncertainty(pe_id, pe_ood, mi_id, mi_ood) -> None:  # noqa: ANN001\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Predictive Entropy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(\n",
    "        pe_id.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"ID\",\n",
    "        color=\"#4C72B0\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        pe_ood.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"OOD\",\n",
    "        color=\"#DC1489\",\n",
    "    )\n",
    "    plt.xlabel(\"Predictive Entropy\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Predictive Entropy: ID vs OOD\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Mutual Information\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(\n",
    "        mi_id.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"ID\",\n",
    "        color=\"#4C72B0\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        mi_ood.cpu().numpy(),\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        label=\"OOD\",\n",
    "        color=\"#DC1489\",\n",
    "    )\n",
    "    plt.xlabel(\"Mutual Information\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Mutual Information: ID vs OOD\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a019191",
   "metadata": {},
   "source": [
    "Now we implement the main function. We set the parameters and define the criterion we are using. In my case, that is the loss proposed in the paper _Information Robust Dirichlet Networks_ by Tsiligkaridis (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"This code presumes that the loss function takes in alpha and y with shape (B, 10).\n",
    "    Currently loss_IRD takes only one-hot encoded y\n",
    "    criterion = loss_IRD.\n",
    "    \"\"\"  # noqa: D205\n",
    "    # --------------- Standard setup --------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    print(\"\\nLoading MNIST dataset...\")\n",
    "\n",
    "    # Use large batch to reduce gradient noise\n",
    "    # Gradients will be noisy if regularization term is used\n",
    "\n",
    "    # TODO(<julia>): Replace with own dataloaders and val_loaders and then uncomment the associated lines  # noqa: TD003\n",
    "    # train_loader, val_loader = get_mnist_dataloaders(batch_size=128)  # noqa: ERA001\n",
    "\n",
    "    print(\"\\nInitialize model...\")\n",
    "    model = CNN(num_classes=10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)  # noqa: F841\n",
    "\n",
    "    # Import your loss function with hyperparameters\n",
    "    from src.probly.losses.evidential.torch import ird_loss  # noqa: PLC0415\n",
    "\n",
    "    criterion = ird_loss(p=2.0, lam=0.01, gamma=0.01)  # noqa: F841\n",
    "\n",
    "    # Train for a few epochs\n",
    "    print(\"\\nStarting training...\")\n",
    "\n",
    "    # train(model, optimizer, criterion, train_loader, val_loader, device, n_epochs=10)  # noqa: ERA001\n",
    "    # evaluate(model, criterion, val_loader, device)  # noqa: ERA001\n",
    "\n",
    "    # stats = validate(model, val_loader, criterion, device)  # noqa: ERA001\n",
    "    # plot_uncertainty(\n",
    "    # stats[\"pe_id\"],  # noqa: ERA001\n",
    "    # stats[\"pe_ood\"],  # noqa: ERA001\n",
    "    # stats[\"mi_id\"],  # noqa: ERA001\n",
    "    # stats[\"mi_ood\"],  # noqa: ERA001\n",
    "    # )  # noqa: ERA001, RUF100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10ec4f",
   "metadata": {},
   "source": [
    "To enhance code reusability and maintain a clean notebook structure, we can get access to the Grayscale-CNN by importing it from the dedicated module.\n",
    "\n",
    "The same goes for the IRD loss. As seen below, we implement it from train.evidential.torch, which contains all the different losses introduced in the papers.\n",
    "\n",
    "- **Model Architecture:** Import `GrayscaleMNISTCNN` from `probly.layers.evidential.torch`\n",
    "- **Loss Function:** Import `ird_loss` from `probly.losses.evidential.torch`  \n",
    "- **Training Interface:** Use `unified_evidential_train` for standardized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Advanced: Unified Evidential Training Interface (Optional)\n",
    "# ============================================================================\n",
    "# NOTE: Use main() above instead. This is for advanced testing only.\n",
    "# Uncomment to test the unified training interface:\n",
    "\n",
    "\n",
    "from probly.losses.evidential.torch import ird_loss\n",
    "from probly.models.evidential.torch import IRDModel\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class DirichletMLPEncoder(nn.Module):\n",
    "    \"\"\"Simple MLP encoder for transforming inputs into feature embeddings.\n",
    "\n",
    "    This module contains no evidential logic, only feature extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        latent_dim: int = 128,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the MLP encoder.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Size of input features (flattened or 1D).\n",
    "            hidden_dim: Number of neurons in hidden layers (default: 128).\n",
    "            latent_dim: Dimension of output feature representation (default: 128).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute feature embedding.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Feature tensor of shape (batch_size, latent_dim).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# train_loader, val_loader = get_mnist_dataloaders(batch_size=128)  # noqa: ERA001\n",
    "enc = DirichletMLPEncoder(input_dim=28 * 28)\n",
    "model = IRDModel(encoder=enc)\n",
    "criterion = ird_loss\n",
    "\n",
    "# Call unified_evidential_train with correct parameters\n",
    "unified_evidential_train(\n",
    "    mode=\"IRD\",\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    loss_fn=criterion,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
