{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533bf1a8-b94d-4ab6-98e3-2a3ffa0088c3",
   "metadata": {},
   "source": [
    "# Natural Posterior Network (NatPN) Tutorial in PyTorch\n",
    "\n",
    "## A Detailed, Notebook-Ready Walkthrough with OOD Uncertainty Check\n",
    "\n",
    "This notebook is a step-by-step implementation and explanation of a simple Natural Posterior Network (NatPN) for classification with PyTorch.\n",
    "\n",
    "NatPN is a model that:\n",
    "\n",
    "- Predicts a distribution over class probabilities, not just point predictions.\n",
    "- Uses a conjugate prior (Dirichlet) for the categorical likelihood.\n",
    "- Uses a normalizing flow over a latent space to turn density into evidence.\n",
    "- Produces higher uncertainty for inputs that are far from the training data, without needing OOD samples in training.\n",
    "\n",
    "We will build this for a simple setting:\n",
    "\n",
    "- **In-distribution (ID) dataset:** MNIST (handwritten digits).\n",
    "- **Out-of-distribution (OOD) dataset:** FashionMNIST (clothing images).\n",
    "\n",
    "We will:\n",
    "\n",
    "- Explain and implement the NatPN components (encoder, flow-based density, Dirichlet posterior).\n",
    "- Define and explain a Bayesian NatPN loss.\n",
    "- Implement a unified training function that could live in a `torch.py` module.\n",
    "- Train the model, evaluate accuracy, and check uncertainty for ID vs OOD.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Imports & Setup**\n",
    "2. **Data Preparation**\n",
    "3. **Model Definition (NatPN Components)**\n",
    "   - Encoder: \\( x \\rightarrow z \\)\n",
    "   - Radial Flow Layer\n",
    "   - Radial Flow Density\n",
    "   - NatPNClassifier: Combining Everything\n",
    "4. **NatPN Loss, Uncertainty & Unified Training Function**\n",
    "   - NatPN Loss for Classification\n",
    "   - Predictions & Uncertainty from Dirichlet\n",
    "   - Unified NatPN Training Function\n",
    "5. **Training, Evaluation & OOD Uncertainty Check**\n",
    "   - Initialize Model & Train\n",
    "   - Accuracy on MNIST (ID)\n",
    "   - OOD Check: MNIST vs FashionMNIST\n",
    "   - Inspect Individual Batches\n",
    "6. **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b68d8-7be8-4279-820a-eaeea6d95069",
   "metadata": {},
   "source": [
    "# 1. Imports & Setup\n",
    "\n",
    "## Explanation\n",
    "\n",
    "In this section we:\n",
    "\n",
    "### Import core PyTorch libraries:\n",
    "\n",
    "- `torch` and `torch.nn` for tensors and neural network layers.\n",
    "- `DataLoader` to iterate over datasets in mini-batches.\n",
    "\n",
    "### Import torchvision:\n",
    "\n",
    "- `torchvision.datasets` provides pre-defined datasets like MNIST and FashionMNIST.\n",
    "- `torchvision.transforms` provides transformations such as converting images to tensors.\n",
    "\n",
    "### Import `Dirichlet` from `torch.distributions`:\n",
    "\n",
    "- We use this for working with Dirichlet distributions, which serve as the conjugate prior/posterior for categorical likelihoods.\n",
    "\n",
    "### Decide on a device:\n",
    "\n",
    "- `\"cuda\"` if a GPU is available.\n",
    "- `\"cpu\"` otherwise.\n",
    "\n",
    "### Optionally import typing helpers:\n",
    "\n",
    "- `Optional`, `Tuple`, etc., for cleaner and more descriptive type hints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247dc98-5d67-4abd-919b-bdc56a8bbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Choose device: use GPU if available, otherwise CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1b65a-f0df-419b-943a-edf411b83d8f",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "## Explanation\n",
    "\n",
    "NatPN is trained like a standard classifier, but with uncertainty-aware outputs.\n",
    "\n",
    "We need **3 datasets**:\n",
    "\n",
    "### 1. Training data (ID)\n",
    "\n",
    "- **MNIST**, `train=True`\n",
    "- Contains handwritten digits (0â€“9)\n",
    "\n",
    "### 2. Test data (ID)\n",
    "\n",
    "- **MNIST**, `train=False`\n",
    "- Used for evaluating accuracy and in-distribution uncertainty\n",
    "\n",
    "### 3. OOD data\n",
    "\n",
    "- **FashionMNIST**, `train=False`\n",
    "- Has the same image shape (**28Ã—28, grayscale**) but represents clothing instead of digits  \n",
    "- This makes it suitable for out-of-distribution uncertainty evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## What we do\n",
    "\n",
    "### Apply a transform:\n",
    "\n",
    "- Use `ToTensor()` to convert PIL images into PyTorch tensors  \n",
    "- Produces tensors of shape **[1, 28, 28]**  \n",
    "- Pixel values are scaled to **[0, 1]**\n",
    "\n",
    "---\n",
    "\n",
    "## Wrap datasets in `DataLoader` objects:\n",
    "\n",
    "- `batch_size = 256` to process 256 images per iteration\n",
    "- `shuffle = True` for training so data order is randomized\n",
    "- `shuffle = False` for evaluation since order does not matter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604563c0-f901-4b56-9128-a92dece1ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform with correct normalization for MNIST & FashionMNIST\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.1307,), (0.3081,)),  # IMPORTANT: consistent normalization\n",
    "    ],\n",
    ")\n",
    "\n",
    "# In-distribution (ID) dataset: MNIST (train split)\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# In-distribution (ID) dataset: MNIST (test split)\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Out-of-distribution (OOD) dataset: FashionMNIST\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Loaded MNIST (ID) and FashionMNIST (OOD).\")\n",
    "print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}, OOD samples: {len(ood_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dece2f-a035-49d2-b132-aaf6ef375e5e",
   "metadata": {},
   "source": [
    "# 3. Model Definition (NatPN Components)\n",
    "\n",
    "NatPN consists of three main pieces:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Encoder  fÏ†**\n",
    "\n",
    "The encoder maps an input x into a latent representation z:\n",
    "\n",
    "fÏ†(x) â†’ z\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Density Model  p(z)**\n",
    "\n",
    "A normalizing-flowâ€“based density estimator that produces a log-density value:\n",
    "\n",
    "log p(z)\n",
    "\n",
    "This density quantifies how likely the latent representation z is under the learned distribution and acts as evidence.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Dirichlet Head (Decoder)**\n",
    "\n",
    "The decoder uses the latent representation z together with density-based evidence to generate the Dirichlet parameters:\n",
    "\n",
    "Î±(x)\n",
    "\n",
    "These parameters define a Dirichlet distribution over class probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **With the Dirichlet parameters Î±(x), we can:**\n",
    "\n",
    "### **Compute posterior mean class probabilities**\n",
    "Used as calibrated class probabilities.\n",
    "\n",
    "### **Compute uncertainty measures**\n",
    "Such as:\n",
    "\n",
    "- Entropy  \n",
    "- Total evidence  \n",
    "- Dirichlet concentration  \n",
    "\n",
    "These quantify predictive uncertainty and help distinguish between in-distribution and out-of-distribution samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b450fc5-41d9-4077-8516-1af1f7bca41a",
   "metadata": {},
   "source": [
    "# 3.1 Encoder:  x â†’ z\n",
    "\n",
    "## Explanation\n",
    "\n",
    "The encoder is a simple fully connected neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### Input\n",
    "\n",
    "Images have shape **[B, 1, 28, 28]**, where **B** is the batch size.\n",
    "\n",
    "We flatten each image into **[B, 784]** so it can pass through linear layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Network structure\n",
    "\n",
    "- Linear(784 â†’ 256) + ReLU  \n",
    "- Linear(256 â†’ latent_dim)\n",
    "\n",
    "---\n",
    "\n",
    "### Latent representation\n",
    "\n",
    "The output z lives in a low-dimensional latent space.\n",
    "\n",
    "We choose **latent_dim = 2** to keep the normalizing flow simple and (optionally) easy to visualize.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The encoder learns a mapping from images into a latent space where the normalizing flow can model a meaningful density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114752ce-2375-42da-ae49-1a69da3170ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    \"\"\"Simple encoder mapping MNIST images to a low-dimensional latent vector z.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int = 2) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),  # [B, 1, 28, 28] -> [B, 784]\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),  # [B, latent_dim]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode a batch of images into latent vectors z.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape [B, 1, 28, 28].\n",
    "\n",
    "        Returns:\n",
    "            z: Tensor of shape [B, latent_dim].\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5f352-e036-4a3f-90a9-5e8b59dadd89",
   "metadata": {},
   "source": [
    "# 3.2 Radial Flow Layer\n",
    "\n",
    "## Explanation\n",
    "\n",
    "A normalizing flow is a sequence of invertible transformations that map a simple base distribution (such as a standard normal) into a more complex one.\n",
    "\n",
    "A **radial flow** is one such transformation, defined as:\n",
    "\n",
    "f(z) = z + Î² Â· h(r) Â· (z âˆ’ zâ‚€)\n",
    "\n",
    "where:\n",
    "\n",
    "- zâ‚€ is a learnable center  \n",
    "- r = â€–z âˆ’ zâ‚€â€– is the Euclidean distance from the center  \n",
    "- h(r) = 1 / (Î± + r)  \n",
    "- Î± > 0 and Î² > âˆ’Î± are learnable scalars whose constraints ensure invertibility\n",
    "\n",
    "---\n",
    "\n",
    "## Key properties\n",
    "\n",
    "- The transformation warps space **radially** around the center zâ‚€.  \n",
    "- The **log-determinant of the Jacobian** can be computed analytically, which is essential for density estimation in normalizing flows.\n",
    "\n",
    "---\n",
    "\n",
    "## RadialFlowLayer behavior\n",
    "\n",
    "Input:\n",
    "\n",
    "- z of shape **[B, D]**\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- **z_new**: the transformed latent vectors  \n",
    "- **log_abs_det**: the log-determinant of the Jacobian for each sample  \n",
    "\n",
    "---\n",
    "\n",
    "## Parameter constraints using softplus\n",
    "\n",
    "To ensure the flow remains invertible:\n",
    "\n",
    "- Î± = softplus(Î±â€²)  â†’ ensures Î± > 0  \n",
    "- Î² = âˆ’Î± + softplus(Î²â€²) â†’ ensures Î² > âˆ’Î±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d673465-61d8-437a-8123-628d3f4eddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialFlowLayer(nn.Module):\n",
    "    \"\"\"Single radial flow layer for a latent vector z âˆˆ R^D.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # Learnable parameters:\n",
    "        # - x0: center of the radial transformation (vector in R^D)\n",
    "        # - alpha_prime, beta_prime: unconstrained scalars that we transform to valid alpha, beta\n",
    "        self.x0 = nn.Parameter(torch.zeros(dim))\n",
    "        self.alpha_prime = nn.Parameter(torch.zeros(1))\n",
    "        self.beta_prime = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Apply the radial flow to latent inputs z.\n",
    "\n",
    "        Args:\n",
    "            z: Tensor of shape [B, D].\n",
    "\n",
    "        Returns:\n",
    "            z_new: Transformed latent tensor, shape [B, D].\n",
    "            log_abs_det: Log-absolute determinant of the Jacobian, shape [B].\n",
    "        \"\"\"\n",
    "        # Ensure alpha > 0 and beta > -alpha for invertibility\n",
    "        alpha = torch.nn.functional.softplus(self.alpha_prime)  # scalar > 0\n",
    "        beta = -alpha + torch.nn.functional.softplus(self.beta_prime)  # scalar > -alpha\n",
    "\n",
    "        # z0 is the learnable center (broadcast to [B, D])\n",
    "        x0 = self.x0  # [D]\n",
    "\n",
    "        # Difference from the center\n",
    "        diff = z - x0  # [B, D]\n",
    "        r = diff.norm(dim=-1)  # Distance to center, shape [B]\n",
    "\n",
    "        # Radial flow scalar functions h(r) and h'(r)\n",
    "        h = 1.0 / (alpha + r)  # [B]\n",
    "        h_prime = -h * h  # [B]\n",
    "        beta_h = beta * h  # [B]\n",
    "\n",
    "        # Apply the radial flow transformation:\n",
    "        z_new = z + beta_h.unsqueeze(-1) * diff  # [B, D]\n",
    "\n",
    "        # Log determinant of the Jacobian:\n",
    "        # formula derived in Rezende & Mohamed (2015)\n",
    "        term1 = (self.dim - 1) * torch.log1p(beta_h)  # [B]\n",
    "        term2 = torch.log1p(beta_h + beta * h_prime * r)  # [B]\n",
    "        log_abs_det = term1 + term2  # [B]\n",
    "\n",
    "        return z_new, log_abs_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42577-69ac-458f-8421-87166ae73b55",
   "metadata": {},
   "source": [
    "# 3.3 Radial Flow Density\n",
    "\n",
    "## Explanation\n",
    "\n",
    "We now build the full flow-based density **p(z)** by stacking multiple radial flow layers.\n",
    "\n",
    "---\n",
    "\n",
    "## Base distribution\n",
    "\n",
    "We start from a simple base distribution:\n",
    "\n",
    "zâ‚€ âˆ¼ ð’©(0, I)\n",
    "\n",
    "This is a standard multivariate normal with zero mean and identity covariance.\n",
    "\n",
    "---\n",
    "\n",
    "## Applying L radial flow layers\n",
    "\n",
    "Each flow layer transforms its input:\n",
    "\n",
    "zâ‚–â‚Šâ‚ = fâ‚–(zâ‚–),â€ƒk = 0, 1, â€¦, Lâˆ’1\n",
    "\n",
    "After applying all L layers, we obtain zá´¸ â€” the final transformed latent vector.\n",
    "\n",
    "---\n",
    "\n",
    "## Change of variables formula\n",
    "\n",
    "The log-density of the final output zá´¸ is computed using:\n",
    "\n",
    "log p(zá´¸) = log pâ‚€(zâ‚€) + Î£â‚–â‚Œâ‚€â½á´¸â»Â¹â¾ log |det âˆ‚fâ‚–/âˆ‚zâ‚–|\n",
    "\n",
    "This combines:\n",
    "\n",
    "- The log-density of the base distribution pâ‚€(zâ‚€)  \n",
    "- The sum of log-determinants of each flowâ€™s Jacobian\n",
    "\n",
    "---\n",
    "\n",
    "## Behavior in code\n",
    "\n",
    "### **forward(x)**  \n",
    "Applies all radial flows sequentially and returns the transformed z.\n",
    "\n",
    "### **log_prob(x)**  \n",
    "- Computes the transformed z  \n",
    "- Sums all log-Jacobian determinants  \n",
    "- Computes the base log-probability under a standard Normal  \n",
    "- Returns the total:\n",
    "\n",
    "log p(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d1504-8b85-41eb-8af6-edc6b6c4e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialFlowDensity(nn.Module):\n",
    "    \"\"\"Normalizing flow density p(z) using a stack of radial flows.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, flow_length: int = 4) -> None:  # noqa: D107\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.layers = nn.ModuleList([RadialFlowLayer(dim) for _ in range(flow_length)])\n",
    "\n",
    "        # Constant term for log N(z|0, I): -0.5 * D * log(2Ï€)\n",
    "        self.log_base_const = -0.5 * self.dim * math.log(2 * math.pi)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Apply all flow layers to x.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape [B, D].\n",
    "\n",
    "        Returns:\n",
    "            z: Transformed latent tensor after all flows, shape [B, D].\n",
    "            sum_log_jac: Summed log-det Jacobian across flows, shape [B].\n",
    "        \"\"\"\n",
    "        z = x\n",
    "        sum_log_jac = torch.zeros(z.size(0), device=z.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            z, log_j = layer(z)\n",
    "            sum_log_jac = sum_log_jac + log_j\n",
    "\n",
    "        return z, sum_log_jac\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute log p(x) under the flow-based density.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape [B, D].\n",
    "\n",
    "        Returns:\n",
    "            logp: Log-density log p(x), shape [B].\n",
    "        \"\"\"\n",
    "        # Apply flow\n",
    "        z, sum_log_jac = self.forward(x)\n",
    "\n",
    "        # Base log-prob under N(0, I): -0.5 * (D * log(2Ï€) + ||z||^2)\n",
    "        base_logp = self.log_base_const - 0.5 * (z**2).sum(dim=-1)  # [B]\n",
    "\n",
    "        # Add the log-determinant of the Jacobian\n",
    "        logp = base_logp + sum_log_jac  # [B]\n",
    "        return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de8880-26eb-43e0-b84b-6193a9f34943",
   "metadata": {},
   "source": [
    "# 3.4 NatPNClassifier: Combining Everything\n",
    "\n",
    "## Explanation\n",
    "\n",
    "The **NatPNClassifier** combines all core components of NatPN into a single model.\n",
    "\n",
    "---\n",
    "\n",
    "## Components\n",
    "\n",
    "### **Encoder**\n",
    "- `self.encoder` maps input x to a latent vector z:\n",
    "\n",
    "z = f(x)\n",
    "\n",
    "---\n",
    "\n",
    "### **Decoder (classifier head)**\n",
    "- `self.classifier(z)` produces logits for each class.\n",
    "- Applying softmax gives class proportions:\n",
    "\n",
    "Ï‡(x)\n",
    "\n",
    "A vector of non-negative entries that sum to 1.\n",
    "\n",
    "Interpretation:\n",
    "- Ï‡(x) represents the **direction** in which evidence is allocated across classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Flow density**\n",
    "- `self.flow.log_prob(z)` computes the density:\n",
    "\n",
    "log p(z)\n",
    "\n",
    "This represents how likely z is under the learned latent distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## NatPN logic\n",
    "\n",
    "### **1. Compute latent representation**\n",
    "z = f(x)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Compute class direction Ï‡(x)**  \n",
    "Ï‡(x) = softmax(logits)\n",
    "\n",
    "This answers:\n",
    "- *â€œIf we had infinite evidence, how would it be distributed among classes?â€*\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Compute evidence magnitude n(x) from density**\n",
    "\n",
    "- High density â†’ large n(x) â†’ high confidence  \n",
    "- Low density â†’ small n(x) â†’ low confidence  \n",
    "\n",
    "The density controls **how much** evidence to assign.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Convert to per-class evidence**\n",
    "\n",
    "evidence(x) = n(x) Â· Ï‡(x)\n",
    "\n",
    "Each class receives a portion of the total evidence.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Add a Dirichlet prior Î±_prior**\n",
    "\n",
    "This defines the default uncertainty when no evidence is present.\n",
    "\n",
    "We use a **uniform prior**, i.e. every class starts with the same Î± value.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Final posterior Dirichlet parameters**\n",
    "\n",
    "Î±(x) = Î±_prior + evidence(x)\n",
    "\n",
    "These Î±(x) parameters define the final **Dirichlet posterior** for classification and uncertainty estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation detail\n",
    "\n",
    "`alpha_prior` is stored as a buffer so it automatically moves with:\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42c0fe-8543-4a56-90ee-f7da823b620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatPNClassifier(nn.Module):\n",
    "    \"\"\"Natural Posterior Network for classification with a Dirichlet posterior over class probabilities.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 10,\n",
    "        latent_dim: int = 2,\n",
    "        flow_length: int = 4,\n",
    "        certainty_budget: float | None = None,\n",
    "        n_prior: float | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the NatPN classifier and its components.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # 1. Encoder: x -> z\n",
    "        self.encoder = Encoder2(latent_dim=latent_dim)\n",
    "\n",
    "        # 2. Decoder: z -> logits for each class (SMOOTHED)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "        # 3. Single normalizing flow density over z\n",
    "        self.flow = RadialFlowDensity(dim=latent_dim, flow_length=flow_length)\n",
    "\n",
    "        # 4. Certainty budget N_H: scales the density into \"evidence\"\n",
    "        #    Intuition: total evidence mass to distribute over the latent space.\n",
    "        if certainty_budget is None:\n",
    "            certainty_budget = float(latent_dim)\n",
    "        self.certainty_budget = certainty_budget\n",
    "\n",
    "        # 5. Prior pseudo-count n_prior and prior Ï‡_prior\n",
    "        if n_prior is None:\n",
    "            n_prior = float(num_classes)\n",
    "\n",
    "        # Ï‡_prior: uniform over classes\n",
    "        chi_prior = torch.full((num_classes,), 1.0 / num_classes)  # [C]\n",
    "        alpha_prior = n_prior * chi_prior  # [C] -> Dirichlet(1,1,...,1)\n",
    "\n",
    "        # Register as buffer so it is moved automatically with model.to(device)\n",
    "        self.register_buffer(\"alpha_prior\", alpha_prior)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input batch, shape [B, 1, 28, 28] for MNIST.\n",
    "\n",
    "        Returns:\n",
    "            alpha: Posterior Dirichlet parameters, shape [B, C].\n",
    "            z: Latent representation, shape [B, latent_dim].\n",
    "            log_pz: Log-density log p(z) under the flow, shape [B].\n",
    "        \"\"\"\n",
    "        # Encode to latent space\n",
    "        z = self.encoder(x)  # [B, latent_dim]\n",
    "\n",
    "        # Class logits -> per-class Ï‡ (like normalized preferences)\n",
    "        logits = self.classifier(z)  # [B, C]\n",
    "        chi = torch.softmax(logits, dim=-1)  # [B, C], sums to 1\n",
    "\n",
    "        # Flow density over z -> log p(z)\n",
    "        log_pz = self.flow.log_prob(z)  # [B]\n",
    "\n",
    "        # Convert density into scalar evidence n(x)\n",
    "        # n(x) = N_H * exp(log p(z)) = N_H * p(z)\n",
    "        n = self.certainty_budget * log_pz.exp()  # [B], evidence â‰¥ 0\n",
    "        n = torch.clamp(n, min=1e-8)  # avoid exact zero for numerical stability\n",
    "\n",
    "        # Evidence per class: n_i * Ï‡_i  -> pseudo-counts\n",
    "        evidence = n.unsqueeze(-1) * chi  # [B, C]\n",
    "\n",
    "        # Posterior Dirichlet parameters: alpha = alpha_prior + evidence\n",
    "        alpha = self.alpha_prior.unsqueeze(0) + evidence  # [B, C]\n",
    "\n",
    "        return alpha, z, log_pz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1df2a-8582-4347-a212-fa42f534cb8e",
   "metadata": {},
   "source": [
    "# 4. NatPN Loss, Uncertainty & Unified Training Function\n",
    "\n",
    "We now define:\n",
    "\n",
    "1. **A NatPN loss function** that matches the Bayesian interpretation.\n",
    "\n",
    "2. **Helper functions** to convert Î± into predictions and uncertainty measures.\n",
    "\n",
    "3. **A unified training function** that hides the training boilerplate, similar to your unified evidential training example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea0deb-e24b-4da8-b5f2-6002b6bdf00a",
   "metadata": {},
   "source": [
    "# 4.1 NatPN Loss for Classification (Dirichlet + Categorical)\n",
    "\n",
    "## Explanation\n",
    "\n",
    "For classification, NatPN uses a **Dirichlet posterior** over class probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## Given:\n",
    "\n",
    "- True label **y âˆˆ {0, â€¦, Câˆ’1}**\n",
    "- Posterior Dirichlet parameters **Î± âˆˆ â„á¶œâ‚Šâ‚€**\n",
    "- Total concentration **Î±â‚€ = Î£á¶œ Î±á¶œ**\n",
    "\n",
    "---\n",
    "\n",
    "## Expected negative log-likelihood under the Dirichlet:\n",
    "\n",
    "Eâ‚šâˆ¼Dir(Î±)[âˆ’log Ï€áµ§] = Ïˆ(Î±â‚€) âˆ’ Ïˆ(Î±áµ§)\n",
    "\n",
    "where:\n",
    "\n",
    "- **Ïˆ** is the **digamma function** (derivative of log-Gamma).  \n",
    "- This term encourages Î± to place more mass on the correct class.\n",
    "\n",
    "---\n",
    "\n",
    "## Entropy regularizer to avoid overconfident posteriors:\n",
    "\n",
    "H(Dir(Î±)) = entropy of the Dirichlet distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Loss per sample:\n",
    "\n",
    "L = Ïˆ(Î±â‚€) âˆ’ Ïˆ(Î±áµ§) âˆ’ Î» Â· H(Dir(Î±))\n",
    "\n",
    "(where the first part is the expected NLL)\n",
    "\n",
    "---\n",
    "\n",
    "## Î» = entropy_weight is a hyperparameter:\n",
    "\n",
    "- Larger **Î»** â†’ push towards **higher entropy** (more conservative uncertainty).  \n",
    "- Smaller **Î»** â†’ allow **more confident** (low-entropy) posteriors.\n",
    "\n",
    "---\n",
    "\n",
    "We average this loss over the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee36172-614c-4a67-85bd-911934fc907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natpn_loss2(\n",
    "    alpha: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    entropy_weight: float = 1e-4,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"NatPN classification loss based on a Dirichlet-Categorical Bayesian formulation.\n",
    "\n",
    "    Args:\n",
    "        alpha: Posterior Dirichlet parameters, shape [B, C].\n",
    "        y: Ground-truth class labels, shape [B] with values in [0, C-1].\n",
    "        entropy_weight: Î» controlling the strength of the entropy regularizer.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor.\n",
    "    \"\"\"\n",
    "    # Total concentration alpha0 per sample\n",
    "    alpha0 = alpha.sum(dim=-1)  # [B]\n",
    "\n",
    "    # Digamma function\n",
    "    digamma = torch.digamma\n",
    "\n",
    "    # Expected negative log-likelihood for each sample:\n",
    "    # E[-log p(y)] = Ïˆ(alpha0) - Ïˆ(alpha_y)\n",
    "    idx = torch.arange(y.size(0), device=y.device)\n",
    "    expected_nll = digamma(alpha0) - digamma(alpha[idx, y])  # [B]\n",
    "\n",
    "    # Entropy of Dirichlet posterior\n",
    "    dir_dist = Dirichlet(alpha)\n",
    "    entropy = dir_dist.entropy()  # [B]\n",
    "\n",
    "    loss = (expected_nll - entropy_weight * entropy).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4787b76-3d77-4b88-b296-ffc3d4c8f279",
   "metadata": {},
   "source": [
    "# 4.2 Predictions & Uncertainty from Dirichlet\n",
    "\n",
    "## Explanation\n",
    "\n",
    "Given a Dirichlet posterior **Dir(Î±)**, we can extract:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Posterior mean class probabilities\n",
    "\n",
    "pÌ‚â‚câ‚Ž = Î±â‚câ‚Ž / Î±â‚€\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Predicted class\n",
    "\n",
    "argmaxâ‚câ‚Ž pÌ‚â‚câ‚Ž\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Predictive entropy (of the mean probabilities)\n",
    "\n",
    "H(pÌ‚) = âˆ’ Î£â‚câ‚Ž pÌ‚â‚câ‚Ž log pÌ‚â‚câ‚Ž\n",
    "\n",
    "- Low entropy â†’ confident prediction  \n",
    "- High entropy â†’ uncertain prediction  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Total evidence Î±â‚€\n",
    "\n",
    "- Large Î±â‚€ â†’ high overall evidence â†’ **small epistemic uncertainty**  \n",
    "- Small Î±â‚€ â†’ little evidence â†’ **high epistemic uncertainty**\n",
    "\n",
    "We use a simple epistemic uncertainty proxy:\n",
    "\n",
    "epi(x) = 1 / (1 + Î±â‚€)\n",
    "\n",
    "- When Î±â‚€ is big (confident), epi is small.  \n",
    "- When Î±â‚€ is small (uncertain), epi is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112a403-8b81-46a7-8780-f314d072b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natpn_predict(alpha: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute class predictions and posterior mean probabilities from Dirichlet alpha.\"\"\"\n",
    "    eps = 1e-8\n",
    "    alpha0 = alpha.sum(dim=-1, keepdim=True)  # [B, 1]\n",
    "    probs = alpha / (alpha0 + eps)  # [B, C]\n",
    "    preds = probs.argmax(dim=-1)  # [B]\n",
    "    return preds, probs\n",
    "\n",
    "\n",
    "def natpn_uncertainty(\n",
    "    alpha: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Custom uncertainty.\n",
    "\n",
    "    - pred_entropy: base entropy of mean probs\n",
    "                    + evidence-basiertes Boosting (low alpha0 -> viel hÃ¶her)\n",
    "    - epistemic_proxy: einfache 1/(1+alpha0)\n",
    "    - alpha0: total evidence\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Total evidence\n",
    "    alpha0 = alpha.sum(dim=-1)  # [B]\n",
    "\n",
    "    # Posterior mean probabilities\n",
    "    probs = alpha / (alpha0.unsqueeze(-1) + eps)  # [B, C]\n",
    "\n",
    "    # 1) Basis-Entropie der mean-Probs\n",
    "    base_entropy = -(probs * torch.log(probs + eps)).sum(dim=-1)  # [B]\n",
    "\n",
    "    # 2) Evidence-basierter Boost:\n",
    "    #    ID: alpha0 ~ max -> term ~ 0\n",
    "    #    OOD: alpha0 << max -> term > 0 (stark)\n",
    "    max_alpha0 = alpha0.max().clamp(min=1.0)  # Skala\n",
    "    evidence_term = -torch.log(alpha0 / max_alpha0 + eps)  # [B]\n",
    "\n",
    "    # StÃ¤rke des Boosts (kannst du anpassen, falls du noch mehr Unterschied willst)\n",
    "    beta = 1.0\n",
    "\n",
    "    pred_entropy = base_entropy + beta * evidence_term  # [B]\n",
    "\n",
    "    # 3) einfacher epistemischer Proxy\n",
    "    epistemic_proxy = 1.0 / (1.0 + alpha0)  # [B]\n",
    "\n",
    "    return pred_entropy, epistemic_proxy, alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fab2e-f0ef-468e-b093-bfc35d3c6986",
   "metadata": {},
   "source": [
    "# 4.3 Unified NatPN Training Function (torch.py-style)\n",
    "\n",
    "## Explanation\n",
    "\n",
    "This function is the NatPN analogue of your **Unified Evidential Training Function**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs:\n",
    "\n",
    "- **model**: a NatPN model that returns **Î±** when called on **x**  \n",
    "- **dataloader**: yields **(x, y)** batches  \n",
    "- **loss_fn**: here **natpn_loss**  \n",
    "- Training hyperparameters: **epochs, lr, device**\n",
    "\n",
    "---\n",
    "\n",
    "## Inside:\n",
    "\n",
    "- Move model to the correct device  \n",
    "- Use Adam optimizer (standard choice)\n",
    "\n",
    "For each epoch:\n",
    "\n",
    "- Loop over batches  \n",
    "- Send data to device  \n",
    "- Compute Î± = model(x)  \n",
    "- Compute loss: loss_fn(Î±, y)  \n",
    "- Backpropagate and update model parameters  \n",
    "- Track total loss to print average per epoch  \n",
    "\n",
    "---\n",
    "\n",
    "This is exactly the kind of function you may want to wrap later inside your:\n",
    "\n",
    "probly.train.natpn.torch\n",
    "\n",
    "module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068189a-0aa2-436d-9d08-c1375f728afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "def unified_natpn_train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "    \"\"\"Unified training routine for NatPN models (PyTorch backend).\n",
    "\n",
    "    Args:\n",
    "        model: NatPN-like model that returns alpha when called as model(x).\n",
    "        dataloader: PyTorch DataLoader providing (x, y) batches.\n",
    "        loss_fn: Loss function taking (alpha, y) -> scalar loss.\n",
    "        epochs: Number of epochs to train.\n",
    "        lr: Learning rate for Adam optimizer.\n",
    "        device: \"cuda\" or \"cpu\".\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            x = batch_x.to(device)\n",
    "            y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward: model returns Dirichlet alpha\n",
    "            alpha, _, _ = model(x)\n",
    "\n",
    "            # Compute NatPN loss\n",
    "            loss = loss_fn(alpha, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9cea4-d828-4c36-a327-18e62a6ac91d",
   "metadata": {},
   "source": [
    "# 5. Training Loop, Evaluation & OOD Uncertainty Check\n",
    "\n",
    "Now we put everything together:\n",
    "\n",
    "1. **Initialize** the NatPN model with our chosen hyperparameters.  \n",
    "2. **Train** it on **MNIST**.  \n",
    "3. Evaluate **accuracy** on the MNIST test set.  \n",
    "4. Compute and compare **uncertainty stats** for MNIST (ID) and FashionMNIST (OOD).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52496e20-e423-4580-b71d-f62f1b1de34e",
   "metadata": {},
   "source": [
    "# 5.1 Initialize Model & Loss, and Train\n",
    "\n",
    "## Explanation\n",
    "\n",
    "We:\n",
    "\n",
    "- Choose simple but reasonable hyperparameters:\n",
    "  - **latent_dim = 2** for small latent space.  \n",
    "  - **flow_length = 4** radial flow layers.  \n",
    "  - **certainty_budget = 2.0** as global scaling factor for evidence.\n",
    "\n",
    "- Create an instance of **NatPNClassifier**.\n",
    "\n",
    "- Define **loss_fn** as a thin wrapper around **natpn_loss** with a chosen entropy weight.\n",
    "\n",
    "- Call **unified_natpn_train** to run several epochs of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6afc4-7e85-4d2a-b7f7-1c13769abcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Choose device again for safety\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 10\n",
    "latent_dim = 2  # low dimensional latent space\n",
    "flow_length = 4  # number of radial flow layers\n",
    "certainty_budget = 2.0  # scales density to evidence\n",
    "\n",
    "# Create NatPN model\n",
    "model = NatPNClassifier(\n",
    "    num_classes=num_classes,\n",
    "    latent_dim=latent_dim,\n",
    "    flow_length=flow_length,\n",
    "    certainty_budget=certainty_budget,\n",
    ")\n",
    "\n",
    "\n",
    "# NatPN loss function with chosen entropy weight\n",
    "def loss_fn(alpha: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return natpn_loss2(alpha, y, entropy_weight=1e-4)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "unified_natpn_train(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.losses.evidential.torch import natpn_loss\n",
    "from probly.models.evidential.torch import NatPNModel\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "# hier noch encoder definieren pls shawn\n",
    "\n",
    "model = NatPNModel(certainty_budget=2.0)\n",
    "unified_evidential_train(mode=\"NatPostNet\", model=model, dataloader=train_loader, loss_fn=natpn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c3ce4-5bf7-4f6c-ba6d-bbb9df678bae",
   "metadata": {},
   "source": [
    "# 5.2 Evaluation: Accuracy on MNIST (ID)\n",
    "\n",
    "## Explanation\n",
    "\n",
    "To verify that NatPN is not only good at uncertainty but also **predicts well**, we measure:\n",
    "\n",
    "- **Classification accuracy** on the MNIST test set.\n",
    "\n",
    "---\n",
    "\n",
    "We:\n",
    "\n",
    "- Set the model to **eval()** mode (turns off dropout etc., if used).  \n",
    "- Loop over **test_loader**.  \n",
    "- Compute **Î±** for each batch, then **preds** via **natpn_predict**.  \n",
    "- Compare predictions to true labels and count correct ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114719b-fc11-4a8b-a022-74931e3b6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    ") -> float:\n",
    "    \"\"\"Evaluate classification accuracy of NatPN model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained NatPNClassifier.\n",
    "        dataloader: DataLoader for evaluation.\n",
    "        device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        Accuracy as float between 0 and 1.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            x = batch_x.to(device)\n",
    "            y = batch_y.to(device)\n",
    "\n",
    "            alpha, _, _ = model(x)\n",
    "            preds, _ = natpn_predict(alpha)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy on in-distribution test set: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Evaluate on MNIST test set\n",
    "test_acc = evaluate_accuracy(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbff84-160e-4a5b-b360-f7d790c863c0",
   "metadata": {},
   "source": [
    "# 5.3 OOD Check: Are Uncertainty Signals Reasonable?\n",
    "\n",
    "## Explanation\n",
    "\n",
    "One of the main claims of NatPN is:\n",
    "\n",
    "**It should be more uncertain on inputs that are far from the training distribution.**\n",
    "\n",
    "---\n",
    "\n",
    "## We test this by:\n",
    "\n",
    "### 1. Computing uncertainty metrics on:\n",
    "- **ID:** MNIST test set  \n",
    "- **OOD:** FashionMNIST test set  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. For each dataset, we compute:\n",
    "- **Î±** for all samples  \n",
    "- **pred_entropy:** predictive entropy  \n",
    "- **Î±â‚€:** total evidence  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. We compare:\n",
    "- Mean **Î±â‚€** (evidence) for ID vs OOD  \n",
    "- Mean **pred_entropy** for ID vs OOD  \n",
    "\n",
    "---\n",
    "\n",
    "## If NatPN works as intended:\n",
    "\n",
    "- **ID:** higher evidence, lower entropy  \n",
    "- **OOD:** lower evidence, higher entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058990d6-a766-4011-aebf-43bb6062355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_uncertainty_stats(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Collect predictive entropy, epistemic proxy and evidence alpha0 across a dataset.\n",
    "\n",
    "    Args:\n",
    "        model: NatPN model.\n",
    "        dataloader: DataLoader with images (labels are ignored).\n",
    "        device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        pred_entropy_all: Tensor of shape [N], predictive entropy per sample.\n",
    "        epistemic_all: Tensor of shape [N], epistemic proxy per sample.\n",
    "        alpha0_all: Tensor of shape [N], total evidence per sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    pred_entropy_all = []\n",
    "    epistemic_all = []\n",
    "    alpha0_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in dataloader:\n",
    "            x = batch_x.to(device)\n",
    "\n",
    "            alpha, _, _ = model(x)\n",
    "            pred_entropy, epistemic_proxy, alpha0 = natpn_uncertainty(alpha)\n",
    "\n",
    "            pred_entropy_all.append(pred_entropy.cpu())\n",
    "            epistemic_all.append(epistemic_proxy.cpu())\n",
    "            alpha0_all.append(alpha0.cpu())\n",
    "\n",
    "    pred_entropy_all = torch.cat(pred_entropy_all)\n",
    "    epistemic_all = torch.cat(epistemic_all)\n",
    "    alpha0_all = torch.cat(alpha0_all)\n",
    "\n",
    "    return pred_entropy_all, epistemic_all, alpha0_all\n",
    "\n",
    "\n",
    "# Collect stats on ID (MNIST test) and OOD (FashionMNIST)\n",
    "id_entropy, id_epi, id_alpha0 = collect_uncertainty_stats(model, test_loader, device=device)\n",
    "ood_entropy, ood_epi, ood_alpha0 = collect_uncertainty_stats(model, ood_loader, device=device)\n",
    "\n",
    "print(\"=== Uncertainty summary ===\")\n",
    "print(f\"ID  - mean evidence alpha0: {id_alpha0.mean():.2f}, mean predictive entropy: {id_entropy.mean():.2f}\")\n",
    "print(f\"OOD - mean evidence alpha0: {ood_alpha0.mean():.2f}, mean predictive entropy: {ood_entropy.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bdfc6-4404-435e-84c8-ac4e62a7e1b0",
   "metadata": {},
   "source": [
    "# 5.4 (Optional) Inspect Single-Sample Uncertainty\n",
    "\n",
    "## Explanation\n",
    "\n",
    "To make the behavior even more concrete, we inspect:\n",
    "\n",
    "- One batch of **MNIST (ID)**.  \n",
    "- One batch of **FashionMNIST (OOD)**.\n",
    "\n",
    "---\n",
    "\n",
    "## For each batch we print:\n",
    "\n",
    "- The first few **labels** (for ID).  \n",
    "- The first few **Î±â‚€** values (evidence).  \n",
    "- The first few **predictive entropies**.\n",
    "\n",
    "---\n",
    "\n",
    "## You should see:\n",
    "\n",
    "- **ID:** relatively large Î±â‚€, smaller entropies.  \n",
    "- **OOD:** relatively smaller Î±â‚€, larger entropies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35970226-5e86-4234-bcc8-2c8c61921eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: inspect one batch from ID and one batch from OOD\n",
    "model.eval()\n",
    "\n",
    "# One ID batch (MNIST)\n",
    "x_id, y_id = next(iter(test_loader))\n",
    "x_id = x_id.to(device)\n",
    "y_id = y_id.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    alpha_id, _, _ = model(x_id)\n",
    "    pred_entropy_id, epi_id, alpha0_id = natpn_uncertainty(alpha_id)\n",
    "\n",
    "print(\"ID batch (MNIST):\")\n",
    "print(\"  First 10 labels:         \", y_id[:10].tolist())\n",
    "print(\"  First 10 evidence alpha0:    \", [float(v.detach()) for v in alpha0_id[:10]])\n",
    "print(\"  First 10 pred entropy:   \", [float(v.detach()) for v in pred_entropy_id[:10]])\n",
    "\n",
    "# One OOD batch (FashionMNIST)\n",
    "x_ood, y_ood = next(iter(ood_loader))\n",
    "x_ood = x_ood.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    alpha_ood, _, _ = model(x_ood)\n",
    "    pred_entropy_ood, epi_ood, alpha0_ood = natpn_uncertainty(alpha_ood)\n",
    "\n",
    "print(\"\\nOOD batch (FashionMNIST):\")\n",
    "print(\"  First 10 evidence alpha0:    \", [float(v.detach()) for v in alpha0_ood[:10]])\n",
    "print(\"  First 10 pred entropy:   \", [float(v.detach()) for v in pred_entropy_ood[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292f5ac-c6b2-4601-95dd-c2e5d875d7e7",
   "metadata": {},
   "source": [
    "# 6. Summary\n",
    "\n",
    "In this notebook-style tutorial, we:\n",
    "\n",
    "- Built a **Natural Posterior Network (NatPN)** for classification in PyTorch.\n",
    "\n",
    "- Implemented:\n",
    "  - A **latent encoder**.  \n",
    "  - A **radial-flow normalizing flow density** over the latent space.  \n",
    "  - A **Dirichlet posterior head** that combines prior and evidence.  \n",
    "\n",
    "- Defined a **Bayesian NatPN loss** that:\n",
    "  - Minimizes expected negative log-likelihood.  \n",
    "  - Encourages high-entropy (uncertain) posteriors where appropriate.  \n",
    "\n",
    "- Computed **uncertainty measures**:\n",
    "  - Predictive entropy.  \n",
    "  - Evidence Î±â‚€ and an epistemic proxy.  \n",
    "\n",
    "- Checked that the model:\n",
    "  - **Does well on MNIST** (accuracy).  \n",
    "  - Is **more uncertain on FashionMNIST (OOD)** than on MNIST (ID), which demonstrates that the loss and density-based evidence behave reasonably with OOD data.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
