{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Evaluation with Ensembles on FashionMNIST\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- train a small ensemble on FashionMNIST,\n",
    "- use ensemble-based uncertainty (mutual information) as an OOD score,\n",
    "- evaluate OOD performance using updated metric definitions via a unified API,\n",
    "- interpret OOD-related scores and metrics in practice.\n",
    "\n",
    "Reference: `fashionmnist_ood_ensemble.ipynb` in `probly/notebooks/examples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup and library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "from probly.evaluation.tasks import out_of_distribution_detection\n",
    "from probly.quantification.classification import mutual_information\n",
    "from probly.transformation import ensemble\n",
    "from probly.evaluation.ood import evaluate_ood  # unified OOD metric API\n",
    "\n",
    "transforms = T.Compose([T.ToTensor(), torch.flatten])\n",
    "\n",
    "train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets/\", train=True, download=True, transform=transforms\n",
    ")\n",
    "test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets/\", train=False, download=True, transform=transforms\n",
    ")\n",
    "train_loader = DataLoader(train, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=256, shuffle=False)\n",
    "\n",
    "ood = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets/\", train=False, download=True, transform=transforms\n",
    ")\n",
    "ood_loader = DataLoader(ood, batch_size=256, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network architecture and construct an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "ensemble_model = ensemble(Net().to(device), 5)\n",
    "ensemble_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the ensemble members on FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for model in tqdm(ensemble_model, desc=\"Training ensemble\"):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for _ in range(10):  \n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ensemble accuracy on FashionMNIST test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "ensemble_model.eval()\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    outputs = []\n",
    "    for model in ensemble_model:\n",
    "        outputs.append(torch.softmax(model(inputs), dim=1))\n",
    "    outputs = torch.stack(outputs, dim=1).mean(dim=1)\n",
    "\n",
    "    correct += (outputs.argmax(1) == targets).sum().item()\n",
    "    total += targets.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on FashionMNIST test set: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect ensemble outputs on ID and OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def torch_get_outputs(model: nn.Module, loader: DataLoader) -> torch.Tensor:\n",
    "    outputs = []\n",
    "    for data, _ in loader:\n",
    "        data = data.to(device)\n",
    "        out_members = []\n",
    "        for m in model:\n",
    "            out_members.append(torch.softmax(m(data), dim=1))\n",
    "        out_members = torch.stack(out_members, dim=1)\n",
    "        outputs.append(out_members)\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute uncertainty scores using mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.eval()\n",
    "\n",
    "outputs_id = torch_get_outputs(ensemble_model, test_loader)\n",
    "outputs_ood = torch_get_outputs(ensemble_model, ood_loader)\n",
    "\n",
    "outputs_id = outputs_id.cpu().numpy()\n",
    "outputs_ood = outputs_ood.cpu().numpy()\n",
    "\n",
    "uncertainty_id = mutual_information(outputs_id)\n",
    "uncertainty_ood = mutual_information(outputs_ood)\n",
    "\n",
    "len(uncertainty_id), len(uncertainty_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Ensemble-based Uncertainty\n",
    "\n",
    "We use **mutual information** over the ensemble's predictive distribution as an\n",
    "uncertainty measure:\n",
    "\n",
    "- low mutual information → model predictions are consistent across ensemble members  \n",
    "- high mutual information → ensemble members disagree, indicating uncertainty  \n",
    "\n",
    "When FashionMNIST acts as in-distribution (ID) and MNIST as out-of-distribution (OOD),\n",
    "we expect:\n",
    "\n",
    "- **ID samples**: lower uncertainty  \n",
    "- **OOD samples**: higher uncertainty  \n",
    "\n",
    "Next, we visualize the distributions of these uncertainty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize uncertainty distributions for ID vs. OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(uncertainty_id, bins=50, alpha=0.5, label=\"In-Distribution (FashionMNIST)\")\n",
    "plt.hist(uncertainty_ood, bins=50, alpha=0.5, label=\"Out-of-Distribution (MNIST)\")\n",
    "plt.xlabel(\"Mutual Information (uncertainty)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Uncertainty distributions: ID vs OOD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AUROC using the classical OOD detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_legacy = out_of_distribution_detection(uncertainty_id, uncertainty_ood)\n",
    "print(f\"Legacy AUROC with FashionMNIST as ID and MNIST as OOD: {auroc_legacy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated OOD Metrics (Unified API)\n",
    "\n",
    "We now use the unified `evaluate_ood` API, which supports:\n",
    "\n",
    "- **Static metrics**\n",
    "  - `auroc` – Area under ROC\n",
    "  - `aupr` – Area under Precision–Recall curve\n",
    "  - `fpr@95` – False Positive Rate when True Positive Rate is 95%\n",
    "\n",
    "- **Dynamic metrics** (string specifications)\n",
    "  - `fpr@0.8`   → FPR at TPR = 0.8\n",
    "  - `fnr@90%`   → FNR at TPR = 90%\n",
    "  - `tnr@0.99`  → True Negative Rate at TPR = 0.99  \n",
    "\n",
    "The API expects:\n",
    "\n",
    "evaluate_ood(in_distribution_scores, out_distribution_scores, metrics=...)\n",
    "\n",
    "Where **higher scores indicate more likely OOD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate updated OOD metrics using the unified API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = evaluate_ood(\n",
    "    in_distribution=uncertainty_id,\n",
    "    out_distribution=uncertainty_ood,\n",
    "    metrics=\"all\",\n",
    ")\n",
    "\n",
    "metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate selected OOD metrics and interpret operational meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_selected = evaluate_ood(\n",
    "    in_distribution=uncertainty_id,\n",
    "    out_distribution=uncertainty_ood,\n",
    "    metrics=[\"auroc\", \"aupr\", \"fpr@95\", \"fpr@0.8\", \"fnr@90%\", \"tnr@0.95\"],\n",
    ")\n",
    "\n",
    "for name, value in metrics_selected.items():\n",
    "    print(f\"{name:8s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting OOD Metrics\n",
    "\n",
    "Given `uncertainty_id` (FashionMNIST) and `uncertainty_ood` (MNIST):\n",
    "\n",
    "- **AUROC**\n",
    "  - Values close to 1.0 indicate that ID and OOD are well separated.\n",
    "  - Here, a high AUROC means OOD samples tend to have higher uncertainty.\n",
    "\n",
    "- **AUPR**\n",
    "  - Focuses on the positive class (OOD) and is especially useful when OOD is rare.\n",
    "  - A high AUPR indicates that among the samples flagged as OOD, many are truly OOD.\n",
    "\n",
    "- **FPR@95**\n",
    "  - False Positive Rate at TPR = 95%.\n",
    "  - Operational view: “If we want to catch 95% of OOD samples, how many ID samples\n",
    "    will we incorrectly mark as OOD?”\n",
    "\n",
    "- **FPR@0.8**\n",
    "  - False Positive Rate at TPR = 80%.\n",
    "  - Lowering the TPR requirement often reduces FPR.\n",
    "\n",
    "- **FNR@90%**\n",
    "  - False Negative Rate at TPR = 90%.\n",
    "  - Fraction of OOD samples that are missed at a relatively high detection rate.\n",
    "\n",
    "- **TNR@0.95**\n",
    "  - True Negative Rate at TPR = 0.95.\n",
    "  - Fraction of ID samples correctly kept as ID when we still detect 95% of OOD samples.\n",
    "\n",
    "These metrics together provide a richer view than AUROC alone:\n",
    "they describe not only separability but also **operating points** that matter in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
