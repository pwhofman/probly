{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc33501",
   "metadata": {},
   "source": [
    "# probly Tutorial — Dropout Transformation (Part A)\n",
    "\n",
    "*Date:* 2025-11-03\n",
    "\n",
    "**Audience:** New probly users · **Framework:** PyTorch · **Author:** Nidhi Jain and Julia\n",
    "\n",
    "This notebook is meant as a gentle, practical introduction to the **Dropout transformation** in `probly`.\n",
    "The goal is not to be mathematically perfect, but to give you an intuition you can actually use when you\n",
    "work on models in PyTorch.\n",
    "\n",
    "We will slowly build up from the very basic idea of *normal* Dropout to the slightly more advanced idea of\n",
    "a **Dropout transformation that makes a model uncertainty‑aware**. After that, we look at a tiny PyTorch\n",
    "example and inspect how the transformation changes the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52834e",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Dropout (normal) vs Dropout Transformation?\n",
    "\n",
    "The original question for this part is:\n",
    "\n",
    "> **“What is Dropout (normal) vs Dropout Transformation?”**  \n",
    "> **1.1 Normal Dropout (no probly, just PyTorch)**  \n",
    "> In “normal” deep learning, Dropout is a layer used during training to reduce overfitting.  \n",
    "> Overfitting = model memorizes training data and sucks on new data.\n",
    "\n",
    "Below is a more detailed version of that explanation, in my own words.\n",
    "\n",
    "### 1.1 Normal Dropout (standard PyTorch Dropout layer)\n",
    "\n",
    "When we train a neural network, there is always the risk of **overfitting**. That means the model becomes\n",
    "very good at the training set but fails on new data, because it has more or less *memorised* patterns that\n",
    "only appear in the training examples.\n",
    "\n",
    "**Normal Dropout** is a simple trick to make overfitting less likely. During training, a `Dropout(p)` layer\n",
    "will, for every mini‑batch, randomly set a fraction `p` of its input activations to zero. You can imagine\n",
    "this as:\n",
    "\n",
    "- with probability `p` a neuron is “switched off” for this training step,\n",
    "- with probability `1 − p` it behaves as usual.\n",
    "\n",
    "Because different neurons get switched off in every step, the network is forced to **spread the information**\n",
    "across many neurons instead of relying on a few very strong ones. This usually makes the model **more robust**\n",
    "and helps it generalise better.\n",
    "\n",
    "Important detail: in **normal PyTorch usage**\n",
    "\n",
    "- Dropout is **active only in training mode** (`model.train()`),\n",
    "- and it is **disabled in evaluation mode** (`model.eval()`).\n",
    "\n",
    "So at test / inference time, the model behaves like a **deterministic function**: the same input always gives\n",
    "the same output, and there is no randomness from Dropout anymore. The purpose of normal Dropout is therefore\n",
    "*only* to improve generalisation during training, not to provide uncertainty information.\n",
    "\n",
    "### 1.2 Dropout Transformation (probly)\n",
    "\n",
    "The **Dropout transformation** in `probly` takes this Dropout idea and uses it in a slightly different role.\n",
    "Instead of treating Dropout purely as a regularisation trick during training, we use it to make the model\n",
    "**uncertainty‑aware** at prediction time.\n",
    "\n",
    "Roughly speaking, the transformation does the following:\n",
    "\n",
    "- It walks through your PyTorch model and finds the relevant linear layers.\n",
    "- It programmatically inserts Dropout layers around those linear layers.\n",
    "- Crucially, these Dropout layers stay **active during inference**, so each forward pass is a bit different.\n",
    "\n",
    "If we now feed the **same input** through the transformed model multiple times, we do **not** get exactly the\n",
    "same output each time. Instead we get a *cloud* of slightly different predictions. From this cloud we can:\n",
    "\n",
    "- compute a mean prediction (what the model “on average” thinks), and\n",
    "- look at how much the predictions vary (this variation is a proxy for **uncertainty**).\n",
    "\n",
    "So the Dropout transformation reuses the usual Dropout mechanism, but with a **different goal**:\n",
    "\n",
    "- normal Dropout: better training, less overfitting, Dropout OFF in eval mode;\n",
    "- Dropout transformation: keep Dropout ON in eval mode to get a distribution of outputs and estimate how\n",
    "  confident the model is.\n",
    "\n",
    "### 1.3 Short side‑by‑side comparison\n",
    "\n",
    "| Aspect                        | Normal Dropout (PyTorch)                               | Dropout Transformation (probly)                          |\n",
    "|------------------------------|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| Where it appears in code     | You explicitly add `nn.Dropout` layers                 | Transformation walks the model and inserts Dropout       |\n",
    "| When Dropout is active       | Only in `model.train()`                                | Also (and intentionally) in `model.eval()`               |\n",
    "| Main purpose                 | Reduce overfitting / improve generalisation            | Make predictions uncertainty‑aware                       |\n",
    "| Output behaviour in eval     | Deterministic (same input → same output)               | Stochastic (same input → slightly different outputs)     |\n",
    "| How we use the randomness    | We ignore it at inference                              | We *use* it to measure spread / uncertainty              |\n",
    "\n",
    "The rest of this notebook now assumes this picture: **“normal” Dropout is a training regulariser, the\n",
    "Dropout transformation turns the same mechanism into a tool for estimating uncertainty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac982de7",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `dropout(model, p)`, and inspect the modified architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50def69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running inside the repo's environment, these imports should work directly.\n",
    "from probly.transformation import dropout\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def build_mlp(in_dim=10, hidden=32, out_dim=1):\n",
    "    # A sequential model that ends on a Linear\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "p = 0.2  # dropout probability\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "model_do = dropout(model, p)\n",
    "print(\"\\nWith Dropout transformation (p=%.2f):\\n\" % p, model_do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e37850",
   "metadata": {},
   "source": [
    "### Notes on the structure\n",
    "- Expect a Dropout layer **before** each intermediate `nn.Linear`.\n",
    "- If the last layer is a linear output head, the transform usually **does not** add a Dropout layer in front of it, preserving your final mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5c9b8",
   "metadata": {},
   "source": [
    "## 3. Uncertainty via Monte Carlo (MC) Dropout\n",
    "\n",
    "To obtain predictive *uncertainty*, we run multiple stochastic forward passes with Dropout **active** and compute the mean and variance of predictions.\n",
    "\n",
    "> **Important:** In PyTorch, Dropout is active in `model.train()` mode. For MC Dropout at inference, we intentionally call `train()` while disabling gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# Toy regression data\n",
    "torch.manual_seed(0)\n",
    "n = 128\n",
    "X = torch.randn(n, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(n, 1)\n",
    "\n",
    "# (Re)build and transform the model\n",
    "model = build_mlp(in_dim=10, hidden=64, out_dim=1)\n",
    "model_do = dropout(model, p=0.2)\n",
    "\n",
    "# Simple training loop (few steps just for illustration)\n",
    "opt = torch.optim.Adam(model_do.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model_do.train()\n",
    "for step in range(200):\n",
    "    opt.zero_grad()\n",
    "    pred = model_do(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# MC dropout prediction function\n",
    "@torch.no_grad()\n",
    "def mc_predict(model_with_dropout, inputs, T=50):\n",
    "    model_with_dropout.train()  # activate dropout\n",
    "    preds = []\n",
    "    for _ in range(T):\n",
    "        preds.append(model_with_dropout(inputs).detach())\n",
    "    stacked = torch.stack(preds, dim=0)  # [T, N, out_dim]\n",
    "    mean = stacked.mean(dim=0)\n",
    "    var = stacked.var(dim=0, unbiased=False)\n",
    "    return mean, var\n",
    "\n",
    "mean_pred, var_pred = mc_predict(model_do, X[:5], T=100)\n",
    "print(\"Predictive mean (first 5):\\n\", mean_pred.squeeze())\n",
    "print(\"\\nPredictive variance (first 5):\\n\", var_pred.squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fea5a0",
   "metadata": {},
   "source": [
    "## 4. Good practices\n",
    "- Tune `p` (e.g., 0.1–0.5) based on validation performance.\n",
    "- Use a reasonable number of MC samples `T` (e.g., 20–200). Larger `T` → smoother uncertainty estimates, but slower.\n",
    "- Keep your **final layer behavior** in mind when interpreting where Dropout is inserted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720ab12",
   "metadata": {},
   "source": [
    "## 5. Common errors\n",
    "- `ValueError: p must be between 0 and 1` — ensure `0 ≤ p ≤ 1`.\n",
    "- Seeing no Dropout layers? Confirm your model actually contains `nn.Linear` modules where you expect them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cad33a",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "- Try other architectures (e.g., with Conv blocks feeding into Linear heads).\n",
    "- Compare models **with vs. without** the transformation using the same training loop.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
