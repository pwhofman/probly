{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc809e5",
   "metadata": {},
   "source": [
    "# A Brief Introduction to Ensemble Transformation\n",
    "\n",
    "The goal of this notebook is to showcase the `ensemble` function, which is based on the provided code files (`common.py`, `torch.py`, `__init__.py`). We will demonstrate how to use it to create an ensemble of models from a base PyTorch model.\n",
    "\n",
    "#### But what is an Ensemble?\n",
    "An *Ensemble* in machine learning is usually a method that uses a finite set of learning algorithms. Instead of relying on a single model, an ensemble combines the results of several models to create a better result.\n",
    "Those different models usually slightly differ in their parameters.\n",
    "This often improves robustness and helps quantify **uncertainty** — for example, when the ensemble members disagree, we know the model is unsure.  \n",
    "In `probly`, the `ensemble` transformation automates the creation of such model collections directly from a base PyTorch model. <br>\n",
    "**Why use Ensemble?** <br>\n",
    "Because it saves you from having to manually copy, reset, and manage multiple model instances yourself. `ensemble` does all of that for you automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b29dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from lazy_dispatch import lazydispatch\n",
    "from lazy_dispatch.isinstance import LazyType\n",
    "from probly.transformation.ensemble import ensemble\n",
    "from pytraverse import singledispatch_traverser, traverse\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from collections.abc import Callable\n",
    "\n",
    "    from lazy_dispatch.isinstance import LazyType\n",
    "    from probly.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model weight (layer1): 0.053378403186798096\n",
      "--- Creating ensemble with reset (default) ---\n",
      "Base model weight: 0.18488982319831848\n",
      "Custom reset_parameters of SimpleNet called!\n",
      "Custom reset_parameters of SimpleNet called!\n",
      "Custom reset_parameters of SimpleNet called!\n",
      "\n",
      "Ensemble created: <class 'torch.nn.modules.container.ModuleList'>\n",
      "Number of members: 3\n",
      "Weight of member 0: -0.11245134472846985\n",
      "Weight of member 1: -0.2733441889286041\n",
      "Weight of member 2: 0.1302759349346161\n",
      "All weights are different (as expected).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "\n",
    "# Our base model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the network architecture with two linear layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):  # noqa: ANN201, ANN001\n",
    "        \"\"\"Run a forward pass of the network.\"\"\"\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        return self.layer2(x)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset the parameters of all layers.\"\"\"\n",
    "        # Important for Deep Ensembles: a custom reset function\n",
    "        print(\"Custom reset_parameters of SimpleNet called!\")\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "\n",
    "\n",
    "base_model = SimpleNet()\n",
    "print(f\"Base model weight (layer1): {base_model.layer1.weight.data[0, 0]}\")\n",
    "print(\"--- Creating ensemble with reset (default) ---\")\n",
    "base_model_2 = SimpleNet()\n",
    "base_weight = base_model_2.layer1.weight.data[0, 0].item()\n",
    "print(f\"Base model weight: {base_weight}\")\n",
    "\n",
    "# ensemble is the function from common.py\n",
    "# reset_params=True is the default\n",
    "model_ensemble = ensemble(base_model_2, num_members=3, reset_params=True)\n",
    "\n",
    "print(f\"\\nEnsemble created: {type(model_ensemble)}\")\n",
    "print(f\"Number of members: {len(model_ensemble)}\")\n",
    "\n",
    "# Let's compare the weights to see if they are different\n",
    "weight0 = model_ensemble[0].layer1.weight.data[0, 0].item()\n",
    "weight1 = model_ensemble[1].layer1.weight.data[0, 0].item()\n",
    "weight2 = model_ensemble[2].layer1.weight.data[0, 0].item()\n",
    "\n",
    "print(f\"Weight of member 0: {weight0}\")\n",
    "print(f\"Weight of member 1: {weight1}\")\n",
    "print(f\"Weight of member 2: {weight2}\")\n",
    "\n",
    "if base_weight not in (weight0, weight1) and weight0 != weight1:\n",
    "    print(\"All weights are different (as expected).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b142e6",
   "metadata": {},
   "source": [
    "## 1. Setup: Dependencies and Code Definitions\n",
    "\n",
    "Before we can use the `ensemble` function, we'll define it and its components as described in the code files (`common.py`, `torch.py`, `__init__.py`). This ensures this notebook is self-contained.\n",
    "\n",
    "We will copy the contents of the provided files here and adjust the relative imports.\n",
    "We use three components: `common.py` for dispatching and `torch.py` for generating an ensemble. `init.py`connects the generic ensemnle logic with the PyTorch implementation. All components will be explained further below.\n",
    "\n",
    "To prevent errors, run the cells in the given order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e10465",
   "metadata": {},
   "source": [
    "### 1.1 How `__init__.py` connects Ensemble Components\n",
    "The `__init__.py` file defines the public interface of the ensemble module.\n",
    "It re-exports the main functions `ensemble` and `register` so they can be imported directly from `probly.ensemble`.\n",
    "Additionally, it performs a *lazy registration* of the PyTorch backend, which means the Torch implementation is only loaded when a Torch model is actually used.\n",
    "This design avoids unnecessary imports, prevents circular dependencies, and keeps the package lightweight and modular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d6025",
   "metadata": {},
   "source": [
    "*NOTE:* Since we are in a notebook, we are performing the registration\n",
    "(which already happened in the 'torch.py' cell) explicitly.\n",
    "The 'delayed_register' logic is not needed here because we loaded 'torch.py'\n",
    "directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d2713",
   "metadata": {},
   "source": [
    "### 1.2 How `common.py` implements the Main Logic\n",
    "The `common.py` file defines the core logic of the ensemble module.\n",
    "It introduces a generic dispatcher called `ensemble_generator`, which dynamically selects the correct ensemble creation function based on the model type.\n",
    "The register function allows developers to link new model types (such as PyTorch or custom predictors) to their specific generator implementations.\n",
    "Finally, the `ensemble()` function provides a simple, user-facing API that hides the dispatch mechanism and automatically calls the right generator.\n",
    "Together, these components make the ensemble system flexible and easily extensible to other frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abca0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # noqa: E402, F404, RUF100\n",
    "\n",
    "\"\"\"Shared ensemble implementation.\"\"\"\n",
    "\n",
    "from typing import TYPE_CHECKING  # noqa: E402\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from collections.abc import Callable\n",
    "\n",
    "    from lazy_dispatch.isinstance import LazyType\n",
    "    from probly.predictor import Predictor\n",
    "\n",
    "\n",
    "@lazydispatch\n",
    "def ensemble_generator[In, KwIn, Out](base: Predictor[In, KwIn, Out]) -> Predictor[In, KwIn, Out]:\n",
    "    \"\"\"Generate an ensemble from a base model.\"\"\"\n",
    "    msg = f\"No ensemble generator is registered for type {type(base)}\"\n",
    "    raise NotImplementedError(msg)\n",
    "\n",
    "\n",
    "def register(cls: LazyType, generator: Callable) -> None:\n",
    "    \"\"\"Register a class which can be used as a base for an ensemble.\"\"\"\n",
    "    ensemble_generator.register(cls=cls, func=generator)\n",
    "\n",
    "\n",
    "def ensemble[T: Predictor](base: T, n_members: int, reset_params: bool = True) -> T:\n",
    "    \"\"\"Create an ensemble predictor from a base predictor.\n",
    "\n",
    "    Args:\n",
    "        base: Predictor, The base model to be used for the ensemble.\n",
    "        n_members: The number of members in the ensemble.\n",
    "        reset_params: Whether to reset the parameters of each member.\n",
    "\n",
    "    Returns:\n",
    "        Predictor, The ensemble predictor.\n",
    "    \"\"\"\n",
    "    return ensemble_generator(base, n_members=n_members, reset_params=reset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37018878",
   "metadata": {},
   "source": [
    "### 1.3 The Torch Implementation: `torch.py`\n",
    "The `torch.py` file implements the ensemble generator specifically for PyTorch models.\n",
    "It uses the `pytraverse` library to recursively clone neural networks and optionally reset their parameters using each layer’s `reset_parameters()` method.\n",
    "The main function, `generate_torch_ensemble()`, creates multiple independent copies of a given base model and returns them as an nn.ModuleList.\n",
    "This ensures that each ensemble member has its own parameters, allowing the ensemble to represent independent model instances.\n",
    "At the end, the PyTorch generator is registered with the dispatcher using register(`nn.Module`, `generate_torch_ensemble`), linking it seamlessly to the common interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required third-party dependencies\n",
    "from collections.abc import Callable  # noqa: TC003\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from lazy_dispatch import lazydispatch\n",
    "from lazy_dispatch.isinstance import LazyType  # noqa: TC001\n",
    "\n",
    "\n",
    "# --- Mocks for 'probly' dependencies that were not provided ---\n",
    "class Predictor:\n",
    "    \"\"\"Mock class for probly.predictor.Predictor.\"\"\"\n",
    "\n",
    "\n",
    "TORCH_MODULE = nn.Module  # Mock for probly.lazy_types.TORCH_MODULE\n",
    "\n",
    "# Mock for probly.traverse_nn.nn_traverser\n",
    "nn_traverser = singledispatch_traverser[nn.Module](name=\"nn_traverser\")\n",
    "\n",
    "\n",
    "@nn_traverser.register(nn.Module)\n",
    "def _nn_traverse_default(obj: nn.Module, traverse: traverse) -> nn.Module:  # type: ignore  # noqa: PGH003\n",
    "    \"\"\"Default traverser that maps children.\"\"\"\n",
    "    return traverse.map_children(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5926d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "import copy\n",
    "\n",
    "reset_traverser = singledispatch_traverser[nn.Module](name=\"reset_traverser\")\n",
    "\n",
    "\n",
    "@reset_traverser.register\n",
    "def _(obj: nn.Module) -> nn.Module:\n",
    "    if hasattr(obj, \"reset_parameters\"):\n",
    "        obj.reset_parameters()  # type: ignore[operator]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _copy(module: nn.Module) -> nn.Module:\n",
    "    # simple deep copy without relying on the nn_traverser mock\n",
    "    return copy.deepcopy(module)\n",
    "\n",
    "\n",
    "def _reset_copy(module: nn.Module) -> nn.Module:\n",
    "    cloned = _copy(module)\n",
    "    for m in cloned.modules():\n",
    "        if hasattr(m, \"reset_parameters\"):\n",
    "            m.reset_parameters()\n",
    "    return cloned\n",
    "\n",
    "\n",
    "def generate_torch_ensemble(\n",
    "    obj: nn.Module,\n",
    "    n_members: int,\n",
    "    reset_params: bool = True,\n",
    ") -> nn.ModuleList:\n",
    "    \"\"\"Build a torch ensemble by copying the base model n_members times, resetting the parameters of each member.\"\"\"\n",
    "    if reset_params:\n",
    "        return nn.ModuleList([_reset_copy(obj) for _ in range(n_members)])\n",
    "    return nn.ModuleList([_copy(obj) for _ in range(n_members)])\n",
    "\n",
    "\n",
    "register(nn.Module, generate_torch_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca910ddc",
   "metadata": {},
   "source": [
    "## 2. The Problem: Manual Ensemble Creation\n",
    "\n",
    "Let's say we have a base model in PyTorch and want to create a \"Deep Ensemble\" for uncertainty quantification. For this, we need several copies of this model, each of which must have different initialized weights.\n",
    "\n",
    "The naive approach would be to manually copy the model and reset the parameters. This can be tedious, especially with complex, nested models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our base model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the network architecture with two linear layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):  # noqa: ANN201, ANN001\n",
    "        \"\"\"Run a forward pass of the network.\"\"\"\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        return self.layer2(x)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset the parameters of all layers.\"\"\"\n",
    "        # Important for Deep Ensembles: a custom reset function\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "\n",
    "\n",
    "base_model = SimpleNet()\n",
    "\n",
    "\n",
    "# Manual approach\n",
    "n_members = 3\n",
    "manual_ensemble = []\n",
    "for _ in range(n_members):\n",
    "    model_copy = copy.deepcopy(base_model)\n",
    "    # We have to remember to reset the parameters manually\n",
    "    if hasattr(model_copy, \"reset_parameters\"):\n",
    "        model_copy.reset_parameters()\n",
    "    manual_ensemble.append(model_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d9481",
   "metadata": {},
   "source": [
    "This works, but it's cumbersome. We have to use `copy.deepcopy` and manually check for a `reset_parameters` method.\n",
    "\n",
    "The `torch.py` code automates this. The `_reset_copy` function uses `pytraverse` to recursively traverse the module and call `reset_parameters()` on every submodule that has it. This is a perfect example of the separation of concerns shown in `pytraverse_tutorial.ipynb` (cell `5ae1e551`): the traversal logic is separate from the reset logic.\n",
    "In the next section, we will see how the `ensemble()` function automates this entire process, making ensemble creation both cleaner and safer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01593f0",
   "metadata": {},
   "source": [
    "## 3. The Automated Solution: The ensemble() Function\n",
    "\n",
    "The `ensemble` function from `common.py` is a `lazydispatch` wrapper. It automatically selects the correct generator based on the type of the base model.\n",
    "\n",
    "Since we registered `generate_torch_ensemble` for `nn.Module` (in the `torch.py` cell above), we can apply the `ensemble` function directly to our `SimpleNet` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2e2ea",
   "metadata": {},
   "source": [
    "## Optional Behavior: Cloning Without Reset\n",
    "\n",
    "The `ensemble` function also accepts `reset_params=False`.\n",
    "\n",
    "In this case, `generate_torch_ensemble` calls the `_copy` function instead of `_reset_copy`. `_copy` simply uses the `nn_traverser` to clone the module without calling `reset_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating ensemble without reset ---\n",
      "Base model weight: 0.016505658626556396\n",
      "\n",
      "Weight of member 0: 0.016505658626556396\n",
      "Weight of member 1: 0.016505658626556396\n",
      "All weights are identical to the base model (as expected).\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating ensemble without reset ---\")\n",
    "base_model_3 = SimpleNet()\n",
    "base_weight_3 = base_model_3.layer1.weight.data[0, 0].item()\n",
    "print(f\"Base model weight: {base_weight_3}\")\n",
    "\n",
    "# This time we set reset_params to False\n",
    "copied_ensemble = ensemble(base_model_3, n_members=2, reset_params=False)\n",
    "\n",
    "weight0 = copied_ensemble[0].layer1.weight.data[0, 0].item()\n",
    "weight1 = copied_ensemble[1].layer1.weight.data[0, 0].item()\n",
    "\n",
    "print(f\"\\nWeight of member 0: {weight0}\")\n",
    "print(f\"Weight of member 1: {weight1}\")\n",
    "\n",
    "assert weight0 == base_weight_3  # noqa: S101\n",
    "assert weight1 == base_weight_3  # noqa: S101\n",
    "print(\"All weights are identical to the base model (as expected).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b94d02",
   "metadata": {},
   "source": [
    "## 4. Summary and Key Takeaway\n",
    "\n",
    "The `ensemble` function is a powerful dispatcher that abstracts away the complexity of creating model ensembles.\n",
    "\n",
    "By registering type-specific generators (like `generate_torch_ensemble` for `nn.Module`) with the `ensemble_generator`, it provides a clean, extensible API.\n",
    "\n",
    "Internally, the PyTorch implementation uses `pytraverse` to efficiently traverse, copy, and optionally reset parameters of the module structure. This demonstrates how the abstract concepts from `pytraverse_tutorial.ipynb` (like `singledispatch_traverser` and `traverse` with `{CLONE: True}`) are used in a real-world application to write robust and maintainable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbabfc",
   "metadata": {},
   "source": [
    "## 5. Further Reading and References\n",
    "- **Probly documentation:** ***for more information on how everything in probly works*** <br> (https://github.com/pwhofman/probly/tree/main/docs)\n",
    "- **PyTraverse intro notebook:** ***offers a deep tutorial on how pytraverse automates traversing***<br> (https://github.com/pwhofman/probly/blob/main/notebooks/examples/pytraverse_tutorial.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
