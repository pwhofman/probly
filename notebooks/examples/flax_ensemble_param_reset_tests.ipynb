{
 "cells": [
  {
   "cell_type": "code",
   "id": "e8be716be7e87221",
   "metadata": {},
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6351545d144a70f",
   "metadata": {},
   "source": "## Flax CNN example"
  },
  {
   "cell_type": "markdown",
   "id": "f8757ab180969c1e",
   "metadata": {},
   "source": "modified from [flax nnx docs MNIST tutorial](https://flax.readthedocs.io/en/latest/mnist_tutorial.html#mnist-tutorial)."
  },
  {
   "cell_type": "code",
   "id": "5f748f08a21d3bea",
   "metadata": {},
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class CNN(nnx.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs) -> None:\n",
    "        \"\"\"Init.\"\"\"\n",
    "        self.conv1 = nnx.Conv(1, 10, kernel_size=(1, 1), rngs=rngs)\n",
    "        self.dropout1 = nnx.Dropout(rate=0.025, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(10, 20, kernel_size=(1, 1), rngs=rngs)\n",
    "        self.avg_pool = partial(nnx.avg_pool, window_shape=(1, 1), strides=(1, 1))\n",
    "        self.linear1 = nnx.Linear(20, 100, rngs=rngs)\n",
    "        self.dropout2 = nnx.Dropout(rate=0.025, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(100, 10, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"Call.\"\"\"\n",
    "        x = self.avg_pool(nnx.relu(self.conv1(x)))\n",
    "        x = self.avg_pool(nnx.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.shape[0], -1)  # flatten\n",
    "        x = nnx.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model.\n",
    "model = CNN(rngs=nnx.Rngs(0))\n",
    "# Visualize it.\n",
    "nnx.display(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59e74ed99a28977",
   "metadata": {},
   "source": [
    "from probly.transformation import ensemble\n",
    "\n",
    "cnn_ensemble = ensemble(model, num_members=2, reset_params=True, key=2)\n",
    "nnx.display(cnn_ensemble)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a957e601192d29ee",
   "metadata": {},
   "source": [
    "print(f\"base model conv1 kernel value:\\n{model.conv1.kernel.value}\")\n",
    "print(f\"ensemble member 1 conv1 kernel value:\\n{cnn_ensemble[0].conv1.kernel.value}\")\n",
    "print(f\"ensemble member 2 conv1 kernel value:\\n{cnn_ensemble[1].conv1.kernel.value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "415340f8c4923206",
   "metadata": {},
   "source": "Resetting the parameters works for models with architecture similar to the CNN from the Flax nnx docs. Each ensemble `member` is initialized with its own key, resulting in different kernel values in this instance."
  },
  {
   "cell_type": "markdown",
   "id": "9d91f04a998e507d",
   "metadata": {},
   "source": "#### Call"
  },
  {
   "cell_type": "code",
   "id": "1f24409fdbd38411",
   "metadata": {},
   "source": [
    "seed = 42\n",
    "x = jax.random.normal(jax.random.key(seed), shape=(1, 1, 1, 1))\n",
    "cnn_out = model(x)\n",
    "print(f\"base cnn:\\n{cnn_out}\")\n",
    "print(\"ensemble:\")\n",
    "for i, member in enumerate(cnn_ensemble):\n",
    "    member_out = member(x)\n",
    "    print(f\"member {i + 1}:\\n{member_out}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4936edbbdaf9b189",
   "metadata": {},
   "source": "We can see that the outputs from the `base cnn` and the `ensemble` are different. The outputs of the ensemble members diversify as a result of their independent initializations."
  },
  {
   "cell_type": "markdown",
   "id": "ef4067ddcc725ddc",
   "metadata": {},
   "source": "## Flax nnx block architecture"
  },
  {
   "cell_type": "markdown",
   "id": "99e4529f6b359bd",
   "metadata": {},
   "source": "Let's have a look how this ensemble functionality behaves for nested models:"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class MixedBlock(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs, in_features: int, out_features: int) -> None:\n",
    "        \"\"\"Init.\"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nnx.Linear(rngs=rngs, in_features=self.in_features, out_features=self.out_features)\n",
    "        self.layernorm = nnx.LayerNorm(rngs=rngs, num_features=out_features)\n",
    "        self.dropout = nnx.Dropout(rate=0.1, rngs=rngs)\n",
    "\n",
    "        # Convolution branch\n",
    "        self.conv = nnx.Conv(\n",
    "            rngs=rngs, in_features=self.in_features, out_features=self.out_features, kernel_size=(3, 3), padding=\"SAME\"\n",
    "        )\n",
    "\n",
    "        # Recurrent branch\n",
    "        self.gru = nnx.GRUCell(rngs=rngs, in_features=self.in_features, hidden_features=self.out_features)\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, rngs: nnx.Rngs = None) -> jax.Array:\n",
    "        \"\"\"Call.\"\"\"\n",
    "        # flatten input for Linear if needed\n",
    "        h = x\n",
    "\n",
    "        if h.ndim > 2:\n",
    "            h = h.reshape(h.shape[0], -1)\n",
    "\n",
    "        # dense -> norm -> activation -> dropout\n",
    "        h1 = self.linear(h)\n",
    "        h1 = self.layernorm(h1)\n",
    "        h1 = jax.nn.gelu(h1)\n",
    "        h1 = self.dropout(h1, rngs=rngs)\n",
    "\n",
    "        # conv -> activation\n",
    "        if x.ndim == 4:\n",
    "            h2 = jax.nn.gelu(self.conv(x))\n",
    "            h2 = h2.reshape(h2.shape[0], -1)\n",
    "        else:\n",
    "            h2 = 0\n",
    "\n",
    "        # combination\n",
    "        h = h1 + h2\n",
    "\n",
    "        # recurrent update\n",
    "        batch_size = x.shape[0]\n",
    "        carry = jnp.zeros((batch_size, h.shape[-1]))\n",
    "        carry, h_rec = self.gru(carry, h)\n",
    "\n",
    "        return h + h_rec\n",
    "\n",
    "\n",
    "class NestedFlaxModel(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs, in_features: int = 80, out_features: int = 80) -> None:\n",
    "        \"\"\"Init.\"\"\"\n",
    "        # multiple nested blocks\n",
    "        self.block = MixedBlock(rngs=rngs, in_features=in_features, out_features=out_features)\n",
    "\n",
    "        # add attention block\n",
    "        self.attention = nnx.MultiHeadAttention(\n",
    "            rngs=rngs,\n",
    "            num_heads=4,\n",
    "            in_features=in_features,\n",
    "            qkv_features=out_features,\n",
    "            out_features=out_features,\n",
    "        )\n",
    "\n",
    "        # add residual MLP\n",
    "        self.mlp = nnx.Sequential(\n",
    "            nnx.Linear(rngs=rngs, in_features=in_features, out_features=out_features),\n",
    "            nnx.LayerNorm(rngs=rngs, num_features=out_features),\n",
    "            jax.nn.relu,\n",
    "            nnx.Linear(rngs=rngs, in_features=out_features, out_features=out_features),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, rngs: nnx.Rngs = None) -> jax.Array:\n",
    "        \"\"\"Call.\"\"\"\n",
    "        batch = x.shape[0]\n",
    "        x_flat = x.reshape(batch, -1)\n",
    "\n",
    "        x_flat = self.block(x_flat, rngs=rngs)\n",
    "\n",
    "        # self-attention\n",
    "        x_att = x_flat[:, None, :]\n",
    "        x_att = self.attention(x_att, x_att, x_att)\n",
    "        x_att = x_att.squeeze(1)\n",
    "\n",
    "        # MLP residual\n",
    "        x_mlp = self.mlp(x_flat)\n",
    "\n",
    "        x_out = x_mlp + x_att\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "rngs = nnx.Rngs(1)\n",
    "nested_flax_model = NestedFlaxModel(rngs=rngs, in_features=100, out_features=100)\n",
    "nnx.display(nested_flax_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f92d9c9c5bce429",
   "metadata": {},
   "source": [
    "from probly.transformation.ensemble import ensemble\n",
    "\n",
    "flax_ensemble = ensemble(nested_flax_model, num_members=2, reset_params=True, key=2)\n",
    "nnx.display(flax_ensemble)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7bc186c3a5e6ae3",
   "metadata": {},
   "source": [
    "`NestedFlaxModel.__init__` has the params `in_features, out_features` with default `80`.\n",
    "\n",
    "With the `iter_children()` logic in the `reset_traverser` the `NestedFlaxModel` is not called and therefore not reinitialized. This should be a neat way to reset flax models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
