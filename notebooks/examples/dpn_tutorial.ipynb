{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bd0907",
   "metadata": {},
   "source": [
    "# Tutorial: Predictive Uncertainty & Dirichlet Prior Networks (DPN)\n",
    "\n",
    "Welcome!  \n",
    "This notebook teaches you the fundamentals of predictive uncertainty and Dirichlet Prior Networks (DPNs).\n",
    "\n",
    "By the end of this tutorial, you will know how to:\n",
    "\n",
    "- understand uncertainty types  \n",
    "- build a DPN from scratch  \n",
    "- train it with in-domain and OOD data  \n",
    "- compute and visualize uncertainty  \n",
    "- evaluate OOD detection performance  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8446b",
   "metadata": {},
   "source": [
    "# 1. Motivation: Why Uncertainty Matters\n",
    "\n",
    "Conventional neural networks are often *overconfident*.  \n",
    "They output strong softmax probabilities even when the input is noisy, ambiguous, or completely out-of-distribution (OOD).\n",
    "\n",
    "Example:\n",
    "\n",
    "A blurry handwritten \"3\" might get a prediction like:\n",
    "\n",
    "**â€œ99.9% confident this is a 3.â€**\n",
    "\n",
    "Clearly, the model *should* be unsure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52342a1",
   "metadata": {},
   "source": [
    "## 1.1 Types of Uncertainty\n",
    "\n",
    "Neural networks face three major types of uncertainty:\n",
    "\n",
    "| Type | Meaning | Example |\n",
    "|------|---------|---------|\n",
    "| **Aleatoric** | Noise or ambiguity in the data | Blurry digit |\n",
    "| **Epistemic** | Model uncertainty due to missing knowledge | Unfamiliar shapes |\n",
    "| **Distributional (OOD)** | Input comes from a different dataset entirely | Clothing image for a digit classifier |\n",
    "\n",
    "Dirichlet Prior Networks focus mainly on **distributional uncertainty**, but they help with all three types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88c29",
   "metadata": {},
   "source": [
    "# 2. The Idea: Distributions Over Distributions\n",
    "\n",
    "A classical classifier outputs **one softmax distribution**.\n",
    "\n",
    "A Dirichlet Prior Network (DPN), however, outputs the **parameters of a Dirichlet distribution**, which is a distribution *over* possible softmax distributions.\n",
    "\n",
    "This gives the model the ability to express:\n",
    "\n",
    "- **high certainty** (sharp Dirichlet, high precision Î±â‚€)  \n",
    "- **uncertainty / OOD** (flat Dirichlet, low precision Î±â‚€)\n",
    "\n",
    "This is a more expressive form of uncertainty than plain softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc88f2",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "\n",
    "We start by importing libraries and checking device availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115742e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core PyTorch modules needed to build neural networks.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard Python libs for arrays and plotting.\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data loading utilities for batching and shuffling datasets.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision gives us MNIST and Fashion-MNIST built in.\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Choose GPU if available; otherwise fall back to CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Display the device being used.\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66baaa",
   "metadata": {},
   "source": [
    "# 4. Loading the Datasets\n",
    "\n",
    "To train a Dirichlet Prior Network, we need:\n",
    "\n",
    "- **In-domain data** â†’ MNIST (handwritten digits)\n",
    "- **Out-of-distribution data (OOD)** â†’ Fashion-MNIST (clothing images)\n",
    "\n",
    "The DPN learns to be **confident** on MNIST and **uncertain** on Fashion-MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac88c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how images should be transformed:\n",
    "# - Convert to PyTorch tensor\n",
    "# - Normalize pixel values from [0,1] to approximately [-1,1]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load MNIST training/test datasets (in-domain data).\n",
    "mnist_train = datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    \"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Load Fashion-MNIST training/test datasets (OOD data).\n",
    "fmnist_train = datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "fmnist_test = datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Size of mini-batches during training.\n",
    "batch_size = 128\n",
    "\n",
    "# Create data loaders that return batches of image/label pairs.\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "fmnist_train_loader = DataLoader(fmnist_train, batch_size=batch_size, shuffle=True)\n",
    "fmnist_test_loader = DataLoader(fmnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ada145",
   "metadata": {},
   "source": [
    "## 4.1 Visualizing the Data\n",
    "\n",
    "Before training, let's take a quick look at some samples from each dataset.\n",
    "\n",
    "This helps us understand why Fashion-MNIST counts as â€œout of distributionâ€ for a digit classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single batch from each loader.\n",
    "images_mnist, labels_mnist = next(iter(mnist_train_loader))\n",
    "images_fmnist, labels_fmnist = next(iter(fmnist_train_loader))\n",
    "\n",
    "# Create a figure with 2 rows and 8 columns of images.\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "\n",
    "# Show 8 MNIST images on the first row.\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(images_mnist[i, 0], cmap=\"gray\")\n",
    "    axes[0, i].set_title(labels_mnist[i].item())\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Show 8 Fashion-MNIST images on the second row.\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow(images_fmnist[i, 0], cmap=\"gray\")\n",
    "    axes[1, i].set_title(labels_fmnist[i].item())\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "# Add a title above both rows.\n",
    "plt.suptitle(\"Top: MNIST (In-Domain)  â€”  Bottom: Fashion-MNIST (OOD)\")\n",
    "\n",
    "# Adjust spacing so the plot looks neat.\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b7b10",
   "metadata": {},
   "source": [
    "# 5. Building the Dirichlet Prior Network (DPN)\n",
    "\n",
    "A DPN outputs **Dirichlet concentration parameters** instead of softmax probabilities.\n",
    "\n",
    "These parameters (Î± values):\n",
    "\n",
    "- must be **positive**\n",
    "- reflect **confidence** (high Î±â‚€)\n",
    "- reflect **uncertainty / OOD** (low Î±â‚€)\n",
    "\n",
    "We build a simple CNN that outputs 10 Î±-values (for 10 classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDPN(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        \"\"\"Initialize the ConvDPN model with the given number of classes.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN feature extractor:\n",
    "        # - Conv layer (1â†’32 channels)\n",
    "        # - ReLU activation\n",
    "        # - MaxPool to shrink spatial size by 2\n",
    "        # Repeat pattern for deeper feature extraction.\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28x28 â†’ 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14 â†’ 7x7\n",
    "        )\n",
    "\n",
    "        # Classifier head:\n",
    "        # - Flatten feature maps\n",
    "        # - Linear hidden layer with 128 neurons\n",
    "        # - Final linear layer produces 10 outputs (RAW logits)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),  # no softmax!\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run a forward pass through the ConvDPN model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "        Input image batch of shape (B, 1, 28, 28).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        Dirichlet concentration parameters Î± of shape (B, num_classes).\n",
    "        \"\"\"  # noqa: RUF002\n",
    "        # Extract features using convolution layers.\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Compute raw logits from fully connected layers.\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        # Convert logits to positive Dirichlet Î±-values.  # noqa: RUF003\n",
    "        # Softplus = log(1 + exp(x)), always > 0.  # noqa: ERA001\n",
    "        alpha = F.softplus(logits) + 1e-3\n",
    "        return alpha\n",
    "\n",
    "\n",
    "# Create model and send it to GPU/CPU.\n",
    "model = ConvDPN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5839d5",
   "metadata": {},
   "source": [
    "# 6. Helper Functions for Dirichlet Distributions\n",
    "\n",
    "We now implement:\n",
    "- KL divergence between Dirichlet distributions\n",
    "- Predictive probabilities\n",
    "- Predictive entropy\n",
    "- Dirichlet differential entropy\n",
    "\n",
    "These are required for training and evaluating a DPN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.special import digamma, gammaln\n",
    "\n",
    "# Extra math functions used for Dirichlet distributions\n",
    "\n",
    "\n",
    "def kl_dirichlet(alpha_p: torch.Tensor, alpha_q: torch.Tensor) -> torch.Tensor:\n",
    "    # Sum of concentration parameters (precision)\n",
    "    alpha_p0 = alpha_p.sum(dim=-1, keepdim=True)\n",
    "    alpha_q0 = alpha_q.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Dirichlet normalization terms\n",
    "    term1 = gammaln(alpha_p0) - gammaln(alpha_q0)\n",
    "    term2 = (gammaln(alpha_q) - gammaln(alpha_p)).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Difference involving digamma (derivative of log gamma function)\n",
    "    term3 = ((alpha_p - alpha_q) * (digamma(alpha_p) - digamma(alpha_p0))).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Return KL divergence for each batch\n",
    "    return (term1 + term2 + term3).squeeze(-1)\n",
    "\n",
    "\n",
    "def predictive_probs(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    # Compute expected class probabilities p(y|x) = alpha / alpha0\n",
    "    alpha0 = alpha.sum(dim=-1, keepdim=True)\n",
    "    return alpha / alpha0\n",
    "\n",
    "\n",
    "def predictive_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    # Entropy of the expected categorical distribution\n",
    "    p = predictive_probs(alpha)\n",
    "    return -(p * torch.log(p + 1e-8)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def dirichlet_differential_entropy(alpha: torch.Tensor) -> torch.Tensor:\n",
    "    # Differential entropy of the Dirichlet distribution\n",
    "    alpha0 = alpha.sum(dim=-1)\n",
    "    k = alpha.size(-1)\n",
    "\n",
    "    log_b = gammaln(alpha).sum(dim=-1) - gammaln(alpha0)\n",
    "    psi_alpha = digamma(alpha)\n",
    "    psi_alpha0 = digamma(alpha0)\n",
    "\n",
    "    # Formula derived from Dirichlet entropy expression\n",
    "    return log_b + (alpha0 - k) * psi_alpha0 - ((alpha - 1) * psi_alpha).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc30524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision values for training:\n",
    "# High for in-domain, low for OOD.\n",
    "alpha0_in = 100.0\n",
    "alpha0_ood = 10.0\n",
    "\n",
    "# Slight label smoothing to avoid overconfidence.\n",
    "label_smoothing = 0.01\n",
    "\n",
    "\n",
    "def make_in_domain_target_alpha(y: torch.Tensor, num_classes: int = 10, alpha0: float = alpha0_in) -> torch.Tensor:\n",
    "    # Create a sharp Dirichlet distribution centered on the true class.\n",
    "    b = y.size(0)\n",
    "\n",
    "    # Start with uniform smoothing across classes.\n",
    "    mu = torch.full(\n",
    "        (b, num_classes),\n",
    "        label_smoothing / (num_classes - 1),\n",
    "        device=y.device,\n",
    "    )\n",
    "\n",
    "    # True class gets most of the weight.\n",
    "    mu[torch.arange(b), y] = 1.0 - label_smoothing\n",
    "\n",
    "    # Scale to high precision.\n",
    "    return mu * alpha0\n",
    "\n",
    "\n",
    "def make_ood_target_alpha(batch_size: int, num_classes: int = 10, alpha0: float = alpha0_ood) -> torch.Tensor:\n",
    "    # Create a flat Dirichlet distribution for OOD samples.\n",
    "\n",
    "    mu = torch.full(\n",
    "        (batch_size, num_classes),\n",
    "        1.0 / num_classes,\n",
    "        device=device,\n",
    "    )\n",
    "    return mu * alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7434245",
   "metadata": {},
   "source": [
    "# 7. Creating Training Targets for a DPN\n",
    "\n",
    "A Dirichlet Prior Network is trained so that:\n",
    "\n",
    "- **MNIST** â†’ produces **sharp Dirichlet distributions** (high precision Î±â‚€)\n",
    "- **Fashion-MNIST** â†’ produces **flat Dirichlet distributions** (low precision Î±â‚€)\n",
    "\n",
    "Below we create the target Î±-vectors for in-domain and OOD data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e113a",
   "metadata": {},
   "source": [
    "# 8. Training the Dirichlet Prior Network (DPN)\n",
    "\n",
    "During training, the model learns:\n",
    "\n",
    "- **High-confidence Dirichlet distributions** for MNIST\n",
    "- **Low-confidence Dirichlet distributions** for Fashion-MNIST (OOD)\n",
    "\n",
    "We combine three losses:\n",
    "\n",
    "1. KL divergence for in-domain samples  \n",
    "2. KL divergence for OOD samples  \n",
    "3. Optional cross-entropy loss for stability  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c353e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adam optimizer for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    mnist_loader: DataLoader,\n",
    "    fmnist_loader: DataLoader,\n",
    ") -> float:\n",
    "    # Train the model for one epoch by pairing MNIST (in-domain) with Fashion-MNIST (OOD) batches.\n",
    "\n",
    "    model.train()  # Enable training mode (activates dropout/batchnorm if present)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    # zip() pairs the MNIST and Fashion-MNIST batches\n",
    "    for (x_in_raw, y_in_raw), (x_ood_raw, _) in zip(mnist_loader, fmnist_loader, strict=True):\n",
    "        # Move data to CPU or GPU\n",
    "        x_in = x_in_raw.to(device)\n",
    "        y_in = y_in_raw.to(device)\n",
    "        x_ood = x_ood_raw.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- In-domain loss (MNIST) ----\n",
    "        alpha_in = model(x_in)  # predicted Dirichlet alphas\n",
    "        alpha_target_in = make_in_domain_target_alpha(y_in)  # target sharp Dirichlet\n",
    "\n",
    "        # KL divergence between target and predicted Dirichlet\n",
    "        kl_in = kl_dirichlet(alpha_target_in, alpha_in).mean()\n",
    "\n",
    "        # Optional cross-entropy loss for stability\n",
    "        probs_in = predictive_probs(alpha_in)  # expected class probabilities\n",
    "        ce_loss = F.nll_loss(torch.log(probs_in + 1e-8), y_in)\n",
    "\n",
    "        # ---- OOD loss (Fashion-MNIST) ----\n",
    "        alpha_ood = model(x_ood)\n",
    "        alpha_target_ood = make_ood_target_alpha(x_ood.size(0))\n",
    "\n",
    "        # KL divergence encourages flat Dirichlet on OOD\n",
    "        kl_ood = kl_dirichlet(alpha_target_ood, alpha_ood).mean()\n",
    "\n",
    "        # ---- Total loss ----\n",
    "        # We scale the CE term slightly to avoid overpowering the KL terms\n",
    "        loss = kl_in + kl_ood + 0.1 * ce_loss\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss for logging\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    return total_loss / total_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a90c0f",
   "metadata": {},
   "source": [
    "## 8.1 Running the Training Loop\n",
    "\n",
    "Now we train the DPN for a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Loop over several epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train for one epoch\n",
    "    loss = train_one_epoch(model, optimizer, mnist_train_loader, fmnist_train_loader)\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Epoch {epoch}/{num_epochs}  - Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97c9c9",
   "metadata": {},
   "source": [
    "# 9. Evaluating the DPN\n",
    "\n",
    "Now we test the model's performance on:\n",
    "\n",
    "1. MNIST classification accuracy  \n",
    "2. Predictive and differential entropies  \n",
    "3. OOD detection scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee00d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model: nn.Module, loader: DataLoader) -> float:\n",
    "    # Compute accuracy on MNIST test set using expected probabilities derived from the Dirichlet alpha values.\n",
    "\n",
    "    model.eval()  # Disable dropout/batchnorm\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation needed\n",
    "        for x_raw, y_raw in loader:\n",
    "            x = x_raw.to(device)\n",
    "            y = y_raw.to(device)\n",
    "\n",
    "            alpha = model(x)  # predict Dirichlet alphas\n",
    "            probs = predictive_probs(alpha)  # convert to expected probabilities\n",
    "\n",
    "            preds = probs.argmax(dim=-1)  # choose most likely class\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "mnist_acc = evaluate_accuracy(model, mnist_test_loader)\n",
    "print(f\"MNIST Test Accuracy: {mnist_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_scores(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Compute predictive entropy and Dirichlet differential entropy for a dataset.\n",
    "    model.eval()\n",
    "\n",
    "    entropies = []\n",
    "    dir_entropies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_raw, _ in loader:\n",
    "            x = x_raw.to(device)\n",
    "\n",
    "            alpha = model(x)\n",
    "\n",
    "            # Predictive entropy: measures class uncertainty\n",
    "            ent = predictive_entropy(alpha)\n",
    "\n",
    "            # Dirichlet differential entropy: uncertainty of the distribution itself\n",
    "            d_ent = dirichlet_differential_entropy(alpha)\n",
    "\n",
    "            entropies.append(ent.cpu().numpy())\n",
    "            dir_entropies.append(d_ent.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(entropies), np.concatenate(dir_entropies)\n",
    "\n",
    "\n",
    "mnist_ent, mnist_dent = collect_scores(model, mnist_test_loader)\n",
    "fmnist_ent, fmnist_dent = collect_scores(model, fmnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99946b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictive entropy and differential entropy histograms\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Predictive entropy histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mnist_ent, bins=50, alpha=0.6, label=\"MNIST (in-domain)\")\n",
    "plt.hist(fmnist_ent, bins=50, alpha=0.6, label=\"Fashion-MNIST (OOD)\")\n",
    "plt.title(\"Predictive Entropy\")\n",
    "plt.legend()\n",
    "\n",
    "# Differential entropy histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mnist_dent, bins=50, alpha=0.6, label=\"MNIST (in-domain)\")\n",
    "plt.hist(fmnist_dent, bins=50, alpha=0.6, label=\"Fashion-MNIST (OOD)\")\n",
    "plt.title(\"Dirichlet Differential Entropy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# Build labels: 0 = in-domain, 1 = out-of-distribution\n",
    "y_true = np.concatenate([np.zeros_like(mnist_dent), np.ones_like(fmnist_dent)])\n",
    "\n",
    "# Use Dirichlet differential entropy as the OOD score\n",
    "y_scores = np.concatenate([mnist_dent, fmnist_dent])\n",
    "\n",
    "# Compute OOD metrics\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "aupr = average_precision_score(y_true, y_scores)\n",
    "\n",
    "print(f\"OOD ROC-AUC: {auc:.4f}\")\n",
    "print(f\"OOD PR-AUC:  {aupr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f84e1a",
   "metadata": {},
   "source": [
    "# 10. Summary\n",
    "\n",
    "You have now implemented a **Dirichlet Prior Network (DPN)** from scratch and explored its ability to model predictive uncertainty.\n",
    "\n",
    "Hereâ€™s a quick recap of what you covered:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43a2a6",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Key Takeaways!\n",
    "\n",
    "### âœ” Classical neural networks are often overconfident  \n",
    "Softmax outputs a single distribution and cannot express uncertainty properly, especially for out-of-distribution (OOD) samples.\n",
    "\n",
    "### âœ” DPNs model *distributions over distributions*  \n",
    "A Dirichlet distribution lets the model express:\n",
    "- high precision â†’ high confidence  \n",
    "- low precision â†’ uncertainty / OOD detection  \n",
    "\n",
    "### âœ” Training requires two types of data  \n",
    "- **In-domain**: sharp Dirichlet  \n",
    "- **OOD**: flat Dirichlet  \n",
    "\n",
    "### âœ” Useful uncertainty measures  \n",
    "- **Predictive entropy**  \n",
    "- **Dirichlet differential entropy**  \n",
    "- **Precision (Î±â‚€)**  \n",
    "\n",
    "### âœ” DPNs can detect OOD samples  \n",
    "By comparing uncertainty levels between datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ef649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.layers.torch import ConvDPN\n",
    "from probly.train.evidential.common import unified_evidential_trainn\n",
    "\n",
    "model = ConvDPN()\n",
    "\n",
    "unified_evidential_trainn(\n",
    "    mode=\"PrNet\",\n",
    "    model=model,\n",
    "    dataloader=mnist_train_loader,\n",
    "    oodloader=fmnist_train_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
