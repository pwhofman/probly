{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad65fcd7",
   "metadata": {},
   "source": [
    "# Information Robust Dirichlet Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb53a5",
   "metadata": {},
   "source": [
    "In this notebook, we implement the specialized training loss proposed in the paper _Information Robust Dirichlet Networks for Predictive Uncertainty Estimation_ by Tsiligkaridis (2019). The method models predictive uncertainty by having a neural network output Dirichlet concentration parameters ð›¼ instead of just a pointwise softmax.\n",
    "\n",
    "The total loss is composed of three terms:\n",
    "\n",
    "1. Calibration term: implemented in the function  lp_fn\n",
    "2. Regularization term: implemented in the function  regularization_fn\n",
    "3. Adversiarial Entropy penalty: implemented in the function  dirichlet_entropy\n",
    "\n",
    "In the paper and in this notenbook, L_p loss is not directly computed but rather an upper bound for it, denoted by F_i (for sample i)  \n",
    "\n",
    "The regularization term penalizes high alpha values for incorrect classes.  \n",
    "\n",
    "The final term uses the alpha values the model assigns to adversarial inputs.\n",
    "The model is rewarded for outputting a Dirichlet-distribution with high entropy on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.special import digamma\n",
    "\n",
    "\n",
    "def log_beta(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"Compute log(Beta(a, b)).\"\"\"\n",
    "    return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "\n",
    "def lp_fn(alpha: Tensor, y: Tensor, p: float) -> Tensor:\n",
    "    \"\"\"Computes F_i (one per sample) using the expectation-based formulation.\n",
    "\n",
    "        F_i = ( E[(1 - p_c)^p] + Î£_{jâ‰ c} E[p_j^p] )^(1/p)\n",
    "\n",
    "    Args:\n",
    "        alpha: (B, K) Dirichlet concentration parameters (>0)\n",
    "        y:     (B, K) one-hot labels\n",
    "        p:     exponent scalar\n",
    "\n",
    "    Returns:\n",
    "        Sum_i F_i  (scalar Tensor)\n",
    "    \"\"\"\n",
    "    B, K = alpha.shape  # noqa: N806\n",
    "\n",
    "    alpha0: Tensor = alpha.sum(dim=1, keepdim=True)\n",
    "    alpha_c: Tensor = (alpha * y).sum(dim=1, keepdim=True)\n",
    "    alpha0_minus_c: Tensor = alpha0 - alpha_c\n",
    "\n",
    "    # E[(1 - p_c)^p]  # noqa: ERA001\n",
    "    log_e1: Tensor = log_beta(alpha0_minus_c + p, alpha_c) - log_beta(\n",
    "        alpha0_minus_c,\n",
    "        alpha_c,\n",
    "    )\n",
    "    e1: Tensor = torch.exp(log_e1)\n",
    "\n",
    "    # E[p_j^p] for all classes\n",
    "    log_ep: Tensor = log_beta(alpha + p, alpha0 - alpha) - log_beta(\n",
    "        alpha,\n",
    "        alpha0 - alpha,\n",
    "    )\n",
    "    ep: Tensor = torch.exp(log_ep)\n",
    "\n",
    "    # zero out correct class\n",
    "    ep = ep * (1 - y)\n",
    "\n",
    "    # E1 + Î£_{jâ‰ c} E[p_j^p]\n",
    "    e_sum: Tensor = e1 + ep.sum(dim=1, keepdim=True)\n",
    "\n",
    "    fi: Tensor = e_sum.pow(1.0 / p).squeeze(1)\n",
    "    return fi.sum()\n",
    "\n",
    "\n",
    "def regularization_fn(alpha: Tensor, y: Tensor) -> Tensor:\n",
    "    \"\"\"Implements the regularization term using trigamma-based formulation.\n",
    "\n",
    "    Args:\n",
    "        alpha: (B, K) Dirichlet parameters (>0)\n",
    "        y:     (B, K) one-hot labels\n",
    "\n",
    "    Returns:\n",
    "        Scalar Tensor representing the regularization penalty.\n",
    "    \"\"\"\n",
    "    alpha_tilde: Tensor = alpha * (1 - y) + y\n",
    "    alpha_tilde_0: Tensor = alpha_tilde.sum(dim=1, keepdim=True)\n",
    "\n",
    "    trigamma_alpha: Tensor = torch.polygamma(1, alpha_tilde)\n",
    "    trigamma_alpha0: Tensor = torch.polygamma(1, alpha_tilde_0)\n",
    "\n",
    "    diff_sq: Tensor = (alpha_tilde - 1.0).pow(2)\n",
    "    mask: Tensor = 1 - y\n",
    "\n",
    "    term: Tensor = 0.5 * diff_sq * (trigamma_alpha - trigamma_alpha0) * mask\n",
    "    return term.sum(dim=1).sum()\n",
    "\n",
    "\n",
    "def dirichlet_entropy(alpha: Tensor) -> Tensor:\n",
    "    \"\"\"Compute Dirichlet entropy for adversarial samples.\n",
    "\n",
    "    Args:\n",
    "        alpha: (B_a, K) Dirichlet parameters\n",
    "\n",
    "    Returns:\n",
    "        Scalar Tensor (sum of entropies)\n",
    "    \"\"\"\n",
    "    K: int = alpha.shape[-1]  # noqa: N806\n",
    "    alpha0: Tensor = alpha.sum(dim=-1)\n",
    "\n",
    "    log_b: Tensor = torch.lgamma(alpha).sum(dim=-1) - torch.lgamma(alpha0)\n",
    "    term1: Tensor = log_b\n",
    "    term2: Tensor = (alpha0 - K) * digamma(alpha0)\n",
    "    term3: Tensor = ((alpha - 1) * digamma(alpha)).sum(dim=-1)\n",
    "\n",
    "    entropy: Tensor = term1 + term2 - term3\n",
    "    return entropy.sum()\n",
    "\n",
    "\n",
    "def loss_IRD(  # noqa: N802\n",
    "    alpha: Tensor,\n",
    "    y: Tensor,\n",
    "    adversarial_alpha: Optional[Tensor] = None,  # noqa: UP045\n",
    "    p: float = 2.0,\n",
    "    lam: float = 1.0,\n",
    "    gamma: float = 1.0,\n",
    ") -> Tensor:\n",
    "    \"\"\"Loss from the paper.\n",
    "\n",
    "    \"Information Robust Dirichlet Networks for Predictive Uncertainty Estimation\".\n",
    "\n",
    "    Args:\n",
    "        alpha: (B, K)\n",
    "        y: (B, K) one-hot labels\n",
    "        adversarial_alpha: (B_a, K) adversarial inputs' Dirichlet params\n",
    "        p: exponent for lp-fn\n",
    "        lam: weight on regularization term\n",
    "        gamma: weight on entropy term (adversarial)\n",
    "\n",
    "    Returns:\n",
    "        Scalar Tensor: IRD loss\n",
    "    \"\"\"\n",
    "    lp_term: Tensor = lp_fn(alpha, y, p)\n",
    "    reg_term: Tensor = regularization_fn(alpha, y)\n",
    "\n",
    "    if adversarial_alpha is not None:\n",
    "        entropy_term: Tensor = dirichlet_entropy(adversarial_alpha)\n",
    "    else:\n",
    "        entropy_term = torch.tensor(0.0, device=alpha.device)\n",
    "\n",
    "    return lp_term + lam * reg_term - gamma * entropy_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00218f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6637, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# small test\n",
    "alpha = torch.tensor([[2.5, 1.2, 0.8], [1.2, 4.0, 1.3]], requires_grad=True)\n",
    "\n",
    "adversarial_alpha = torch.tensor([[5, 4, 3], [1.2, 2.0, 1.3]], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "loss = loss_IRD(alpha, y, adversarial_alpha)\n",
    "print(loss)  # tensor of shape (B,)\n",
    "loss.mean().backward()  # gradients flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa1340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
