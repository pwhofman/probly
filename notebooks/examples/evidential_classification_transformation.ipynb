{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96f1324",
   "metadata": {},
   "source": [
    "# `probly` Tutorial — Evidential Classification Transformation\n",
    "This notebook is a practical introduction to the **Evidential Classification transformation** in `probly`. Evidential Deep Learning is a powerful and computationally efficient method for uncertainty quantification that differs significantly from sampling-based approaches like MC-Dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fcf7db",
   "metadata": {},
   "source": [
    " We will start by explaining the core idea behind evidential learning and see how `probly`'s transformation helps you build such models. We will then walk through a PyTorch example to see how to get an uncertainty estimate from a **single forward pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3b636",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A — Introduction to Evidential Learning\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e509ae",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Evidential Classification?\n",
    "\n",
    "### 1.1 The Problem: Overconfident Softmax\n",
    "\n",
    "A standard classification network outputs logits, which are converted to probabilities using a `softmax` function. \n",
    "While useful, a high softmax probability (e.g., 0.99) is often misinterpreted as high model confidence. A model can be \"confidently wrong,\" especially on out-of-distribution data.\n",
    "\n",
    "### 1.2 The Evidential Approach: Learning \"Evidence\"\n",
    "Evidential Deep Learning reframes the problem. Instead of learning a direct mapping from input to class probabilities, the model learns to collect **evidence** for each class.\n",
    "Think of the model as a detective gathering clues for different suspects (the classes):\n",
    "-   If the model finds **many clues** pointing to one suspect and very few for others (e.g., evidence of `[100, 2, 5]`), it is very **confident**.\n",
    " -   If the model finds **very few clues for any suspect** (e.g., evidence of `[0.1, 0.2, 0.15]`), it is very **uncertain**. This might happen if the input is ambiguous or something the model has never seen before.\n",
    " The model's final output is a vector of these evidence scores. The total amount of evidence collected is a direct measure of confidence. \n",
    "\n",
    "  ### 1.3 The Evidential Transformation (probly)\n",
    "  The `probly` transformation helps you build an evidential model by ensuring the output can be interpreted as evidence.\n",
    "  -   You design your network as usual, but your final layer should output raw logits that represent the \"evidence.\"\n",
    "  -   The `evidential_classification` transformation simply **appends a `torch.nn.Softplus()` activation function.**\n",
    "  -   This ensures the evidence scores are always positive, a requirement for the underlying mathematical theory (the Dirichlet distribution).\n",
    "\n",
    "The uncertainty can then be calculated directly from these evidence scores in a **single forward pass**.\n",
    "\n",
    " ### 1.4 Short side‑by‑side comparison\n",
    "\n",
    "| Aspect                       | Evidential Classification                                        | Standard (Softmax) Classification |\n",
    "|------------------------------|------------------------------------------------------------------|-----------------------------------------            |\n",
    "| **Model Output**             | A vector of **evidence** for each class                          |  A vector of **probabilities** for each class.      |\n",
    "| **Final Activation**         | `Softplus` (to ensure positive evidence).                        | `Softmax` (to ensure probabilities sum to 1).       |\n",
    "| **Uncertainty Source**       | The **magnitude** of the total evidence.                         | No direct measure; high probability is a poor proxy.|\n",
    "| **Inference Cost**           |  **One single forward pass.**                                    | One single forward pass.                            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4bc88",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch) \n",
    "Below: build a small MLP and apply `evidential_classification(model)` to see how the final activation is appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca29520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "With Evidential transformation:\n",
      " Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
      "  )\n",
      "  (1): Softplus(beta=1.0, threshold=20.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import evidential_classification\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 3) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "# Apply the Evidential Classification transformation\n",
    "model_evidential = evidential_classification(model)\n",
    "print(\"\\nWith Evidential transformation:\\n\", model_evidential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cb0d8",
   "metadata": {},
   "source": [
    "### Notes on the structure\n",
    "-   Notice that the transformation has wrapped the original model in a `Sequential` module and **appended a `Softplus` layer** at the end.\n",
    "-   The output of this new model will now always be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99a4bf",
   "metadata": {},
   "source": [
    "## 3. Uncertainty from a Single Forward Pass\n",
    "The key advantage of evidential learning is that uncertainty can be calculated directly from the output of a single prediction.\n",
    "The output of the model gives us the evidence `alpha` for each class. The total evidence, or Dirichlet strength `S`, is the sum of all `alpha`. The uncertainty `u` is then simply the number of classes `K` divided by this strength.\n",
    " -   **High `S`** (lots of evidence) -> **Low `u`** (low uncertainty).\n",
    " -   **Low `S`** (little evidence) -> **High `u`** (high uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7776fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " tensor([[ 0.1167,  0.1689, -1.1233,  1.8116,  0.6322, -0.8759,  0.3580, -0.4363,\n",
      "         -0.7609,  1.5249]])\n",
      "\n",
      "Output Evidence (alpha):\n",
      " tensor([[0.8682, 0.9021, 0.5639]])\n",
      "\n",
      "Calculated Uncertainty: 0.5624\n",
      "\n",
      "Uncertainty for high evidence: 0.0273\n"
     ]
    }
   ],
   "source": [
    "from probly.quantification.classification import evidential_uncertainty\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create a dummy evidential model\n",
    "model_evidential = evidential_classification(build_mlp())\n",
    "\n",
    "# A dummy input\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# Get the evidence from a single forward pass\n",
    "with torch.no_grad():\n",
    "    evidence = model_evidential(x)\n",
    "\n",
    "# `probly` provides a function to calculate uncertainty directly\n",
    "uncertainty = evidential_uncertainty(evidence.numpy())\n",
    "\n",
    "print(\"Input data:\\n\", x)\n",
    "print(\"\\nOutput Evidence (alpha):\\n\", evidence)\n",
    "print(f\"\\nCalculated Uncertainty: {uncertainty.item():.4f}\")\n",
    "\n",
    "# Example with higher evidence (more confidence)\n",
    "high_evidence = torch.tensor([[100.0, 2.0, 5.0]])\n",
    "low_uncertainty = evidential_uncertainty(high_evidence.numpy())\n",
    "print(f\"\\nUncertainty for high evidence: {low_uncertainty.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b18d18",
   "metadata": {},
   "source": [
    "## 4. Part A Summary\n",
    "\n",
    "In Part A, we introduced Evidential Deep Learning as a powerful alternative to standard softmax classification. Instead of outputting probabilities, an evidential model outputs \"evidence\" for each class. We learned that the `probly` transformation makes this easy by appending a `Softplus` activation to a standard network. The key advantage is that model uncertainty can be directly calculated from the magnitude of this evidence in a **single, deterministic forward pass**, making it much faster than sampling-based methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f33a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# # Part B — Applied Evidential Classification\n",
    "\n",
    "----\n",
    "\n",
    "In **Part A**, we learned the concept of the **Evidential Classification transformation**.\n",
    "In this **Part B**, we will apply it to a classification model, get a prediction, and calculate the uncertainty from a single forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1197bcb",
   "metadata": {},
   "source": [
    "An indepth tutorial showing:\n",
    "- How to define a standard neural network (LeNet) and make it an Evidential model using the `evidential_classification` transformation.\n",
    "\n",
    "- How to set up the specialized training loop required for an Evidential model, using the Evidential Log Loss and a KL Divergence regularizer.\n",
    "\n",
    "- How to train the Evidential model on a real-world dataset **(FashionMNIST)**.\n",
    "\n",
    "- How to evaluate the final classification accuracy of the trained model.\n",
    "\n",
    "- How to compute and visualize Evidential Uncertainty by rotating an image.\n",
    "\n",
    "can be found here [Training an Evidential Model for Classification](train_evidential_classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653f5b",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    "# # Final Summary — Evidential Transformation Tutorial\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eecba7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This tutorial introduced the core concepts of **Evidential Deep Learning**, a powerful and efficient method for uncertainty quantification. We learned that instead of outputting probabilities like a standard classifier, an evidential model outputs **\"evidence\"** for each class.\n",
    "\n",
    "We saw that `probly`'s `evidential_classification` transformation automates this by simply appending a `Softplus` layer to a standard network, ensuring the evidence is always positive. The key advantage of this approach is its speed: a meaningful uncertainty score can be calculated directly from the magnitude of the evidence in a **single forward pass**.\n",
    "\n",
    "For a complete, end-to-end example that shows how to train an evidential model on the **FashionMNIST** dataset using the specialized evidential loss functions, please see the next tutorial: **[Training an Evidential Model for Classification](train_evidential_classification.ipynb)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
