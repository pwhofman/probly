{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b81968",
   "metadata": {},
   "source": [
    "# Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts\n",
    "\n",
    "Posterior Networks (PostNet) extend the idea of Evidential Deep Learning (EDL) by producing a full Dirichlet distribution over class probabilities for each input. However, instead of evidence being directly predicted by the neural network, PostNet does so by deriving evidence from class-conditional density estimates in a latent space. This assures that out-of-distribution (OOD) samples are not needed during training, as uncertainty increases for inputs that lie outside the learned density.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Build a small encoder to map inputs into a latent space\n",
    "- Train a single batched radial flow that models all class-conditional densities\n",
    "- Convert densities into evidence (pseudo-counts)\n",
    "- Construct Dirichlet posteriors and evaluate uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fd6fa",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d22d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from probly.train.evidential.common import unified_evidential_trainn\n",
    "from probly.train.evidential.torch import PostNetLoss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d91721",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Posterior Networks require:\n",
    "- an in-distribution (ID) dataset used for training and standard evaluation\n",
    "- an out-of-distribution dataset used only for testing epistemic uncertainty\n",
    "\n",
    "Here, we use **MNIST** as the ID dataset and **FashionMNIST** as the OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bbf28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST (ID) and FashionMNIST (OOD)\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Loaded MNIST (ID) and FashionMNIST (OOD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c437fd8",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Posterior Networks are composed of:\n",
    "1. **Encoder**: maps each image to a low-dimensional latent vector.\n",
    "2. **Class-conditional normalizing flows**: instead of training one flow per class, we use a single batched radial-flow model that jointly computes all class-conditional densities P(z|c). These densities provide the evidence used to construct the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4886e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder (x -> z)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2) -> None:  # noqa: ANN001\n",
    "        \"\"\"Initialize encoder with a small MLP and BatchNorm.\"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(latent_dim)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:  # noqa: ANN001\n",
    "        \"\"\"Encode a batch of images x into latent vectors z.\"\"\"\n",
    "        z = self.net(x)\n",
    "        z = self.bn(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "num_classes = 10\n",
    "\n",
    "encoder = Encoder(latent_dim).to(device)\n",
    "encoder  # noqa: B018\n",
    "\n",
    "# Normalizing flows\n",
    "\n",
    "\n",
    "class RadialFlowLayer(nn.Module):\n",
    "    \"\"\"Single radial flow transformation shared across all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int) -> None:\n",
    "        \"\"\"Initialize parameters for a radial flow transform.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.x0 = nn.Parameter(torch.zeros(self.c, self.dim))\n",
    "        self.alpha_prime = nn.Parameter(torch.zeros(self.c))\n",
    "        self.beta_prime = nn.Parameter(torch.zeros(self.c))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Reset learnable parameters with a small uniform init.\"\"\"\n",
    "        stdv = 1.0 / math.sqrt(self.dim)\n",
    "        self.x0.data.uniform_(-stdv, stdv)\n",
    "        self.alpha_prime.data.uniform_(-stdv, stdv)\n",
    "        self.beta_prime.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, zc) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Apply the radial flow to latent inputs zc.\"\"\"\n",
    "        alpha = torch.nn.functional.softplus(self.alpha_prime)\n",
    "        beta = -alpha + torch.nn.functional.softplus(self.beta_prime)\n",
    "\n",
    "        x0 = self.x0.unsqueeze(1)\n",
    "        diff = zc - x0\n",
    "        r = diff.norm(dim=-1)\n",
    "\n",
    "        h = 1.0 / (alpha.unsqueeze(1) + r)\n",
    "        h_prime = -h * h\n",
    "        beta_h = beta.unsqueeze(1) * h\n",
    "\n",
    "        z_new = zc + beta_h.unsqueeze(-1) * diff\n",
    "\n",
    "        term1 = (self.dim - 1) * torch.log1p(beta_h)\n",
    "        term2 = torch.log1p(beta_h + beta.unsqueeze(1) * h_prime * r)\n",
    "        log_abs_det = term1 + term2\n",
    "\n",
    "        return z_new, log_abs_det\n",
    "\n",
    "\n",
    "class BatchedRadialFlowDensity(nn.Module):\n",
    "    \"\"\"Radial-flow density estimator that computes P(z|c) for all classes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, dim: int, flow_length: int = 6) -> None:\n",
    "        \"\"\"Create a sequence of radial flow layers and base distribution.\"\"\"\n",
    "        super().__init__()\n",
    "        self.c = num_classes\n",
    "        self.dim = dim\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [RadialFlowLayer(num_classes, dim) for _ in range(flow_length)],\n",
    "        )\n",
    "\n",
    "        self.log_base_const = -0.5 * self.dim * math.log(2 * math.pi)\n",
    "\n",
    "    def forward(self, x) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: ANN001\n",
    "        \"\"\"Expand input x for all classes and apply flow layers.\"\"\"\n",
    "        B = x.size(0)  # noqa: N806\n",
    "        zc = x.unsqueeze(0).expand(self.c, B, self.dim)\n",
    "        sum_log_jac = torch.zeros(self.c, B, device=x.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            zc, log_j = layer(zc)\n",
    "            sum_log_jac = sum_log_jac + log_j\n",
    "\n",
    "        return zc, sum_log_jac\n",
    "\n",
    "    def log_prob(self, x) -> torch.Tensor:  # noqa: ANN001\n",
    "        \"\"\"Return class-conditional log densities log P(x|c).\"\"\"\n",
    "        zc, sum_log_jac = self.forward(x)  # zc: [C,B,D]\n",
    "\n",
    "        base_logp = self.log_base_const - 0.5 * (zc**2).sum(dim=-1)\n",
    "        logp = base_logp + sum_log_jac  # [C,B]\n",
    "\n",
    "        return logp.transpose(0, 1)  # [B,C]\n",
    "\n",
    "\n",
    "flow = BatchedRadialFlowDensity(num_classes, latent_dim, flow_length=6).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45df54f",
   "metadata": {},
   "source": [
    "## Dirichlet Evidence and PostNet Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937e8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = torch.zeros(num_classes, device=device)\n",
    "\n",
    "for _, y in train_loader:\n",
    "    y = y.to(device)  # noqa: PLW2901\n",
    "    for c in range(num_classes):\n",
    "        class_counts[c] += (y == c).sum()\n",
    "\n",
    "\n",
    "def postnet_loss(z, y, flow, class_counts, entropy_weight=1e-5) -> torch.Tensor:  # noqa: ANN001\n",
    "    log_dens = flow.log_prob(z)  # [B,C]\n",
    "    dens = log_dens.exp()\n",
    "\n",
    "    beta = dens * class_counts.unsqueeze(0)\n",
    "    alpha = beta + 1.0\n",
    "    alpha0 = alpha.sum(dim=1)\n",
    "\n",
    "    digamma = torch.digamma\n",
    "    batch_idx = torch.arange(len(y), device=y.device)\n",
    "    expected_ce = digamma(alpha0) - digamma(alpha[batch_idx, y])\n",
    "\n",
    "    entropy = Dirichlet(alpha).entropy()\n",
    "\n",
    "    loss = (expected_ce - entropy_weight * entropy).mean()\n",
    "    return loss, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79bb6d",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Loss: 1.8278\n",
      "Epoch [2/5] - Loss: 1.6705\n",
      "Epoch [3/5] - Loss: 1.6365\n",
      "Epoch [4/5] - Loss: 1.6253\n",
      "Epoch [5/5] - Loss: 1.6095\n"
     ]
    }
   ],
   "source": [
    "def train_postnet(encoder, flow, loader, class_counts, epochs=5, lr=1e-3) -> None:  # noqa: ANN001\n",
    "    encoder.train()\n",
    "    flow.train()\n",
    "\n",
    "    optim = torch.optim.Adam(list(encoder.parameters()) + list(flow.parameters()), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)  # noqa: PLW2901\n",
    "            z = encoder(x)\n",
    "\n",
    "            loss, _ = postnet_loss(z, y, flow, class_counts)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} — Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "latent_dim = 2\n",
    "loss = PostNetLoss()\n",
    "flow = BatchedRadialFlowDensity(num_classes=num_classes, dim=latent_dim, flow_length=6).to(device)\n",
    "\n",
    "unified_evidential_trainn(encoder, train_loader, loss, flow=flow, class_count=class_counts, epochs=5, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a82ee",
   "metadata": {},
   "source": [
    "## Evaluation: Predictions & Accuracy\n",
    "\n",
    "For each input, we compute the Dirichlet posterior parameters, derive the posterior mean class probabilitiesm and calculate accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, flow, loader, class_counts) -> float:  # noqa: ANN001\n",
    "    encoder.eval()\n",
    "    flow.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)  # noqa: PLW2901\n",
    "            z = encoder(x)\n",
    "\n",
    "            log_dens = flow.log_prob(z)\n",
    "            dens = log_dens.exp()\n",
    "            beta = dens * class_counts.unsqueeze(0)\n",
    "            alpha = beta + 1.0\n",
    "            alpha0 = alpha.sum(dim=1, keepdim=True)\n",
    "\n",
    "            probs = alpha / alpha0\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += len(y)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "evaluate(encoder, flow, test_loader, class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6207d",
   "metadata": {},
   "source": [
    "## Epistemic Uncertainty Extraction\n",
    "\n",
    "Posterior Networks quantify epistemic uncertainty using the total Dirichlet evidence, defined as the sum of all Dirichlet parameters for each class. High evidence indicates high confidence (in-distribution), while low evidence indicates uncertainty. Here, we compute evidence values for MNIST and FashionMNIST samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha0(encoder, flow, loader, class_counts) -> torch.Tensor:  # noqa: ANN001\n",
    "    encoder.eval()\n",
    "    flow.eval()\n",
    "\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)  # noqa: PLW2901\n",
    "            z = encoder(x)\n",
    "            log_dens = flow.log_prob(z)\n",
    "            dens = log_dens.exp()\n",
    "            beta = dens * class_counts.unsqueeze(0)\n",
    "            alpha = beta + 1.0\n",
    "            alpha0 = alpha.sum(dim=1)\n",
    "            out.append(alpha0.cpu())\n",
    "    return torch.cat(out)\n",
    "\n",
    "\n",
    "id_alpha0 = compute_alpha0(encoder, flow, test_loader, class_counts)\n",
    "ood_alpha0 = compute_alpha0(encoder, flow, ood_loader, class_counts)\n",
    "\n",
    "print(\"Mean ID α₀:\", id_alpha0.mean().item())\n",
    "print(\"Mean OOD α₀:\", ood_alpha0.mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
