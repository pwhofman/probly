{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8050dd9962ec34ba",
   "metadata": {},
   "source": [
    "# Uncertainty for a Synthetic Regression Task using probly\n",
    "\n",
    "This notebook gives an example for quantifying uncertainty in a synthetic regression setting using the setup from Valdenegro-Toro et al. (2022). Paper: https://arxiv.org/abs/2204.09308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:57:58.238562Z",
     "start_time": "2025-03-24T08:57:57.201945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab742c0274aa69",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "Generate a synthetic data set based on a sinusoidal function. Sample data points (x) uniformly between 0 and 10 and generate labels based on the function. Create training data, test data and out of distribution data (x between 10 and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d1b59ece7440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_function(x: np.ndarray, *, remove_noise: bool = False) -> np.ndarray:\n",
    "    \"\"\"Sigmoidal function to generate synthetic data.\n",
    "\n",
    "    Args:\n",
    "        x: numpy.ndarray, input data\n",
    "        remove_noise: bool, whether to have noise in the function outcome or not\n",
    "    Returns:\n",
    "        numpy.ndarray, output data\n",
    "    \"\"\"\n",
    "    eps1 = np.random.normal(0, 0.3, x.shape)\n",
    "    eps2 = np.random.normal(0, 0.1, x.shape)\n",
    "    if remove_noise:\n",
    "        eps1 = np.mean(eps1)\n",
    "        eps2 = np.mean(eps2)\n",
    "    return x * np.sin(x) + eps1 * x + eps2\n",
    "\n",
    "\n",
    "# Generate data points between 0 and 10\n",
    "X = np.expand_dims(np.random.uniform(0, 10, 1500), axis=1)\n",
    "X_train = X[:1000]\n",
    "X_test = X[1000:]\n",
    "\n",
    "# Generate ood data points between 10 and 15\n",
    "X_ood = np.expand_dims(np.random.uniform(10, 15, 200), axis=1)\n",
    "\n",
    "# Generate labels\n",
    "y = toy_function(X)\n",
    "y_train = y[:1000, 0]\n",
    "y_test = y[1000:, 0]\n",
    "\n",
    "# Sort test/ood samples for easy plotting\n",
    "id_test = np.argsort(X_test[:, 0])\n",
    "id_ood = np.argsort(X_ood[:, 0])\n",
    "X_test, y_test, X_ood = X_test[id_test], y_test[id_test], X_ood[id_ood]\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X_train[:, 0], y_train, c=\"darkorange\", s=5, alpha=0.4, label=\"Train\")\n",
    "plt.scatter(X_test[:, 0], y_test, c=\"darkgreen\", s=5, alpha=0.4, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43685cca7222fa21",
   "metadata": {},
   "source": [
    "### Create Dropout Model\n",
    "Create a simple neural network and transform it to a Dropout model using probly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d54d3369c53736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:58:02.305066Z",
     "start_time": "2025-03-24T08:58:01.700571Z"
    }
   },
   "outputs": [],
   "source": [
    "from probly.representation import Dropout\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Simple Neural Network class with two heads.\n",
    "\n",
    "    Attributes:\n",
    "        fc1: nn.Module, first fully connected layer\n",
    "        fc2: nn.Module, second fully connected layer\n",
    "        fc31: nn.Module, fully connected layer of first head\n",
    "        fc32: nn.Module, fully connected layer of second head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize an instance of the Net class.\"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc31 = nn.Linear(32, 1)\n",
    "\n",
    "        self.fc32 = nn.Linear(32, 1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x: torch.Tensor, input data\n",
    "        Returns:\n",
    "            torch.Tensor, output data\n",
    "        \"\"\"\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        mu = self.fc31(x)\n",
    "        sigma2 = F.softplus(self.fc32(x))\n",
    "        x = torch.cat([mu, sigma2], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# transform model to a Dropout model\n",
    "model = Dropout(net, p=0.25)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "class GaussianNLL(nn.Module):\n",
    "    \"\"\"Implementation of the Gaussian negative log-likelihood loss.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize an instance of the GaussianNLL class.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, mu: torch.Tensor, sigma2: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the Gaussian negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            mu: torch.Tensor, predicted mean\n",
    "            sigma2: torch.Tensor, predicted variance\n",
    "            y: torch.Tensor, target labels\n",
    "        \"\"\"\n",
    "        return 0.5 * torch.mean(torch.log(sigma2) + (y - mu) ** 2 / sigma2)\n",
    "\n",
    "\n",
    "criterion = GaussianNLL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b870e0d409dad5",
   "metadata": {},
   "source": [
    "### Train Dropout Model\n",
    "Train the Dropout model based on the training data, the given loss (criterion) and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c412ebe550bd2a0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:58:14.010454Z",
     "start_time": "2025-03-24T08:58:03.476126Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7382 Progress: 100%|██████████| 700/700 [00:10<00:00, 66.62epoch/s] \n"
     ]
    }
   ],
   "source": [
    "epochs = 700\n",
    "\n",
    "pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "for _ in pbar:\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, 1)\n",
    "        mean = outputs[:, 0, 0]\n",
    "        var = outputs[:, 0, 1]\n",
    "        loss = criterion(mean, var, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    pbar.set_description(desc=f\"Loss: {loss:.4f} Progress\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f983af69bef599",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluate model performance and uncertainty behavior for the test and ood data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c7121070534644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T08:58:24.111436Z",
     "start_time": "2025-03-24T08:58:24.040170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 12.30\n"
     ]
    }
   ],
   "source": [
    "from probly.quantification.regression import (\n",
    "    expected_conditional_variance,\n",
    "    total_variance,\n",
    "    variance_conditional_expectation,\n",
    ")\n",
    "\n",
    "# generate prediction\n",
    "y_pred = model(torch.from_numpy(X_test).float(), 100).detach().cpu().numpy()\n",
    "y_pred_ood = model(torch.from_numpy(X_ood).float(), 100).detach().cpu().numpy()\n",
    "\n",
    "# evaluate model performance\n",
    "mse = ((y_pred.mean(axis=1)[:, 0] - y_test) ** 2).mean()\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "\n",
    "# quantify uncertainty\n",
    "tu = total_variance(y_pred)\n",
    "au = expected_conditional_variance(y_pred)\n",
    "eu = variance_conditional_expectation(y_pred)\n",
    "tu_ood = total_variance(y_pred_ood)\n",
    "au_ood = expected_conditional_variance(y_pred_ood)\n",
    "eu_ood = variance_conditional_expectation(y_pred_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56593b5ac41830ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot uncertainty\n",
    "plot_x = np.concatenate((X_test, X_ood), axis=0)[2:, 0]  # cut off first two values to increase smoothness\n",
    "plot_y = np.concatenate((y_pred.mean(axis=1), y_pred_ood.mean(axis=1)), axis=0)[2:, 0]\n",
    "plot_tu = np.concatenate((tu, tu_ood), axis=0)[2:]\n",
    "plot_au = np.concatenate((au, au_ood), axis=0)[2:]\n",
    "plot_eu = np.concatenate((eu, eu_ood), axis=0)[2:]\n",
    "plt.plot(X_test, toy_function(X_test, remove_noise=True), label=\"Ground truth\", c=\"k\")\n",
    "plt.scatter(plot_x, plot_y, c=\"darkblue\", label=\"Mean Prediction\", s=1, zorder=10)\n",
    "plt.fill_between(\n",
    "    plot_x,\n",
    "    plot_y - (plot_tu / 2),\n",
    "    plot_y + (plot_tu / 2),\n",
    "    alpha=0.2,\n",
    "    color=\"darkgreen\",\n",
    "    label=\"Total Uncertainty\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    plot_x,\n",
    "    plot_y - (plot_au / 2),\n",
    "    plot_y + (plot_au / 2),\n",
    "    alpha=0.2,\n",
    "    color=\"darkred\",\n",
    "    label=\"Aleatoric Uncertainty\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    plot_x,\n",
    "    plot_y - (plot_eu / 2),\n",
    "    plot_y + (plot_eu / 2),\n",
    "    alpha=0.2,\n",
    "    color=\"darkgoldenrod\",\n",
    "    label=\"Epistemic Uncertainty\",\n",
    ")\n",
    "plt.ylim([-20, 20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
