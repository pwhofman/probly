{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e47f87",
   "metadata": {},
   "source": [
    "# Multi-Loss Sub-Ensemble Example\n",
    "\n",
    "In this notebook we explore **Sub-Ensemble** methods for uncertainty estimation, focusing on\n",
    "a **Multi-Loss Sub-Ensemble** architecture inspired by:\n",
    "\n",
    "> \"Multi-Loss Sub-Ensembles for Accurate Classification with Uncertainty Estimation\"\n",
    "> (Achrack et al., 2020, arXiv:2010.01917)\n",
    "\n",
    "Instead of training many independent models as a full ensemble, we:\n",
    "\n",
    "- use a **shared backbone** (trunk) to extract features, and\n",
    "- attach several **classifier heads** (sub-ensembles) on top of the trunk.\n",
    "\n",
    "Each head is trained with a **different loss function**, which introduces diversity between\n",
    "the heads while keeping training in a single phase and with a single backbone.\n",
    "\n",
    "The goals of this notebook are:\n",
    "\n",
    "- to explain the idea of Sub-Ensembles and how Multi-Loss Sub-Ensembles extend them,\n",
    "- to implement a simple shared-trunk + multi-head architecture on CIFAR-10 in PyTorch,\n",
    "- to train different heads with different losses,\n",
    "- and to show how to obtain **accuracy** and **uncertainty** estimates from the disagreement\n",
    "  between heads.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "1. Imports & Setup  \n",
    "2. Full Ensembles vs. Sub-Ensembles  \n",
    "3. Multi-Loss Sub-Ensembles  \n",
    "4. Data Preparation  \n",
    "5. Model Definition (shared trunk + multiple heads)  \n",
    "6. Multi-Loss Sub-Ensemble Training Function  \n",
    "7. Short-run Experiment on CIFAR-10  \n",
    "8. Evaluation: Predictions & Uncertainty  \n",
    "9. Conclusion and connection to `probly`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41308c47",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup\n",
    "\n",
    "We first import the required libraries:\n",
    "\n",
    "- **torch, torchvision**: core tools for neural networks and datasets,\n",
    "- **CIFAR-10**: small image classification dataset,\n",
    "- standard utilities for data loading and device selection.\n",
    "\n",
    "Later, the same ideas can be integrated into the `probly` package and wrapped by a\n",
    "SubEnsemble utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6b4b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b641975",
   "metadata": {},
   "source": [
    "## 2. Full Ensembles vs. Sub-Ensembles\n",
    "\n",
    "### 2.1 Full ensembles\n",
    "\n",
    "A **full ensemble** consists of several independent models:\n",
    "\n",
    "- each model has its own parameters and random initialisation,\n",
    "- all models are trained separately on the same (or slightly resampled) dataset,\n",
    "- at test time, their predictions are averaged (or combined in some other way).\n",
    "\n",
    "Formally, for an input $x$ and $M$ models $f_m$, we obtain predictions\n",
    "$\\hat{y}_m = f_m(x)$ and aggregate them as\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\text{ens}}(x) = \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_m.\n",
    "$$\n",
    "\n",
    "Full ensembles are strong baselines for both **accuracy** and **uncertainty**, but they are:\n",
    "\n",
    "- computationally expensive (M separate models),\n",
    "- memory-intensive (M copies of all weights),\n",
    "- and sometimes hard to deploy in practice.\n",
    "\n",
    "### 2.2 Sub-Ensembles\n",
    "\n",
    "**Sub-Ensembles** aim to reduce this cost by sharing a large part of the model:\n",
    "\n",
    "- a single **trunk** processes the input and produces a feature representation,\n",
    "- multiple **heads** (sub-ensembles) take the same features and output predictions,\n",
    "- at test time, the heads are treated as ensemble members and their outputs are aggregated.\n",
    "\n",
    "This keeps many of the benefits of ensembles:\n",
    "\n",
    "- diversity between heads,\n",
    "- the ability to measure disagreement,\n",
    "- and simple averaging for predictions,\n",
    "\n",
    "while requiring only **one backbone** to be stored and operated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4a86d",
   "metadata": {},
   "source": [
    "## 3. Multi-Loss Sub-Ensembles\n",
    "\n",
    "A **Multi-Loss Sub-Ensemble** keeps the shared trunk + multi-head structure, but introduces\n",
    "an additional source of diversity:\n",
    "\n",
    "> each head is trained with a **different loss function**.\n",
    "\n",
    "Intuitively:\n",
    "\n",
    "- a standard Sub-Ensemble uses the same training objective for all heads,\n",
    "- a Multi-Loss Sub-Ensemble lets each head see the same data through a different loss.\n",
    "\n",
    "In this notebook we combine:\n",
    "\n",
    "- a head trained with standard cross-entropy,\n",
    "- a head trained with label-smoothed cross-entropy,\n",
    "- a head trained with a simple margin-based loss that enforces a gap between the correct\n",
    "  class and the most likely wrong class.\n",
    "\n",
    "Even though all heads share the same backbone, the different losses encourage:\n",
    "\n",
    "- slightly different decision boundaries,\n",
    "- different confidence behaviours,\n",
    "- and therefore more informative disagreement patterns between heads.\n",
    "\n",
    "We will now implement this setting with a small CNN trunk and three heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e388d",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "We use CIFAR-10 as a small image classification benchmark:\n",
    "\n",
    "- 50,000 training images and 10,000 test images,\n",
    "- 10 classes,\n",
    "- RGB images of size 32x32.\n",
    "\n",
    "We apply:\n",
    "\n",
    "- conversion to tensors,\n",
    "- simple normalisation of pixel values,\n",
    "- `DataLoader` wrappers for batching and shuffling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb71521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:45<00:00, 3.74MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000,  Val samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "val_data = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)},  Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091f120",
   "metadata": {},
   "source": [
    "## 5. Model Definition: shared trunk + multiple heads\n",
    "\n",
    "We now define a small CNN with:\n",
    "\n",
    "- one **shared trunk** that extracts features from the input image, and\n",
    "- several **classifier heads** (sub-ensembles), each producing logits for the 10 classes.\n",
    "\n",
    "During training, each head will be paired with a different loss function.\n",
    "At inference time, all heads are treated as ensemble members and their\n",
    "predictions are aggregated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de693df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubEnsembleNet(\n",
      "  (trunk): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SubEnsembleNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, num_heads: int = 3) -> None:\n",
    "        \"\"\"Initialize the shared-trunk multi-head classifier.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # shared trunk\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        trunk_out_dim = 64 * 8 * 8\n",
    "\n",
    "        # multiple classifier heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [nn.Linear(trunk_out_dim, num_classes) for _ in range(num_heads)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
    "        \"\"\"Compute logits for each head.\"\"\"\n",
    "        feat = self.trunk(x)\n",
    "        logits_per_head = [head(feat) for head in self.heads]\n",
    "        return logits_per_head\n",
    "\n",
    "\n",
    "num_heads = 3\n",
    "model = SubEnsembleNet(num_classes=10, num_heads=num_heads).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d784e05",
   "metadata": {},
   "source": [
    "## 6. Multi-Loss Sub-Ensemble Training Function\n",
    "\n",
    "We now define:\n",
    "\n",
    "1. a small set of loss functions:\n",
    "   - standard cross-entropy,\n",
    "   - label-smoothed cross-entropy,\n",
    "   - a margin-based loss that enforces a gap between the correct and the most likely\n",
    "     incorrect logit;\n",
    "\n",
    "2. a unified training routine:\n",
    "\n",
    "   `train_multi_loss_sub_ensemble(model, dataloader, losses, ...)`\n",
    "\n",
    "The training function:\n",
    "\n",
    "- moves the model to the chosen device,\n",
    "- iterates over epochs and batches,\n",
    "- computes logits for each head,\n",
    "- applies a different loss to each head,\n",
    "- sums the per-head losses into a single scalar,\n",
    "- performs backpropagation and optimisation.\n",
    "\n",
    "This mimics how a future SubEnsemble training utility in `probly` could look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f08d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Small implementation of label-smoothed cross-entropy.\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing: float = 0.1) -> None:\n",
    "        \"\"\"Initialize the loss with a smoothing factor.\"\"\"\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the smoothed cross-entropy loss.\"\"\"\n",
    "        num_classes = logits.size(-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logits)\n",
    "            true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        loss = torch.sum(-true_dist * log_probs, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class MarginLoss(nn.Module):\n",
    "    \"\"\"Encourage margin between correct logit and max wrong logit.\"\"\"\n",
    "\n",
    "    def __init__(self, margin: float = 0.5) -> None:\n",
    "        \"\"\"Initialize the margin loss with a target margin.\"\"\"\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute the margin-based loss.\"\"\"\n",
    "        num_classes = logits.size(-1)\n",
    "        one_hot = F.one_hot(target, num_classes=num_classes).bool()\n",
    "\n",
    "        correct_logit = logits[one_hot].view(-1)\n",
    "        wrong_logits = logits.masked_fill(one_hot, float(\"-inf\"))\n",
    "        max_wrong_logit, _ = wrong_logits.max(dim=-1)\n",
    "\n",
    "        margin_term = correct_logit - max_wrong_logit\n",
    "        loss = F.relu(self.margin - margin_term)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21dacc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_loss_sub_ensemble(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    losses: list[nn.Module],\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\",\n",
    ") -> None:\n",
    "    \"\"\"Demonstration of a multi-loss sub-ensemble training function.\"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if len(losses) != model.num_heads:\n",
    "        msg = f\"Need one loss per head, got {len(losses)} losses for {model.num_heads} heads.\"\n",
    "        raise ValueError(msg)\n",
    "    losses = [ls.to(device) for ls in losses]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            inputs = batch_inputs.to(device)\n",
    "            targets = batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_per_head = model(inputs)\n",
    "\n",
    "            batch_loss = 0.0\n",
    "            for head_logits, loss_fn in zip(logits_per_head, losses, strict=False):\n",
    "                batch_loss = batch_loss + loss_fn(head_logits, targets)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, num_batches)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5e76",
   "metadata": {},
   "source": [
    "## 7. Short-run experiment on CIFAR-10\n",
    "\n",
    "To keep the runtime manageable, we perform a **short-run** experiment on CIFAR-10:\n",
    "\n",
    "- Dataset: CIFAR-10 (50k train, 10k test)\n",
    "- Model: small CNN trunk with 3 classifier heads\n",
    "- Heads / losses:\n",
    "  - Head 1: standard cross-entropy\n",
    "  - Head 2: label-smoothed cross-entropy\n",
    "  - Head 3: margin-based loss\n",
    "- Training: a few epochs with Adam\n",
    "- Evaluation: accuracy and simple uncertainty metrics on the validation set\n",
    "\n",
    "This is not meant to be a state-of-the-art benchmark.  \n",
    "The goal is to clearly illustrate how a Multi-Loss Sub-Ensemble can be trained and how\n",
    "head disagreement can be turned into uncertainty estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093413ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Loss: 3.6277\n",
      "Epoch [2/2] - Loss: 2.8852\n"
     ]
    }
   ],
   "source": [
    "num_heads = 3\n",
    "base_model = SubEnsembleNet(num_classes=10, num_heads=num_heads)\n",
    "\n",
    "loss_list: list[nn.Module] = [\n",
    "    nn.CrossEntropyLoss(),\n",
    "    LabelSmoothingCrossEntropy(smoothing=0.1),\n",
    "    MarginLoss(margin=0.5),\n",
    "]\n",
    "\n",
    "train_multi_loss_sub_ensemble(\n",
    "    model=base_model,\n",
    "    dataloader=train_loader,\n",
    "    losses=loss_list,\n",
    "    epochs=2,  # keep small for a quick demo\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4817573",
   "metadata": {},
   "source": [
    "## 8. Evaluation: accuracy and uncertainty\n",
    "\n",
    "After training, we treat each head as an ensemble member:\n",
    "\n",
    "1. For each input, we collect the softmax probabilities from all heads.\n",
    "2. We compute the **mean prediction** by averaging probabilities over heads.\n",
    "3. From this, we derive:\n",
    "\n",
    "   - the **ensemble accuracy** (argmax of the mean prediction),\n",
    "   - the **predictive entropy** of the mean prediction,\n",
    "   - the **variance** of the predicted probability of the chosen class across heads.\n",
    "\n",
    "The entropy and variance are simple but useful uncertainty indicators:\n",
    "\n",
    "- high entropy / variance → heads disagree or are unsure,  \n",
    "- low entropy / variance → heads agree and are confident.\n",
    "\n",
    "In practice, misclassified samples tend to have higher uncertainty scores than correctly\n",
    "classified ones, which can be exploited for tasks such as rejection, out-of-distribution\n",
    "detection, or risk-aware decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6888ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_uncertainty(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_mean_probs = []\n",
    "    all_targets = []\n",
    "    all_entropies = []\n",
    "    all_var_maxclass = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            inputs = batch_inputs.to(device)\n",
    "            targets = batch_targets.to(device)\n",
    "\n",
    "            logits_per_head = model(inputs)\n",
    "            probs_per_head = [F.softmax(logits, dim=-1) for logits in logits_per_head]\n",
    "\n",
    "            # stack along head axis: (heads, batch, classes)\n",
    "            probs_stack = torch.stack(probs_per_head, dim=0)\n",
    "\n",
    "            mean_probs = probs_stack.mean(dim=0)  # (batch, classes)\n",
    "            all_mean_probs.append(mean_probs.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "\n",
    "            # predictive entropy of mean prediction\n",
    "            entropy = -torch.sum(mean_probs * torch.log(mean_probs + 1e-8), dim=-1)\n",
    "\n",
    "            # variance of probability of the predicted class across heads\n",
    "            preds = mean_probs.argmax(dim=-1)  # (batch,)\n",
    "            head_probs_max = []\n",
    "            for probs in probs_per_head:\n",
    "                head_probs_max.append(\n",
    "                    probs[torch.arange(probs.size(0), device=probs.device), preds],\n",
    "                )\n",
    "            head_probs_max = torch.stack(head_probs_max, dim=0)  # (heads, batch)\n",
    "            var_max = head_probs_max.var(dim=0)\n",
    "\n",
    "            all_entropies.append(entropy.cpu())\n",
    "            all_var_maxclass.append(var_max.cpu())\n",
    "\n",
    "    mean_probs = torch.cat(all_mean_probs, dim=0)\n",
    "    targets = torch.cat(all_targets, dim=0)\n",
    "    entropies = torch.cat(all_entropies, dim=0)\n",
    "    var_maxclass = torch.cat(all_var_maxclass, dim=0)\n",
    "\n",
    "    preds = mean_probs.argmax(dim=-1)\n",
    "    acc = (preds == targets).float().mean().item()\n",
    "\n",
    "    print(f\"Validation accuracy (ensemble): {acc:.4f}\")\n",
    "    print(f\"Entropy: mean={entropies.mean():.4f}, std={entropies.std():.4f}\")\n",
    "    print(f\"Var(max-class prob): mean={var_maxclass.mean():.4f}, std={var_maxclass.std():.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mean_probs\": mean_probs,\n",
    "        \"targets\": targets,\n",
    "        \"entropy\": entropies,\n",
    "        \"var_maxclass\": var_maxclass,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbee59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (ensemble): 0.6439\n",
      "Entropy: mean=1.6502, std=0.3162\n",
      "Var(max-class prob): mean=0.0443, std=0.0339\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_with_uncertainty(base_model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fa47083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct samples: 6439\n",
      "Wrong samples   : 3561\n",
      "Entropy  - correct: 1.5590, wrong: 1.8151\n",
      "Var(max) - correct: 0.0546, wrong: 0.0258\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = metrics[\"mean_probs\"]  # (N, num_classes)\n",
    "targets = metrics[\"targets\"]  # (N,)\n",
    "entropy = metrics[\"entropy\"]  # (N,)\n",
    "var_max = metrics[\"var_maxclass\"]  # (N,)\n",
    "\n",
    "# boolean mask for correct / wrong predictions\n",
    "preds = probs.argmax(dim=-1)\n",
    "correct_mask = preds == targets\n",
    "wrong_mask = ~correct_mask\n",
    "\n",
    "# compute mean uncertainty for each group\n",
    "mean_ent_correct = entropy[correct_mask].mean().item()\n",
    "mean_ent_wrong = entropy[wrong_mask].mean().item()\n",
    "\n",
    "mean_var_correct = var_max[correct_mask].mean().item()\n",
    "mean_var_wrong = var_max[wrong_mask].mean().item()\n",
    "\n",
    "print(\"Correct samples:\", int(correct_mask.sum()))\n",
    "print(\"Wrong samples   :\", int(wrong_mask.sum()))\n",
    "print(f\"Entropy  - correct: {mean_ent_correct:.4f}, wrong: {mean_ent_wrong:.4f}\")\n",
    "print(f\"Var(max) - correct: {mean_var_correct:.4f}, wrong: {mean_var_wrong:.4f}\")\n",
    "\n",
    "# --- bar plots ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# entropy\n",
    "axes[0].bar([\"correct\", \"wrong\"], [mean_ent_correct, mean_ent_wrong])\n",
    "axes[0].set_title(\"Mean predictive entropy\")\n",
    "axes[0].set_ylabel(\"entropy\")\n",
    "\n",
    "# variance of max-class prob\n",
    "axes[1].bar([\"correct\", \"wrong\"], [mean_var_correct, mean_var_wrong])\n",
    "axes[1].set_title(\"Mean var of max-class prob\")\n",
    "axes[1].set_ylabel(\"variance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41de76b",
   "metadata": {},
   "source": [
    "We see that both entropy and the variance of the max-class probability are higher on misclassified samples than on correctly classified ones, which indicates that the Multi-Loss Sub-Ensemble provides meaningful uncertainty estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fb5f0",
   "metadata": {},
   "source": [
    "## 9. Conclusion and connection to `probly`\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- reviewed the idea of full ensembles and their computational cost,\n",
    "- introduced Sub-Ensembles with a shared backbone and multiple heads,\n",
    "- extended this to Multi-Loss Sub-Ensembles, where each head uses a different loss,\n",
    "- implemented a simple CNN-based Multi-Loss Sub-Ensemble on CIFAR-10,\n",
    "- and demonstrated how to extract accuracy and basic uncertainty metrics from the ensemble.\n",
    "\n",
    "From the perspective of `probly`, Multi-Loss Sub-Ensembles are a natural example of\n",
    "an **extended ensemble structure**:\n",
    "\n",
    "- the ensemble axis is realised by multiple heads,\n",
    "- parameters are partially shared (backbone) and partially head-specific,\n",
    "- each head can be associated with its own loss and possibly its own training settings.\n",
    "\n",
    "Future work in `probly` could expose this pattern via a dedicated SubEnsemble\n",
    "transformation, similar to existing ensemble utilities, so that:\n",
    "\n",
    "- users can quickly wrap an existing backbone into a Multi-Loss Sub-Ensemble,\n",
    "- and obtain uncertainty estimates without writing the full multi-head training loop\n",
    "  themselves.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
