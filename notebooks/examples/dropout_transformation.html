<!doctype html>
<html class="no-js" lang="en" data-content_root="../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" /><link rel="next" title="Ensemble Transformation" href="ensemble_transformation.html" /><link rel="prev" title="Dropconnect Transformation" href="dropconnect_transformation.html" />

    <!-- Generated with Sphinx 8.2.3 and Furo 2024.08.06 -->
        <title>Dropout Transformation - probly 0.3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=201d0c9a" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">probly 0.3.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><div class="sidebar-scroll"><a class="sidebar-brand" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo/logo_light.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../../_static/logo/logo_dark.png" alt="Dark Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">The <code class="docutils literal notranslate"><span class="pre">probly</span></code> Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core_concepts.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../main_components.html">Main Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_topics.html">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples_and_tutorials.html">Examples and Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../methods.html">Implemented methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to probly üèîÔ∏è</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references.html">References and Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ and Troubleshooting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Notebook Examples</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Notebook Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="utilities_and_layers/index.html">Utilities and Layers</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Utilities and Layers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="utilities_and_layers/custom_loss_functions.html">Custom Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilities_and_layers/metrics.html">Evaluation Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="utilities_and_layers/probabilistic_layers.html">Key Probabilistic Layers in <code class="docutils literal notranslate"><span class="pre">probly</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="utilities_and_layers/utility_functions.html">Utility Functions</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="evaluation_and_quantification/index.html">Evaluation and Quantification</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Evaluation and Quantification</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="evaluation_and_quantification/calibration_metrics.html">Calibration Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation_and_quantification/interpretation_techniques.html">Interpretation techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation_and_quantification/visualization_tools.html">Visualisation Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="bayesian_transformation.html">Bayesian Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dropconnect_transformation.html">Dropconnect Transformation</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Dropout Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ensemble_transformation.html">Ensemble Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="evidential_classification_transformation.html">Evidential Classification Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="evidential_regression_transformation.html">Evidential Regression Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="fashionmnist_ood_ensemble.html">Out-of-Distribution Detection with an Ensemble</a></li>
<li class="toctree-l2"><a class="reference internal" href="label_relaxation_calibration.html">Calibration with Label Relaxation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lazy_dispatch_test.html">Lazy Dispatch Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="multilib_demo.html">Multilib Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytraverse_tutorial.html">A Brief Introduction to PyTraverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn_selective_prediction.html">Using probly with scikit-learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="temperature_scaling_calibration.html">Calibration using Temperature Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_bnn_classification.html">Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_evidential_classification.html">Evidential Model for Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="train_evidential_regression.html">Evidential Regression Model</a></li>
</ul>
</li>
</ul>

</div>
</div><div style="text-align: center; margin-top: 1rem; margin-bottom: 1rem;">
  <a href="https://github.com/pwhofman/probly" target="_blank" rel="noopener noreferrer" title="GitHub Repository">
    <img src="../../_static/github-mark.svg" alt="GitHub Logo" width="28" height="28" style="display: inline-block;">
  </a>
</div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          

<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="dropout-transformation">
<h1>Dropout Transformation<a class="headerlink" href="#dropout-transformation" title="Link to this heading">¬∂</a></h1>
<p><em>Date:</em> 2025-11-03</p>
<p><strong>Audience:</strong> New probly users ¬∑ <strong>Framework:</strong> PyTorch ¬∑ <strong>Author:</strong> Nidhi Jain and Julia Goihman</p>
<p>This notebook is meant as a gentle, practical introduction to the <strong>Dropout transformation</strong> in <code class="docutils literal notranslate"><span class="pre">probly</span></code>.
The goal is not to be mathematically perfect, but to give you an intuition you can actually use when you
work on models in PyTorch.</p>
<p>We will slowly build up from the very basic idea of <em>normal</em> Dropout to the slightly more advanced idea of
a <strong>Dropout transformation that makes a model uncertainty‚Äëaware</strong>. After that, we look at a tiny PyTorch
example and inspect how the transformation changes the model.</p>
<hr class="docutils" />
<section id="part-a-introduction-to-dropout-and-the-dropout-transformation">
<h2>Part A ‚Äî Introduction to Dropout and the Dropout Transformation<a class="headerlink" href="#part-a-introduction-to-dropout-and-the-dropout-transformation" title="Link to this heading">¬∂</a></h2>
</section>
<hr class="docutils" />
<section id="concept-what-is-dropout-normal-vs-dropout-transformation">
<h2>1. Concept: What is Dropout (normal) vs Dropout Transformation?<a class="headerlink" href="#concept-what-is-dropout-normal-vs-dropout-transformation" title="Link to this heading">¬∂</a></h2>
<p>The original question for this part is:</p>
<blockquote>
<div><p><strong>‚ÄúWhat is Dropout (normal) vs Dropout Transformation?‚Äù</strong>
<strong>1.1 Normal Dropout (no probly, just PyTorch)</strong>
In ‚Äúnormal‚Äù deep learning, Dropout is a layer used during training to reduce overfitting.
Overfitting = model memorizes training data and sucks on new data.</p>
</div></blockquote>
<p>Below is a more detailed version of that explanation, in my own words.</p>
<section id="normal-dropout-standard-pytorch-dropout-layer">
<h3>1.1 Normal Dropout (standard PyTorch Dropout layer)<a class="headerlink" href="#normal-dropout-standard-pytorch-dropout-layer" title="Link to this heading">¬∂</a></h3>
<p>When we train a neural network, there is always the risk of <strong>overfitting</strong>. That means the model becomes
very good at the training set but fails on new data, because it has more or less <em>memorised</em> patterns that
only appear in the training examples.</p>
<p><strong>Normal Dropout</strong> is a simple trick to make overfitting less likely. During training, a <code class="docutils literal notranslate"><span class="pre">Dropout(p)</span></code> layer
will, for every mini‚Äëbatch, randomly set a fraction <code class="docutils literal notranslate"><span class="pre">p</span></code> of its input activations to zero. You can imagine
this as:</p>
<ul class="simple">
<li><p>with probability <code class="docutils literal notranslate"><span class="pre">p</span></code> a neuron is ‚Äúswitched off‚Äù for this training step,</p></li>
<li><p>with probability <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">‚àí</span> <span class="pre">p</span></code> it behaves as usual.</p></li>
</ul>
<p>Because different neurons get switched off in every step, the network is forced to <strong>spread the information</strong>
across many neurons instead of relying on a few very strong ones. This usually makes the model <strong>more robust</strong>
and helps it generalise better.</p>
<p>Important detail: in <strong>normal PyTorch usage</strong></p>
<ul class="simple">
<li><p>Dropout is <strong>active only in training mode</strong> (<code class="docutils literal notranslate"><span class="pre">model.train()</span></code>),</p></li>
<li><p>and it is <strong>disabled in evaluation mode</strong> (<code class="docutils literal notranslate"><span class="pre">model.eval()</span></code>).</p></li>
</ul>
<p>So at test / inference time, the model behaves like a <strong>deterministic function</strong>: the same input always gives
the same output, and there is no randomness from Dropout anymore. The purpose of normal Dropout is therefore
<em>only</em> to improve generalisation during training, not to provide uncertainty information.</p>
</section>
<section id="dropout-transformation-probly">
<h3>1.2 Dropout Transformation (probly)<a class="headerlink" href="#dropout-transformation-probly" title="Link to this heading">¬∂</a></h3>
<p>The <strong>Dropout transformation</strong> in <code class="docutils literal notranslate"><span class="pre">probly</span></code> takes this Dropout idea and uses it in a slightly different role.
Instead of treating Dropout purely as a regularisation trick during training, we use it to make the model
<strong>uncertainty‚Äëaware</strong> at prediction time.</p>
<p>Roughly speaking, the transformation does the following:</p>
<ul class="simple">
<li><p>It walks through your PyTorch model and finds the relevant linear layers.</p></li>
<li><p>It programmatically inserts Dropout layers around those linear layers.</p></li>
<li><p>Crucially, these Dropout layers stay <strong>active during inference</strong>, so each forward pass is a bit different.</p></li>
</ul>
<p>If we now feed the <strong>same input</strong> through the transformed model multiple times, we do <strong>not</strong> get exactly the
same output each time. Instead we get a <em>cloud</em> of slightly different predictions. From this cloud we can:</p>
<ul class="simple">
<li><p>compute a mean prediction (what the model ‚Äúon average‚Äù thinks), and</p></li>
<li><p>look at how much the predictions vary (this variation is a proxy for <strong>uncertainty</strong>).</p></li>
</ul>
<p>So the Dropout transformation reuses the usual Dropout mechanism, but with a <strong>different goal</strong>:</p>
<ul class="simple">
<li><p>normal Dropout: better training, less overfitting, Dropout OFF in eval mode;</p></li>
<li><p>Dropout transformation: keep Dropout ON in eval mode to get a distribution of outputs and estimate how
confident the model is.</p></li>
</ul>
</section>
<section id="short-sidebyside-comparison">
<h3>1.3 Short side‚Äëby‚Äëside comparison<a class="headerlink" href="#short-sidebyside-comparison" title="Link to this heading">¬∂</a></h3>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Normal Dropout (PyTorch)</p></th>
<th class="head"><p>Dropout Transformation (probly)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Where it appears in code</p></td>
<td><p>You explicitly add <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code> layers</p></td>
<td><p>Transformation walks the model and inserts Dropout</p></td>
</tr>
<tr class="row-odd"><td><p>When Dropout is active</p></td>
<td><p>Only in <code class="docutils literal notranslate"><span class="pre">model.train()</span></code></p></td>
<td><p>Also (and intentionally) in <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Main purpose</p></td>
<td><p>Reduce overfitting / improve generalisation</p></td>
<td><p>Make predictions uncertainty‚Äëaware</p></td>
</tr>
<tr class="row-odd"><td><p>Output behaviour in eval</p></td>
<td><p>Deterministic (same input ‚Üí same output)</p></td>
<td><p>Stochastic (same input ‚Üí slightly different outputs)</p></td>
</tr>
<tr class="row-even"><td><p>How we use the randomness</p></td>
<td><p>We ignore it at inference</p></td>
<td><p>We <em>use</em> it to measure spread / uncertainty</p></td>
</tr>
</tbody>
</table>
</div>
<p>The rest of this notebook now assumes this picture: <strong>‚Äúnormal‚Äù Dropout is a training regulariser, the
Dropout transformation turns the same mechanism into a tool for estimating uncertainty.</strong></p>
</section>
</section>
<section id="dropout-quickstart-pytorch">
<h2>2. Dropout Quickstart (PyTorch)<a class="headerlink" href="#dropout-quickstart-pytorch" title="Link to this heading">¬∂</a></h2>
<p>Below: build a small MLP, apply <code class="docutils literal notranslate"><span class="pre">dropout(model,</span> <span class="pre">p)</span></code>, and inspect the modified architecture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you&#39;re running inside the repo&#39;s environment, these imports should work directly.</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">probly.transformation</span><span class="w"> </span><span class="kn">import</span> <span class="n">dropout</span>


<span class="k">def</span><span class="w"> </span><span class="nf">build_mlp</span><span class="p">(</span><span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">hidden</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="c1"># A sequential model that ends on a Linear</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span>
    <span class="p">)</span>


<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># dropout probability</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">build_mlp</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original model:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="n">model_do</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">With Dropout transformation (p=</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model_do</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original model:
 Sequential(
  (0): Linear(in_features=10, out_features=32, bias=True)
  (1): ReLU()
  (2): Linear(in_features=32, out_features=32, bias=True)
  (3): ReLU()
  (4): Linear(in_features=32, out_features=1, bias=True)
)

With Dropout transformation (p=0.20):
 Sequential(
  (0): Linear(in_features=10, out_features=32, bias=True)
  (1): ReLU()
  (2_0): Dropout(p=0.2, inplace=False)
  (2_1): Linear(in_features=32, out_features=32, bias=True)
  (3): ReLU()
  (4_0): Dropout(p=0.2, inplace=False)
  (4_1): Linear(in_features=32, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<section id="notes-on-the-structure">
<h3>Notes on the structure<a class="headerlink" href="#notes-on-the-structure" title="Link to this heading">¬∂</a></h3>
<ul class="simple">
<li><p>Expect a Dropout layer <strong>before</strong> each intermediate <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>.</p></li>
<li><p>If the last layer is a linear output head, the transform usually <strong>does not</strong> add a Dropout layer in front of it, preserving your final mapping.</p></li>
</ul>
</section>
</section>
<section id="uncertainty-via-monte-carlo-mc-dropout">
<h2>3. Uncertainty via Monte Carlo (MC) Dropout<a class="headerlink" href="#uncertainty-via-monte-carlo-mc-dropout" title="Link to this heading">¬∂</a></h2>
<p>To obtain predictive <em>uncertainty</em>, we run multiple stochastic forward passes with Dropout <strong>active</strong> and compute the mean and variance of predictions.</p>
<blockquote>
<div><p><strong>Important:</strong> In PyTorch, Dropout is active in <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> mode. For MC Dropout at inference, we intentionally call <code class="docutils literal notranslate"><span class="pre">train()</span></code> while disabling gradients.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Toy regression data</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">true_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">true_w</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># (Re)build and transform the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">build_mlp</span><span class="p">(</span><span class="n">in_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_do</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Simple training loop (few steps just for illustration)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_do</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">model_do</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model_do</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="c1"># MC dropout prediction function</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mc_predict</span><span class="p">(</span>
    <span class="n">model_with_dropout</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">model_with_dropout</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># activate dropout</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_with_dropout</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">stacked</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [n_samples, N, out_dim]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">stacked</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">stacked</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span>


<span class="n">mean_pred</span><span class="p">,</span> <span class="n">var_pred</span> <span class="o">=</span> <span class="n">mc_predict</span><span class="p">(</span><span class="n">model_do</span><span class="p">,</span> <span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictive mean (first 5):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">mean_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Predictive variance (first 5):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">var_pred</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictive mean (first 5):
 tensor([-1.0892,  3.3118,  1.8753, -1.4207, -0.6193])

Predictive variance (first 5):
 tensor([0.0295, 0.1808, 0.0822, 0.0377, 0.0293])
</pre></div>
</div>
</div>
</div>
</section>
<section id="good-practices">
<h2>4. Good practices<a class="headerlink" href="#good-practices" title="Link to this heading">¬∂</a></h2>
<ul class="simple">
<li><p>Tune <code class="docutils literal notranslate"><span class="pre">p</span></code> (e.g., 0.1‚Äì0.5) based on validation performance.</p></li>
<li><p>Use a reasonable number of MC samples <code class="docutils literal notranslate"><span class="pre">T</span></code> (e.g., 20‚Äì200). Larger <code class="docutils literal notranslate"><span class="pre">T</span></code> ‚Üí smoother uncertainty estimates, but slower.</p></li>
<li><p>Keep your <strong>final layer behavior</strong> in mind when interpreting where Dropout is inserted.</p></li>
</ul>
</section>
<section id="common-errors">
<h2>5. Common errors<a class="headerlink" href="#common-errors" title="Link to this heading">¬∂</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">p</span> <span class="pre">must</span> <span class="pre">be</span> <span class="pre">between</span> <span class="pre">0</span> <span class="pre">and</span> <span class="pre">1</span></code> ‚Äî ensure <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">‚â§</span> <span class="pre">p</span> <span class="pre">‚â§</span> <span class="pre">1</span></code>.</p></li>
<li><p>Seeing no Dropout layers? Confirm your model actually contains <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> modules where you expect them.</p></li>
</ul>
</section>
<section id="next-steps">
<h2>6. Next steps<a class="headerlink" href="#next-steps" title="Link to this heading">¬∂</a></h2>
<ul class="simple">
<li><p>Try other architectures (e.g., with Conv blocks feeding into Linear heads).</p></li>
<li><p>Compare models <strong>with vs. without</strong> the transformation using the same training loop.</p></li>
</ul>
</section>
<section id="part-a-summary">
<h2>7. Part A Summary<a class="headerlink" href="#part-a-summary" title="Link to this heading">¬∂</a></h2>
<p>In Part A, we explored how Dropout works in deep learning and how the Dropout Transformation extends that idea in <code class="docutils literal notranslate"><span class="pre">probly</span></code>. Normal Dropout is a regularization technique used during training to reduce overfitting by randomly deactivating neurons, forcing the model to learn more general patterns. However, it is disabled during inference, making the model‚Äôs predictions deterministic. The Dropout Transformation, in contrast, keeps dropout active during inference, allowing the model to produce slightly different outputs for the same input. This variability reveals how confident or uncertain the model is about its predictions. In short, Part A explained the conceptual shift from using dropout purely for training robustness to using it as a tool for estimating predictive uncertainty.</p>
</section>
<hr class="docutils" />
<section id="part-b-applied-mc-dropout">
<h2>Part B ‚Äî Applied MC-Dropout<a class="headerlink" href="#part-b-applied-mc-dropout" title="Link to this heading">¬∂</a></h2>
</section>
<section id="dropout-transformation-part-b">
<h2>Dropout Transformation (Part B)<a class="headerlink" href="#dropout-transformation-part-b" title="Link to this heading">¬∂</a></h2>
<p><em>Date:</em> 2025-11-03
<strong>Audience:</strong> New probly users ¬∑ <strong>Framework:</strong> PyTorch ¬∑ <strong>Author:</strong> Nidhi Jain and Julia Goihman</p>
<p>In <strong>Part A</strong>, we learned what the <strong>Dropout transformation</strong> in <code class="docutils literal notranslate"><span class="pre">probly</span></code> does and how it modifies a model‚Äôs structure.
In this <strong>Part B</strong>, we will <em>apply</em> that transformation to make a model uncertainty-aware, run several stochastic predictions, and visualize the variability in outputs.</p>
</section>
<hr class="docutils" />
<section id="part-b">
<h2>Part B<a class="headerlink" href="#part-b" title="Link to this heading">¬∂</a></h2>
</section>
<section id="setup-and-base-model">
<h2>1. Setup and base model<a class="headerlink" href="#setup-and-base-model" title="Link to this heading">¬∂</a></h2>
<p>We start from the same idea as before ‚Äî a small fully-connected network ‚Äî but this time we will focus on the <em>inference behaviour</em> under MC-Dropout.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">probly.transformation</span><span class="w"> </span><span class="kn">import</span> <span class="n">dropout</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">TinyNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A tiny neural network for demonstration.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TinyNet.</span>

<span class="sd">        Args:</span>
<span class="sd">            p: Dropout probability</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input tensor</span>
<span class="sd">        Returns:</span>
<span class="sd">            Output logits</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">do1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">do2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="c1"># dummy input</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">TinyNet</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="apply-the-dropout-transformation">
<h2>2. Apply the Dropout transformation<a class="headerlink" href="#apply-the-dropout-transformation" title="Link to this heading">¬∂</a></h2>
<p>We now transform the base model with <code class="docutils literal notranslate"><span class="pre">dropout()</span></code> so that Dropout stays <em>active</em> during inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mc_model</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>
<span class="n">mc_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># MC-Dropout active even in eval mode</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TinyNet(
  (fc1): Linear(in_features=16, out_features=32, bias=True)
  (do1): Dropout(p=0.3, inplace=False)
  (fc2): Sequential(
    (0): Dropout(p=0.25, inplace=False)
    (1): Linear(in_features=32, out_features=8, bias=True)
  )
  (do2): Dropout(p=0.3, inplace=False)
  (out): Sequential(
    (0): Dropout(p=0.25, inplace=False)
    (1): Linear(in_features=8, out_features=3, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-inference-repeated-forward-passes">
<h2>3. Monte-Carlo inference: repeated forward passes<a class="headerlink" href="#monte-carlo-inference-repeated-forward-passes" title="Link to this heading">¬∂</a></h2>
<p>We feed the same input through the model multiple times and collect the stochastic outputs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_passes</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">logits_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_passes</span><span class="p">):</span>
        <span class="n">logits_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mc_model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">logits_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [num_passes, 3]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># convert to probabilities</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="quantify-uncertainty">
<h2>4. Quantify uncertainty<a class="headerlink" href="#quantify-uncertainty" title="Link to this heading">¬∂</a></h2>
<p>Compute the mean and standard deviation across all passes ‚Äî these capture the central tendency and spread (uncertainty).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std_probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">pred_class</span> <span class="o">=</span> <span class="n">mean_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">predictive_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">mean_probs</span> <span class="o">*</span> <span class="p">(</span><span class="n">mean_probs</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">1e-12</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean probabilities:&quot;</span><span class="p">,</span> <span class="n">mean_probs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std probabilities:&quot;</span><span class="p">,</span> <span class="n">std_probs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted class:&quot;</span><span class="p">,</span> <span class="n">pred_class</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictive entropy:&quot;</span><span class="p">,</span> <span class="n">predictive_entropy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean probabilities: tensor([0.3192, 0.4036, 0.2773])
Std probabilities: tensor([0., 0., 0.])
Predicted class: 1
Predictive entropy: 1.0863797664642334
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualization-inspecting-uncertainty-distributions">
<h2>5. Visualization ‚Äì Inspecting uncertainty distributions<a class="headerlink" href="#visualization-inspecting-uncertainty-distributions" title="Link to this heading">¬∂</a></h2>
<p>We visualize how much the predicted probabilities fluctuate across Monte-Carlo runs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">winning_class</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mean_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">probs</span><span class="p">[:,</span> <span class="n">winning_class</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Distribution of predicted probability - class </span><span class="si">{</span><span class="n">winning_class</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b0edf82eeb9feacd6b7849ec01e36ad6ceefc260c2eb917ce70a913564e590f2.png" src="../../_images/b0edf82eeb9feacd6b7849ec01e36ad6ceefc260c2eb917ce70a913564e590f2.png" />
</div>
</div>
<blockquote>
<div><p><strong>Interpretation:</strong>
‚Äì Narrow peak near 1.0 ‚Üí model confident
‚Äì Wide or multimodal distribution ‚Üí model uncertain</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">mean_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">C</span><span class="p">),</span> <span class="n">mean_probs</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std_probs</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightcoral&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">C</span><span class="p">),</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Class </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Mean ¬± Std of predicted probabilities per class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b2b8bbb3190f492ca15e077308759d3a8f1fa54b52410628b8dbd5b9ef118260.png" src="../../_images/b2b8bbb3190f492ca15e077308759d3a8f1fa54b52410628b8dbd5b9ef118260.png" />
</div>
</div>
<blockquote>
<div><p><strong>Interpretation:</strong>
‚Äì Taller bars ‚Üí more probable classes
‚Äì Longer error bars ‚Üí higher uncertainty</p>
</div></blockquote>
</section>
<section id="optional-turn-mc-dropout-off-deterministic-check">
<h2>6. Optional: turn MC-Dropout off (deterministic check)<a class="headerlink" href="#optional-turn-mc-dropout-off-deterministic-check" title="Link to this heading">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">no_mc</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>
<span class="n">no_mc</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">no_mc</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">no_mc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Deterministic without MC-Dropout:&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">no_mc</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">enable_at_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">no_mc</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

<span class="ne">TypeError</span>: dropout() got an unexpected keyword argument &#39;enable_at_eval&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h2>7. Summary<a class="headerlink" href="#summary" title="Link to this heading">¬∂</a></h2>
<p>In <strong>Part B</strong>, we applied the <code class="docutils literal notranslate"><span class="pre">dropout()</span></code> transformation from <code class="docutils literal notranslate"><span class="pre">probly</span></code> to make a PyTorch model uncertainty-aware.
By running multiple stochastic forward passes on the same input, we observed small variations in the output.
This variability represents <strong>model uncertainty</strong>, because dropout remains active during inference.
We then computed the <strong>mean</strong>, <strong>standard deviation</strong>, and <strong>predictive entropy</strong> of the outputs, and visualized them using histograms and error-bar plots to understand how confident the model was in its predictions.</p>
</section>
<hr class="docutils" />
<section id="final-summary-dropout-transformation-tutorial">
<h2>Final Summary ‚Äî Dropout Transformation Tutorial<a class="headerlink" href="#final-summary-dropout-transformation-tutorial" title="Link to this heading">¬∂</a></h2>
<hr class="docutils" />
<p>This tutorial showed how the concept of <strong>Dropout</strong> can evolve from a simple regularization technique into a foundation for <strong>uncertainty-aware deep learning</strong>.
We began by understanding how dropout reduces overfitting by randomly turning off neurons during training, helping models learn more general patterns.
From there, we extended this idea to the <strong>Dropout Transformation</strong> in <code class="docutils literal notranslate"><span class="pre">probly</span></code>, where dropout remains active during inference.
By running multiple stochastic forward passes, the model reveals not only its predictions but also how confident it is about them.
Through this process, we transformed dropout from a tool that improves generalization into one that also provides valuable insight into a model‚Äôs <strong>confidence and reliability</strong>, bridging the gap between stable learning and interpretable uncertainty.</p>
<hr class="docutils" />
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="ensemble_transformation.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Ensemble Transformation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="dropconnect_transformation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Dropconnect Transformation</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, probly team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Dropout Transformation</a><ul>
<li><a class="reference internal" href="#part-a-introduction-to-dropout-and-the-dropout-transformation">Part A ‚Äî Introduction to Dropout and the Dropout Transformation</a></li>
<li><a class="reference internal" href="#concept-what-is-dropout-normal-vs-dropout-transformation">1. Concept: What is Dropout (normal) vs Dropout Transformation?</a><ul>
<li><a class="reference internal" href="#normal-dropout-standard-pytorch-dropout-layer">1.1 Normal Dropout (standard PyTorch Dropout layer)</a></li>
<li><a class="reference internal" href="#dropout-transformation-probly">1.2 Dropout Transformation (probly)</a></li>
<li><a class="reference internal" href="#short-sidebyside-comparison">1.3 Short side‚Äëby‚Äëside comparison</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dropout-quickstart-pytorch">2. Dropout Quickstart (PyTorch)</a><ul>
<li><a class="reference internal" href="#notes-on-the-structure">Notes on the structure</a></li>
</ul>
</li>
<li><a class="reference internal" href="#uncertainty-via-monte-carlo-mc-dropout">3. Uncertainty via Monte Carlo (MC) Dropout</a></li>
<li><a class="reference internal" href="#good-practices">4. Good practices</a></li>
<li><a class="reference internal" href="#common-errors">5. Common errors</a></li>
<li><a class="reference internal" href="#next-steps">6. Next steps</a></li>
<li><a class="reference internal" href="#part-a-summary">7. Part A Summary</a></li>
<li><a class="reference internal" href="#part-b-applied-mc-dropout">Part B ‚Äî Applied MC-Dropout</a></li>
<li><a class="reference internal" href="#dropout-transformation-part-b">Dropout Transformation (Part B)</a></li>
<li><a class="reference internal" href="#part-b">Part B</a></li>
<li><a class="reference internal" href="#setup-and-base-model">1. Setup and base model</a></li>
<li><a class="reference internal" href="#apply-the-dropout-transformation">2. Apply the Dropout transformation</a></li>
<li><a class="reference internal" href="#monte-carlo-inference-repeated-forward-passes">3. Monte-Carlo inference: repeated forward passes</a></li>
<li><a class="reference internal" href="#quantify-uncertainty">4. Quantify uncertainty</a></li>
<li><a class="reference internal" href="#visualization-inspecting-uncertainty-distributions">5. Visualization ‚Äì Inspecting uncertainty distributions</a></li>
<li><a class="reference internal" href="#optional-turn-mc-dropout-off-deterministic-check">6. Optional: turn MC-Dropout off (deterministic check)</a></li>
<li><a class="reference internal" href="#summary">7. Summary</a></li>
<li><a class="reference internal" href="#final-summary-dropout-transformation-tutorial">Final Summary ‚Äî Dropout Transformation Tutorial</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../_static/documentation_options.js?v=4621528c"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=ccdb6887"></script>
    </body>
</html>