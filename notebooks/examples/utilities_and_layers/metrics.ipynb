{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18977d47",
   "metadata": {},
   "source": [
    "# `probly` Tutorial â€” Evaluation Metrics\n",
    "\n",
    "This notebook provides a practical introduction to the evaluation metrics available in `probly`. While standard accuracy is important, it doesn't tell the whole story about a model's performance, especially for probabilistic models.\n",
    "\n",
    "We will cover three key types of metrics:\n",
    "\n",
    "- **Proper Scoring Rules:** Metrics like Negative Log-Likelihood (NLL) that evaluate the quality of the entire predicted probability distribution.\n",
    "- **Calibration Metrics:** Metrics like Expected Calibration Error (ECE) that measure how well a model's predicted confidence aligns with its actual accuracy.\n",
    "- **Sharpness Metrics:** Metrics like efficiency and coverage that evaluate the precision and reliability of set-valued (uncertain) predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Proper Scoring Rules\n",
    "\n",
    "Proper scoring rules are loss functions that evaluate the quality of a predictive distribution. They don't just care about whether the top-1 prediction is correct; they penalize a model for being confidently wrong and reward it for being accurately uncertain.\n",
    "\n",
    "The most common proper scoring rule is the **Negative Log-Likelihood (NLL)**, also known as **Log Loss**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11881217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss (Calibrated): 0.2231\n",
      "Log Loss (Overconfident): 4.6052\n",
      "\n",
      "Brier Score (Calibrated): 0.0600\n",
      "Brier Score (Overconfident): 1.9406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probly.evaluation.metrics import brier_score, log_loss\n",
    "\n",
    "# Imagine a 3-class problem\n",
    "# A well-calibrated, correct prediction\n",
    "calibrated_probs = np.array([[0.8, 0.1, 0.1]])\n",
    "targets = np.array([0])\n",
    "\n",
    "# An overconfident, wrong prediction\n",
    "overconfident_probs = np.array([[0.01, 0.98, 0.01]])\n",
    "\n",
    "print(f\"Log Loss (Calibrated): {log_loss(calibrated_probs, targets):.4f}\")\n",
    "print(f\"Log Loss (Overconfident): {log_loss(overconfident_probs, targets):.4f}\")\n",
    "\n",
    "print(f\"\\nBrier Score (Calibrated): {brier_score(calibrated_probs, targets):.4f}\")\n",
    "print(f\"Brier Score (Overconfident): {brier_score(overconfident_probs, targets):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b1a24",
   "metadata": {},
   "source": [
    "Notice how the Log Loss heavily penalizes the overconfident wrong prediction.\n",
    "The Brier Score is another proper scoring rule that measures the mean squared error between the predicted probabilities and the one-hot encoded true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cb140",
   "metadata": {},
   "source": [
    "## 2. Calibration Metrics\n",
    "\n",
    "A model is well-calibrated if its predicted probabilities reflect its true accuracy. For example, if a model makes 100 predictions with 80% confidence, we expect it to be correct on 80 of those predictions.\n",
    "\n",
    "The Expected Calibration Error (ECE) is the standard metric for measuring this. It groups predictions into bins based on their confidence scores and calculates the weighted average difference between the confidence and accuracy in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d923c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE (Perfectly Calibrated): 0.0000\n",
      "ECE (Overconfident): 0.1900\n"
     ]
    }
   ],
   "source": [
    "from probly.evaluation.metrics import expected_calibration_error\n",
    "\n",
    "# A perfectly calibrated model\n",
    "# Predicts with 80% confidence and is correct 80% of the time\n",
    "perfect_probs = np.array([[0.8, 0.1, 0.1]] * 10)\n",
    "perfect_labels = np.array([0] * 8 + [1] * 2)\n",
    "\n",
    "# An overconfident model\n",
    "# Predicts with 99% confidence but is only correct 80% of the time\n",
    "overconfident_probs = np.array([[0.99, 0.005, 0.005]] * 10)\n",
    "overconfident_labels = np.array([0] * 8 + [1] * 2)\n",
    "\n",
    "ece_perfect = expected_calibration_error(perfect_probs, perfect_labels, num_bins=5)\n",
    "ece_overconfident = expected_calibration_error(overconfident_probs, overconfident_labels, num_bins=5)\n",
    "\n",
    "print(f\"ECE (Perfectly Calibrated): {ece_perfect:.4f}\")\n",
    "print(f\"ECE (Overconfident): {ece_overconfident:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30caf3ec",
   "metadata": {},
   "source": [
    "As expected, the overconfident model has a much higher ECE.\n",
    "A lower ECE score indicates a more reliable and trustworthy model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8c147",
   "metadata": {},
   "source": [
    "## 3. Sharpness Metrics (for Set-Valued Predictions)\n",
    "\n",
    "For models that output a set of possible predictions (like from an ensemble or a credal set), we need metrics to evaluate the quality of that set.\n",
    "\n",
    "- Coverage: How often does the true label fall within the predicted set?\n",
    "\n",
    "- Efficiency: How sharp or precise is the predicted set? A smaller set is more efficient.\n",
    "\n",
    "There is a natural trade-off: a model can achieve 100% coverage by always predicting all possible classes, but this would be completely inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09473bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.0000\n",
      "Efficiency (Lower is better): 0.1167\n"
     ]
    }
   ],
   "source": [
    "from probly.evaluation.metrics import coverage, efficiency\n",
    "\n",
    "# Predictions from a 3-member ensemble for two data points\n",
    "# Each row is a sample, each column is a class\n",
    "ensemble_preds = np.array(\n",
    "    [\n",
    "        [[0.7, 0.2, 0.1], [0.6, 0.3, 0.1], [0.8, 0.1, 0.1]],  # Instance 1: High agreement (sharp)\n",
    "        [[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]],  # Instance 2: High disagreement (unsharp)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# For simplicity, assume one-hot encoded targets\n",
    "# Note: `coverage` can also accept integer labels\n",
    "targets = np.array([[1, 0, 0], [1, 0, 0]])\n",
    "\n",
    "cov = coverage(ensemble_preds, targets)\n",
    "eff = efficiency(ensemble_preds)\n",
    "\n",
    "print(f\"Coverage: {cov:.4f}\")\n",
    "print(f\"Efficiency (Lower is better): {1 - eff:.4f}\")  # Print 1-eff for intuitive sharpness"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
