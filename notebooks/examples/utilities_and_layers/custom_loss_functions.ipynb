{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62fb88d",
   "metadata": {},
   "source": [
    "# `probly` Tutorial â€” Custom Loss Functions\n",
    "\n",
    "This notebook provides a practical introduction to the specialized loss functions in `probly`. While standard losses like `nn.CrossEntropyLoss` are sufficient for deterministic models, probabilistic models often require custom loss functions to handle uncertainty.\n",
    "\n",
    "We will cover three key types of custom losses:\n",
    "-   **Negative Log-Likelihood (NLL) Losses:** Adaptations for probabilistic outputs.\n",
    "-   **Evidential Losses:** Specialized functions for models that learn \"evidence.\"\n",
    "-   **Calibration-Aware Losses:** Losses that directly optimize for model calibration.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Negative Log-Likelihood (NLL) Losses\n",
    "NLL losses are a foundational concept in training probabilistic models. Instead of just penalizing wrong predictions, they evaluate how well the entire predicted *distribution* explains the true target.\n",
    "\n",
    "### Example: The ELBO Loss for Bayesian Neural Networks\n",
    "A Bayesian Neural Network (BNN) requires a unique loss function that balances two goals:\n",
    "1.  **Fit the data:** Make accurate predictions (similar to a standard loss).\n",
    "2.  **Stay simple:** Keep the weight distributions close to a simple prior distribution.\n",
    "\n",
    "The **Evidence Lower Bound (ELBO)** loss achieves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898b8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ELBOLoss(nn.Module):\n",
    "    def __init__(self, kl_penalty: float = 1e-5) -> None:\n",
    "        \"\"\"Initialize the ELBO loss parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        self.kl_penalty = kl_penalty\n",
    "\n",
    "    def forward(self, inputs: Tensor, targets: Tensor, kl: Tensor) -> Tensor:\n",
    "        \"\"\"Compute the ELBO loss with KL regularization.\"\"\"\n",
    "        # 1. Standard Cross-Entropy\n",
    "        cross_entropy_loss = F.cross_entropy(inputs, targets)\n",
    "\n",
    "        # 2. KL Divergence Regularizer\n",
    "        kl_divergence = self.kl_penalty * kl\n",
    "\n",
    "        return cross_entropy_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964cf1",
   "metadata": {},
   "source": [
    "The ELBOLoss combines a standard cross-entropy loss with a KL Divergence term, which penalizes the model for having weight distributions that are too complex or far from the initial prior\n",
    "For more information on how Bayesian models work, see the [Bayesian Transformation](../bayesian_transformation.ipynb) tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2e474",
   "metadata": {},
   "source": [
    "## 2. Evidential Losses\n",
    "\n",
    "Evidential Deep Learning models do not output probabilities directly.  \n",
    "Instead, they output **evidence** for each class, which requires specialized loss functions.\n",
    "\n",
    "### Example: Evidential Losses for Classification and Regression\n",
    "\n",
    "The `probly` library provides custom loss functions for evidential learning, based on the original research papers:\n",
    "\n",
    "- **EvidentialLogLoss (Classification)**  \n",
    "  Adapts the standard log loss to work with evidence scores (`alpha`) instead of probabilities.\n",
    "\n",
    "- **EvidentialNIGNLLLoss (Regression)**  \n",
    "  A more complex negative log-likelihood (NLL) loss that handles the four parameters of an evidential regression model:  \n",
    "  `gamma`, `nu`, `alpha`, and `beta`.\n",
    "\n",
    "### Training an Evidential Model\n",
    "\n",
    "The training loop for an evidential model typically combines:\n",
    "\n",
    "1. An evidential NLL loss (classification or regression), and  \n",
    "2. A regularization term that encourages the model to remain uncertain on out-of-distribution data.\n",
    "The total loss is a weighted sum of these two components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073bbd0",
   "metadata": {},
   "source": [
    "For full implementations, see the [**Evidential Classification**](../train_evidential_classification.ipynb) and [**Evidential Regression**](../train_evidential_regression.ipynb) tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db127c1",
   "metadata": {},
   "source": [
    "## 3. Calibration-Aware Losses\n",
    "\n",
    "Sometimes, the most effective way to achieve good calibration is to include a calibration objective directly in the loss function.  \n",
    "This forces the model to optimize calibration as part of training.\n",
    "\n",
    "### Example: Label Relaxation\n",
    "\n",
    "**Label Relaxation** is a simple but effective technique for reducing over-confidence and improving model calibration.  \n",
    "Instead of using hard one-hot encoded labels (e.g., `[0, 0, 1]`), the labels are softened:\n",
    "\n",
    "- The true class is assigned a slightly lower value (e.g., `0.9`).\n",
    "- The remaining probability mass (e.g., `0.1`) is distributed across the other classes.\n",
    "\n",
    "This discourages the model from producing extreme, over-confident predictions.\n",
    "\n",
    "The `probly` library provides a direct implementation of this approach through the **`LabelRelaxationLoss`**.\n",
    "The `LabelRelaxationLoss` can be used as a drop-in replacement for standard losses like `nn.CrossEntropyLoss`, making it easy to integrate into existing training pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3431d4",
   "metadata": {},
   "source": [
    "By optimizing this \"softer\" objective, the model learns to produce better-calibrated probability estimates.\n",
    "\n",
    "For a full implementation, see the [**Label Relaxation Calibration**](../label_relaxation_calibration.ipynb) tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
