{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a7dddf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides a practical introduction to the core utility functions in `probly`.  \n",
    "These helpers are essential building blocks for training probabilistic models and quantifying uncertainty.\n",
    "\n",
    "We will focus on two main categories:\n",
    "\n",
    "- **Model traversal functions**, which inspect a model\u2019s architecture  \n",
    "- **Uncertainty quantification functions**, which compute meaningful uncertainty scores from model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2eb79d",
   "metadata": {},
   "source": [
    "## Key Utility Functions in `probly`\n",
    "\n",
    "### 1. `collect_kl_divergence` (for BNNs)\n",
    "\n",
    "**What it does:**  \n",
    "Automatically traverses a Bayesian Neural Network and sums the KL divergence from each Bayesian layer.\n",
    "\n",
    "**Why it\u2019s useful:**  \n",
    "This function is critical for computing the **ELBO loss** during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94811953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected the Total KL Divergence from the BNN.\n",
      "Total KL Divergence: 1637.1818\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "from probly.train.bayesian.torch import collect_kl_divergence\n",
    "from probly.transformation import bayesian\n",
    "\n",
    "\n",
    "# 1. Define a standard model\n",
    "def build_mlp() -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Linear(10, 50), nn.ReLU(), nn.Linear(50, 2))\n",
    "\n",
    "\n",
    "# 2. Apply the Bayesian transformation\n",
    "# This replaces nn.Linear layers with BayesLinear layers, each of which has a\n",
    "# .kl_divergence property.\n",
    "bnn_model = bayesian(build_mlp())\n",
    "\n",
    "# 3. Use the utility function to sum the KL divergence across all Bayesian layers\n",
    "total_kl = collect_kl_divergence(bnn_model)\n",
    "\n",
    "print(\"Successfully collected the Total KL Divergence from the BNN.\")\n",
    "print(f\"Total KL Divergence: {total_kl.item():.4f}\")\n",
    "\n",
    "# This `total_kl` value would then be passed to the ELBOLoss during a training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b241b",
   "metadata": {},
   "source": [
    "### 2. `total_entropy`, `conditional_entropy`, `mutual_information`\n",
    "\n",
    "**What they do:**  \n",
    "These functions take a set of predictions (for example, from an ensemble) and decompose predictive uncertainty.\n",
    "\n",
    "**Why they\u2019re useful:**  \n",
    "They allow you to separately measure:\n",
    "\n",
    "- **Aleatoric uncertainty** (inherent randomness in the data)  \n",
    "- **Epistemic uncertainty** (uncertainty due to limited model knowledge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b0751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Uncertainty (Entropy): 1.2208\n",
      "Aleatoric Uncertainty (Data Noise): 1.1804\n",
      "Epistemic Uncertainty (Model Ignorance): 0.0404\n",
      "\n",
      "Sum of parts: 1.2208\n",
      "Decomposition is correct: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probly.quantification.classification import conditional_entropy, mutual_information, total_entropy\n",
    "\n",
    "# 1. Create a dummy set of predictions for one input instance.\n",
    "# Shape: (n_instances, n_samples, n_classes) -> (1, 5, 3)\n",
    "# This simulates getting 5 different predictions from an ensemble for a 3-class problem.\n",
    "# These predictions show some disagreement, indicating uncertainty.\n",
    "prob_samples = np.array(\n",
    "    [\n",
    "        [\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.6, 0.3, 0.1],\n",
    "            [0.8, 0.1, 0.1],\n",
    "            [0.6, 0.2, 0.2],\n",
    "            [0.7, 0.1, 0.2],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Decompose the uncertainty\n",
    "# Total uncertainty in the average prediction\n",
    "total = total_entropy(prob_samples)\n",
    "\n",
    "# Aleatoric uncertainty: average uncertainty *within* each prediction\n",
    "aleatoric = conditional_entropy(prob_samples)\n",
    "\n",
    "# Epistemic uncertainty: how much the predictions disagree with each other\n",
    "epistemic = mutual_information(prob_samples)\n",
    "\n",
    "print(f\"Total Uncertainty (Entropy): {total.item():.4f}\")\n",
    "print(f\"Aleatoric Uncertainty (Data Noise): {aleatoric.item():.4f}\")\n",
    "print(f\"Epistemic Uncertainty (Model Ignorance): {epistemic.item():.4f}\")\n",
    "\n",
    "# 3. Verify the decomposition\n",
    "# Total uncertainty is approximately the sum of its two parts.\n",
    "print(f\"\\nSum of parts: {(aleatoric + epistemic).item():.4f}\")\n",
    "print(f\"Decomposition is correct: {np.allclose(total, aleatoric + epistemic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16430318",
   "metadata": {},
   "source": [
    "\n",
    "### 3. `evidential_uncertainty` (for Evidential Models)\n",
    "\n",
    "**What it does:**  \n",
    "Computes an uncertainty score directly from the **evidence vector** produced by an evidential model.\n",
    "\n",
    "**Why it\u2019s useful:**  \n",
    "It provides a fast, single-pass way to determine whether a model is uncertain about its prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041c13a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence vector (low confidence): [[0.1  0.2  0.15]]\n",
      "Resulting Uncertainty Score: 0.8696\n",
      "\n",
      "Evidence vector (high confidence): [[100.   2.   5.]]\n",
      "Resulting Uncertainty Score: 0.0273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probly.quantification.classification import evidential_uncertainty\n",
    "\n",
    "# 1. Simulate the output of an evidential model for two different inputs.\n",
    "# The output is a vector of \"evidence\", not probabilities.\n",
    "\n",
    "# Case 1: The model is UNCERTAIN (it found very little evidence for any class)\n",
    "low_evidence = np.array([[0.1, 0.2, 0.15]])\n",
    "\n",
    "# Case 2: The model is CONFIDENT (it found a lot of evidence for one class)\n",
    "high_evidence = np.array([[100.0, 2.0, 5.0]])\n",
    "\n",
    "# 2. Calculate the uncertainty score for each case\n",
    "uncertainty_low_confidence = evidential_uncertainty(low_evidence)\n",
    "uncertainty_high_confidence = evidential_uncertainty(high_evidence)\n",
    "\n",
    "print(f\"Evidence vector (low confidence): {low_evidence}\")\n",
    "print(f\"Resulting Uncertainty Score: {uncertainty_low_confidence.item():.4f}\\n\")\n",
    "\n",
    "print(f\"Evidence vector (high confidence): {high_evidence}\")\n",
    "print(f\"Resulting Uncertainty Score: {uncertainty_high_confidence.item():.4f}\")\n",
    "\n",
    "# The uncertainty score is much higher when the total evidence is low, as expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
