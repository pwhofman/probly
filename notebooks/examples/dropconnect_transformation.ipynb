{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd810d76",
   "metadata": {},
   "source": [
    "# `probly` Tutorial — Dropconnect Transformation \n",
    "\n",
    "This notebook is meant as a, practical introduction to the **Dropconnect transformation** in `probly`.\n",
    "The goal is not to be mathematically perfect, but to give you an intuition.\n",
    "\n",
    "We will slowly build up from the very basic idea of *normal* Dropout to the slightly more advanced idea of\n",
    "a **Dropout transformation that makes a model uncertainty‑aware**. After that, we look at a small PyTorch\n",
    "example and inspect how the transformation changes the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c83c7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Introduction to Dropconnect and the Dropconnect Transformation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24e17",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Dropconnect (normal) vs Dropconnect Transformation?\n",
    "\n",
    "To understand the DropConnect transformation, it's helpful to first compare it to the more common Dropout.\n",
    "### 1.1 Normal Dropout (Recap)\n",
    "\n",
    "Dropout is a regularization technique that works on activations. During training, it randomly sets the outputs of some neurons to zero.\n",
    " This prevents the network from relying too heavily on any single neuron.\n",
    "### 1.2 Normal DropConnect\n",
    "DropConnect is a similar regularization technique, but it works on weights. Instead of setting a neuron's entire output to zero, \n",
    "DropConnect randomly sets a fraction p of the individual weights within a layer to zero for each training step. \n",
    "You can imagine this as temporarily deleting connections between neurons.\n",
    "\n",
    "This is considered a more generalized form of Dropout. Like Dropout, its main purpose during normal training is to prevent overfitting\n",
    "and improve the model's robustness. At inference time `(model.eval())`, this randomness is disabled, and the model becomes deterministic.\n",
    "\n",
    "### 1.3 DropConnect Transformation (probly)\n",
    "\n",
    "The DropConnect transformation in `probly`takes this idea and uses it to make a model **uncertainty‑aware** at prediction time.\n",
    "\n",
    "The transformation does the following:\n",
    " \n",
    "- It walks through your PyTorch model and finds the relevant linear layers (e.g., `nn.Linear`).\n",
    "- It programmatically replaces each `nn.Linear`layer with a custom `DropConnectLinear` layer.\n",
    "\n",
    "- Crucially, this custom layer keeps the DropConnect mechanism **active during inference**.\n",
    "\n",
    "If we now feed the same input through the transformed model multiple times, we get a cloud of slightly different predictions. The variation in this cloud is a direct measure of the model's uncertainty.\n",
    "\n",
    "### 1.4 A Short side‑by‑side comparison\n",
    "\n",
    "| Aspect                       | DropConnect Transformation (probly)                    | Dropout Transformation (probly)                          |\n",
    "|------------------------------|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| What is dropped?             | Individual weights inside a layer                      | Entire activations (neuron outputs)                      |\n",
    "| How it modifies the model    | Replaces `nn.Linear` with `DropConnectLinear`          | Inserts `nn.Dropout layers` before `nn.Linear`           |\n",
    "| When it's active             | Intentionally in `model.eval()`                        | Intentionally in `model.eval()`                          |\n",
    "| Main purpose                 | Make predictions uncertainty‑aware                     | Make predictions uncertainty‑aware           |\n",
    "|Output behaviour in eval      | Stochastic (same input → slightly different outputs)   | Stochastic (same input → slightly different outputs)     |\n",
    "\n",
    "The rest of this notebook now assumes this picture: **“normal” Dropout is a training regulariser, the\n",
    "Dropout transformation turns the same mechanism into a tool for estimating uncertainty.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edbded",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `dropconnect(model, p)`, and inspect the modified architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8de24",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "With DropConnect transformation (p=0.20):\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): DropConnectLinear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): DropConnectLinear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Qimport torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import dropconnect\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 1) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "p = 0.2  # dropconnect probability\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "model_dc = dropconnect(model, p)\n",
    "print(f\"\\nWith DropConnect transformation (p={p:.2f}):\\n\", model_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95734b16",
   "metadata": {},
   "source": [
    "### Notes on the structure\n",
    "\n",
    "Notice that each `Linear` layer has been replaced by a `DropConnectLinear` layer.\n",
    "\n",
    "The overall architecture (`Sequential`, `ReLU`) remains the same, but the core linear modules are now uncertainty-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968b3a",
   "metadata": {},
   "source": [
    "## 3. Uncertainty via DropConnect\n",
    "\n",
    "To obtain predictive uncertainty, we run multiple stochastic forward passes (with DropConnect active) and compute the mean and variance of the predictions. The process is identical to MC-Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bac14a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive mean (first 5):\n",
      " tensor([-1.2788,  3.1010,  1.6500, -1.6051, -0.8758])\n",
      "\n",
      "Predictive variance (first 5):\n",
      " tensor([0.0370, 0.1376, 0.0453, 0.0445, 0.0266])\n"
     ]
    }
   ],
   "source": [
    "# Toy regression data\n",
    "torch.manual_seed(0)\n",
    "n = 128\n",
    "X = torch.randn(n, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(n, 1)\n",
    "\n",
    "# Build and transform the model\n",
    "model = build_mlp(in_dim=10, hidden=64, out_dim=1)\n",
    "model_dc = dropconnect(model, p=0.2)\n",
    "\n",
    "# Simple training loop\n",
    "opt = torch.optim.Adam(model_dc.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model_dc.train() # Activate DropConnect for training\n",
    "for _step in range(200):\n",
    "    opt.zero_grad()\n",
    "    pred = model_dc(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# MC prediction function\n",
    "@torch.no_grad()\n",
    "def mc_predict(\n",
    "    model_with_dropconnect: nn.Module,\n",
    "    inputs: torch.Tensor,\n",
    "    n_samples: int = 50,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Activate training mode to enable the stochasticity of DropConnect\n",
    "    model_with_dropconnect.train()\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        preds.append(model_with_dropconnect(inputs).detach())\n",
    "    stacked = torch.stack(preds, dim=0)  # [n_samples, N, out_dim]\n",
    "    mean = stacked.mean(dim=0)\n",
    "    var = stacked.var(dim=0, unbiased=False)\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "mean_pred, var_pred = mc_predict(model_dc, X[:5], n_samples=100)\n",
    "print(\"Predictive mean (first 5):\\n\", mean_pred.squeeze())\n",
    "print(\"\\nPredictive variance (first 5):\\n\", var_pred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b32fdf",
   "metadata": {},
   "source": [
    "## 4. Good practices\n",
    "\n",
    "- Tune the DropConnect probability `p` (e.g., 0.1–0.5) as a hyperparameter.\n",
    "- Use a reasonable number of Monte Carlo samples (e.g., 20–200). More samples give smoother estimates but are computationally slower.\n",
    "- Since DropConnect replaces layers, it's a good idea to confirm your model's performance doesn't degrade significantly after transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2412d",
   "metadata": {},
   "source": [
    "## 5. Common errors\n",
    "- `ValueError: p must be between 0 and 1` — ensure `0 ≤ p ≤ 1`.\n",
    "- Seeing no `DropConnectLinear` layers? Confirm your model contains `nn.Linear` modules that probly can detect and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de7dd8",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "\n",
    "- Explore how DropConnect behaves with different architectures.\n",
    "- Compare the uncertainty estimates from DropConnect with those from the Dropout transformation on the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b14bb",
   "metadata": {},
   "source": [
    "## 7. Part A Summary\n",
    "In Part A, we distinguished between Dropout and DropConnect and explored how the DropConnect Transformation works in `probly`. \n",
    "While standard DropConnect is a regularization technique that drops individual weights during training, the `probly` transformation adapts this mechanism for uncertainty estimation.\n",
    " It achieves this by replacing standard `nn.Linear ` layers with custom `DropConnectLinear` layers that remain stochastic even during inference. \n",
    " This allows us to generate a distribution of predictions for a single input, where the variance serves as a measure of model uncertainty. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddee3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B — Applied MC-Dropconnect\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b8a81",
   "metadata": {},
   "source": [
    "In **Part A** , we learned what the `DropConnect`transformation in `probly` does by replacing layers to make a model stochastic.\n",
    "In this **Part B** , we will apply that transformation, run several stochastic predictions, and visualize the resulting uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eadf720",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from probly.transformation import dropconnect\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    \"\"\"A tiny neural network for demonstration, built with standard layers.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(16, 32)\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.out = nn.Linear(8, 3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# dummy input\n",
    "x = torch.randn(1, 16)\n",
    "base_model = TinyNet().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6232d",
   "metadata": {},
   "source": [
    "## 2. Apply the DropConnect transformation\n",
    "\n",
    "We now transform the base model with `dropconnect()`. This will replace its `nn.Linear` layers with `DropConnectLinear`layers that stay active during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea1834",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "mc_model_dc = dropconnect(base_model, p=0.25)\n",
    "mc_model_dc.train()  # Activate stochasticity for MC passes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7f42b",
   "metadata": {},
   "source": [
    "## 3. Monte-Carlo inference: repeated forward passes\n",
    "We feed the same input through the model multiple times and collect the stochastic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f11179",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "num_passes = 100\n",
    "logits_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_passes):\n",
    "        logits_list.append(mc_model_dc(x))\n",
    "\n",
    "logits = torch.cat(logits_list, dim=0)  # [num_passes, 3]\n",
    "probs = torch.softmax(logits, dim=-1)  # convert to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11160a3",
   "metadata": {},
   "source": [
    "## 4. Quantify uncertainty\n",
    "\n",
    "Compute the mean and standard deviation across all passes — these capture the central tendency and spread (uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f6d19",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "from probly.quantification import classification as q\n",
    "\n",
    "# Reshape for probly's quantification functions: [n_instances, n_samples, n_classes]\n",
    "probs_reshaped = probs.unsqueeze(0)\n",
    "\n",
    "mean_probs = probs.mean(dim=0)\n",
    "std_probs = probs.std(dim=0, unbiased=False)\n",
    "\n",
    "pred_class = mean_probs.argmax().item()\n",
    "predictive_entropy = q.total_entropy(probs_reshaped).item()\n",
    "\n",
    "print(\"Mean probabilities:\", mean_probs)\n",
    "print(\"Std probabilities:\", std_probs)\n",
    "print(\"Predicted class:\", pred_class)\n",
    "print(f\"Predictive entropy: {predictive_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1d945",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Summary — Dropout Transformation Tutorial\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial showed how the concept of `DropConnect` can be used as a powerful tool for uncertainty-aware deep learning with `probly`.\n",
    "We began by understanding that `DropConnect` regularizes a model by randomly dropping individual weights, a more generalized approach than Dropout's method of dropping entire neuron activations.\n",
    "We then saw how the `DropConnect` Transformation in `probly` operationalizes this for uncertainty: it replaces standard linear layers with custom, stochastic versions that remain active during inference.\n",
    "By running multiple forward passes, the model reveals not only its predictions but also its confidence, which we quantified and visualized.\n",
    "Through this process, we transformed `DropConnect` from a training regularizer into a practical tool for providing valuable insight into a model’s confidence and reliability.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
