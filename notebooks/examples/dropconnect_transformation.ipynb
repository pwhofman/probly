{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acba8f8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# probly Tutorial — Dropconnect Transformation \n",
    "\n",
    "This notebook is meant as a, practical introduction to the **Dropconnect transformation** in `probly`.\n",
    "The goal is not to be mathematically perfect, but to give you an intuition.\n",
    "\n",
    "We will slowly build up from the very basic idea of *normal* Dropout to the slightly more advanced idea of\n",
    "a **Dropout transformation that makes a model uncertainty‑aware**. After that, we look at a small PyTorch\n",
    "example and inspect how the transformation changes the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c83c7",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction to Dropconnect and the Dropconnect Transformation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24e17",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Dropconnect (normal) vs Dropconnect Transformation?\n",
    "\n",
    "To understand the DropConnect transformation, it's helpful to first compare it to the more common Dropout.\n",
    "### 1.1 Normal Dropout (Recap)\n",
    "\n",
    "Dropout is a regularization technique that works on activations. During training, it randomly sets the outputs of some neurons to zero.\n",
    " This prevents the network from relying too heavily on any single neuron.\n",
    "### 1.2 Normal DropConnect\n",
    "DropConnect is a similar regularization technique, but it works on weights. Instead of setting a neuron's entire output to zero, \n",
    "DropConnect randomly sets a fraction p of the individual weights within a layer to zero for each training step. \n",
    "You can imagine this as temporarily deleting connections between neurons.\n",
    "\n",
    "This is considered a more generalized form of Dropout. Like Dropout, its main purpose during normal training is to prevent overfitting\n",
    "and improve the model's robustness. At inference time `(model.eval())`, this randomness is disabled, and the model becomes deterministic.\n",
    "\n",
    "### 1.3 DropConnect Transformation (probly)\n",
    "\n",
    "The DropConnect transformation in `probly`takes this idea and uses it to make a model **uncertainty‑aware** at prediction time.\n",
    "\n",
    "The transformation does the following:\n",
    " \n",
    "- It walks through your PyTorch model and finds the relevant linear layers (e.g., `nn.Linear`).\n",
    "- It programmatically replaces each `nn.Linear`layer with a custom `DropConnectLinear` layer.\n",
    "\n",
    "- Crucially, this custom layer keeps the DropConnect mechanism **active during inference**.\n",
    "\n",
    "If we now feed the same input through the transformed model multiple times, we get a cloud of slightly different predictions. The variation in this cloud is a direct measure of the model's uncertainty.\n",
    "\n",
    "### 1.4 A Short side‑by‑side comparison\n",
    "\n",
    "| Aspect                       | DropConnect Transformation (probly)                    | Dropout Transformation (probly)                          |\n",
    "|------------------------------|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| What is dropped?             | Individual weights inside a layer                      | Entire activations (neuron outputs)                      |\n",
    "| How it modifies the model    | Replaces `nn.Linear` with `DropConnectLinear`          | Inserts `nn.Dropout layers` before `nn.Linear`           |\n",
    "| When it's active             | Intentionally in `model.eval()`                        | Intentionally in `model.eval()`                          |\n",
    "| Main purpose                 | Make predictions uncertainty‑aware                     | Make predictions uncertainty‑aware           |\n",
    "|Output behaviour in eval      | Stochastic (same input → slightly different outputs)   | Stochastic (same input → slightly different outputs)     |\n",
    "\n",
    "The rest of this notebook now assumes this picture: **“normal” Dropout is a training regulariser, the\n",
    "Dropout transformation turns the same mechanism into a tool for estimating uncertainty.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edbded",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `dropconnect(model, p)`, and inspect the modified architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa8de24",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "With DropConnect transformation (p=0.20):\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): DropConnectLinear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): DropConnectLinear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import dropconnect\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 1) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "p = 0.2  # dropconnect probability\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "model_dc = dropconnect(model, p)\n",
    "print(f\"\\nWith DropConnect transformation (p={p:.2f}):\\n\", model_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95734b16",
   "metadata": {},
   "source": [
    "### Notes on the structure\n",
    "\n",
    "Notice that each `Linear` layer has been replaced by a `DropConnectLinear` layer.\n",
    "\n",
    "The overall architecture (`Sequential`, `ReLU`) remains the same, but the core linear modules are now uncertainty-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968b3a",
   "metadata": {},
   "source": [
    "## 3. Uncertainty via DropConnect\n",
    "\n",
    "To obtain predictive uncertainty, we run multiple stochastic forward passes (with DropConnect active) and compute the mean and variance of the predictions. The process is identical to MC-Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bac14a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Toy regression data\n",
    "torch.manual_seed(0)\n",
    "n = 128\n",
    "X = torch.randn(n, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(n, 1)\n",
    "\n",
    "# Build and transform the model\n",
    "model = build_mlp(in_dim=10, hidden=64, out_dim=1)\n",
    "model_dc = dropconnect(model, p=0.2)\n",
    "\n",
    "# Simple training loop\n",
    "opt = torch.optim.Adam(model_dc.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model_dc.train() # Activate DropConnect for training\n",
    "for _step in range(200):\n",
    "    opt.zero_grad()\n",
    "    pred = model_dc(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# MC prediction function\n",
    "@torch.no_grad()\n",
    "def mc_predict(\n",
    "    model_with_dropconnect: nn.Module,\n",
    "    inputs: torch.Tensor,\n",
    "    n_samples: int = 50,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Activate training mode to enable the stochasticity of DropConnect\n",
    "    model_with_dropconnect.train()\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        preds.append(model_with_dropconnect(inputs).detach())\n",
    "    stacked = torch.stack(preds, dim=0)  # [n_samples, N, out_dim]\n",
    "    mean = stacked.mean(dim=0)\n",
    "    var = stacked.var(dim=0, unbiased=False)\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "mean_pred, var_pred = mc_predict(model_dc, X[:5], n_samples=100)\n",
    "print(\"Predictive mean (first 5):\\n\", mean_pred.squeeze())\n",
    "print(\"\\nPredictive variance (first 5):\\n\", var_pred.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
