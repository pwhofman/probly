{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acba8f8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# probly Tutorial — Dropconnect Transformation \n",
    "\n",
    "This notebook is meant as a, practical introduction to the **Dropconnect transformation** in `probly`.\n",
    "The goal is not to be mathematically perfect, but to give you an intuition.\n",
    "\n",
    "We will slowly build up from the very basic idea of *normal* Dropout to the slightly more advanced idea of\n",
    "a **Dropout transformation that makes a model uncertainty‑aware**. After that, we look at a small PyTorch\n",
    "example and inspect how the transformation changes the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c83c7",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction to Dropconnect and the Dropconnect Transformation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad24e17",
   "metadata": {},
   "source": [
    "## 1. Concept: What is Dropconnect (normal) vs Dropconnect Transformation?\n",
    "\n",
    "To understand the DropConnect transformation, it's helpful to first compare it to the more common Dropout.\n",
    "### 1.1 Normal Dropout (Recap)\n",
    "\n",
    "Dropout is a regularization technique that works on activations. During training, it randomly sets the outputs of some neurons to zero.\n",
    " This prevents the network from relying too heavily on any single neuron.\n",
    "### 1.2 Normal DropConnect\n",
    "DropConnect is a similar regularization technique, but it works on weights. Instead of setting a neuron's entire output to zero, \n",
    "DropConnect randomly sets a fraction p of the individual weights within a layer to zero for each training step. \n",
    "You can imagine this as temporarily deleting connections between neurons.\n",
    "\n",
    "This is considered a more generalized form of Dropout. Like Dropout, its main purpose during normal training is to prevent overfitting\n",
    "and improve the model's robustness. At inference time `(model.eval())`, this randomness is disabled, and the model becomes deterministic.\n",
    "\n",
    "### 1.3 DropConnect Transformation (probly)\n",
    "\n",
    "The DropConnect transformation in `probly`takes this idea and uses it to make a model **uncertainty‑aware** at prediction time.\n",
    "\n",
    "The transformation does the following:\n",
    " \n",
    "- It walks through your PyTorch model and finds the relevant linear layers (e.g., `nn.Linear`).\n",
    "- It programmatically replaces each `nn.Linear`layer with a custom `DropConnectLinear` layer.\n",
    "\n",
    "- Crucially, this custom layer keeps the DropConnect mechanism **active during inference**.\n",
    "\n",
    "If we now feed the same input through the transformed model multiple times, we get a cloud of slightly different predictions. The variation in this cloud is a direct measure of the model's uncertainty.\n",
    "\n",
    "### 1.4 A Short side‑by‑side comparison\n",
    "\n",
    "| Aspect                       | DropConnect Transformation (probly)                    | Dropout Transformation (probly)                          |\n",
    "|------------------------------|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| What is dropped?             | Individual weights inside a layer                      | Entire activations (neuron outputs)                      |\n",
    "| How it modifies the model    | Replaces `nn.Linear` with `DropConnectLinear`          | Inserts `nn.Dropout layers` before `nn.Linear`           |\n",
    "| When it's active             | Intentionally in `model.eval()`                        | Intentionally in `model.eval()`                          |\n",
    "| Main purpose                 | Make predictions uncertainty‑aware                     | Make predictions uncertainty‑aware           |\n",
    "|Output behaviour in eval      | Stochastic (same input → slightly different outputs)   | Stochastic (same input → slightly different outputs)     |\n",
    "\n",
    "The rest of this notebook now assumes this picture: **“normal” Dropout is a training regulariser, the\n",
    "Dropout transformation turns the same mechanism into a tool for estimating uncertainty.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edbded",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `dropconnect(model, p)`, and inspect the modified architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968b3a",
   "metadata": {},
   "source": [
    "## 3. Uncertainty via DropConnect\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
