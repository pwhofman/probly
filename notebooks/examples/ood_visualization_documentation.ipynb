{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Evaluation Utilities — Documentation\n",
    "\n",
    "- This notebook provides utilities for evaluating Out-of-Distribution (OOD) detection\n",
    "performance using common metrics such as AUROC and AUPR.\n",
    "- The evaluation is based on\n",
    "confidence scores assigned by a model to in-distribution (ID) and out-of-distribution\n",
    "(OOD) samples.\n",
    "- Lower scores indicate higher likelihood of being OOD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OodEvaluationResult\n",
    "\n",
    "A dataclass that stores all metrics and diagnostic arrays generated during OOD evaluation.\n",
    "\n",
    "**Fields**\n",
    "\n",
    "- **auroc (float)**  \n",
    "  Area Under the ROC Curve. Measures how well ID and OOD samples are separable.\n",
    "\n",
    "- **fpr (np.ndarray)**  \n",
    "  False Positive Rates for different classification thresholds.\n",
    "\n",
    "- **tpr (np.ndarray)**  \n",
    "  True Positive Rates for the same thresholds.\n",
    "\n",
    "- **labels (np.ndarray)**  \n",
    "  Ground-truth labels used for evaluation:  \n",
    "  0 = ID sample, 1 = OOD sample.\n",
    "\n",
    "- **preds (np.ndarray)**  \n",
    "  Anomaly scores computed as 1 - confidence.  \n",
    "  Higher values correspond to higher OOD likelihood.\n",
    "\n",
    "- **aupr (float)**  \n",
    "  Area Under the Precision–Recall Curve.\n",
    "\n",
    "- **precision (np.ndarray)**  \n",
    "  Precision values for different thresholds.\n",
    "\n",
    "- **recall (np.ndarray)**  \n",
    "  Recall values for the same thresholds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_ood_performance(id_scores, ood_scores)\n",
    "\n",
    "Computes OOD detection metrics given confidence scores for ID and OOD samples.\n",
    "\n",
    "#### Parameters\n",
    "- **id_scores (np.ndarray)**  \n",
    "  Confidence scores for in-distribution samples. Expected in [0, 1].\n",
    "\n",
    "- **ood_scores (np.ndarray)**  \n",
    "  Confidence scores for out-of-distribution samples. Expected in [0, 1].\n",
    "\n",
    "#### Core Logic\n",
    "The function assumes the convention:\n",
    "\n",
    "**Low confidence → more likely OOD**\n",
    "\n",
    "To transform scores into anomaly predictions, it computes:  \n",
    "preds = 1.0 - score.\n",
    "\n",
    "This ensures that **higher values indicate higher OOD likelihood**, which is required\n",
    "by metrics such as ROC and PR curves.\n",
    "\n",
    "#### Steps Performed\n",
    "\n",
    "1. **Clip scores** to [0, 1] to ensure numerical stability:\n",
    "  - id_scores = np.clip(id_scores, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create Labels** \n",
    "- 0 for ID samples\n",
    "- 1 for OOD samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "id_scores = np.array([0.9, 0.8, 0.85])\n",
    "ood_scores = np.array([0.1, 0.2, 0.05])\n",
    "\n",
    "labels = np.concatenate([np.zeros(len(id_scores)), np.ones(len(ood_scores))])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Combine Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = np.concatenate([id_scores, ood_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Convert scores → anomaly predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = 1.0 - all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Compute ROC curve:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(labels, preds)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Compute Precision–Recall curve & AUPR:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "aupr = average_precision_score(labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Return all metrics inside an OodEvaluationResult.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After computing AUROC, AUPR, the ROC curve, and the precision–recall curve, the function\n",
    "packages all results into an `OodEvaluationResult` dataclass, which stores:\n",
    "\n",
    "- `auroc`: Area under the ROC curve  \n",
    "- `fpr`: False positive rates  \n",
    "- `tpr`: True positive rates  \n",
    "- `labels`: Ground-truth labels (0 = ID, 1 = OOD)  \n",
    "- `preds`: Anomaly scores (`1 - confidence`)  \n",
    "- `precision`: Precision values  \n",
    "- `recall`: Recall values  \n",
    "- `aupr`: Area under the precision–recall curve  \n",
    "\n",
    "This structured return object makes it easy to access or visualize each metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Evaluating OOD Performance\n",
    "\n",
    "The following example shows how to use **evaluate_ood_performance** with\n",
    "a small set of in-distribution (ID) and out-of-distribution (OOD) confidence scores.\n",
    "\n",
    "**1. Define example scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_scores = np.array([0.95, 0.88, 0.91, 0.85, 0.93])   # Higher scores → ID\n",
    "ood_scores = np.array([0.12, 0.08, 0.22, 0.15, 0.05])  # Lower scores → OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Run OOD evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_ood_performance(id_scores, ood_scores)\n",
    "\n",
    "print(\"AUROC:\", result.auroc)\n",
    "print(\"AUPR :\", result.aupr)\n",
    "print(\"Labels:\", result.labels)\n",
    "print(\"Predictions (anomaly scores):\", result.preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Plot Roc Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Plot Precision-Recall Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
