{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMO — Multi-Input Multi-Output Networks\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In machine learning there are a few problems, one of them is confident misprediction. <br>\n",
    "One solutions to tackle this problem are ensembles which use a base model multiple times to lower uncertainty for the prediction. <br>\n",
    "There are different ways to use the output of the ensemble members but this topic is not part of this notebook. <br>\n",
    "The problem with ensembles is there high cost because of training the members of the ensemble separately. One way to tackle this problem is to use MIMO (multi-input-multi-output) configuration. <br>\n",
    "For further information: [Training independent subnetwroks for robust prediction](https://openreview.net/pdf?id=OGg9XnKxFAH)\n",
    "\n",
    "### What is a MIMO ?\n",
    "\n",
    "A MIMO uses mulitple inputs and outputs which can train subnetworks of the main network concurrently. By using this we get \"free\" forward passes while testing and leveraging the outputs similar to an ensemble. This method uses less computational cost and has the benefits of an ensemble.\n",
    "\n",
    "### How to test performance ?\n",
    "\n",
    "In the rest of the notebook is a short demo and comparison between a MIMO and an ensemble. Note, that this setup is not ideal and could be better for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toy dataset (classification variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=2000, noise=0.2, random_state=seed)\n",
    "X = X.astype('float32')\n",
    "y = y.astype('int64')\n",
    "\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Train size:', len(train_ds), 'Val size:', len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline model (standard MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden=128, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "m = MLP().to(device)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MIMO wrapper: make an M-input M-output version of the MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMO(nn.Module):\n",
    "    def __init__(self, base_hidden=128, in_dim=2, out_dim=2, M=3):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.input_layer = nn.Linear(in_dim * M, base_hidden)\n",
    "        self.body = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(base_hidden, base_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(base_hidden, out_dim) for _ in range(M)])\n",
    "\n",
    "    def forward(self, x_concat):\n",
    "        h = self.input_layer(x_concat)\n",
    "        h = self.body(h)\n",
    "        outs = [head(h) for head in self.heads]\n",
    "        outs = torch.stack(outs, dim=1)\n",
    "        return outs\n",
    "\n",
    "mimo_model = MIMO(M=3).to(device)\n",
    "print(mimo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93787cf9",
   "metadata": {},
   "source": [
    "## 5. Comparison between Ensemble and MIMO-Idea\n",
    "\n",
    "Training an Ensemble and a MIMO to compare acc, loss, ece, MI(disagreement), forward calls.\n",
    "The number of subnetworks is four as recommended in [TRAINING INDEPENDENT SUBNETWORKS FOR ROBUST\n",
    "PREDICTION](https://openreview.net/pdf?id=OGg9XnKxFAH)\n",
    "\n",
    "\n",
    "Setup:\n",
    "\n",
    "K members in ensemble = 3\n",
    "\n",
    "K subnetworks in the MIMO = 3 \n",
    "\n",
    "Epochs = 250\n",
    "\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Small experiments: train MIMO and an ensemble with comparable capacity\n",
    "K = 3\n",
    "epochs = 250\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "def softmax_np(logits: np.ndarray, axis=-1) -> np.ndarray:\n",
    "    e = np.exp(logits - logits.max(axis=axis, keepdims=True))\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def entropy_np(probs: np.ndarray, axis=-1, eps=1e-12) -> np.ndarray:\n",
    "    p = np.clip(probs, eps, 1.0)\n",
    "    return -np.sum(p * np.log(p), axis=axis)\n",
    "\n",
    "def ece_score(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15) -> float:\n",
    "    # probs: (N, C) predictive mean probs; labels: (N,)\n",
    "    confs = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    N = len(labels)\n",
    "    for i in range(n_bins):\n",
    "        mask = (confs >= bins[i]) & (confs < bins[i+1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        acc = (preds[mask] == labels[mask]).mean()\n",
    "        conf = confs[mask].mean()\n",
    "        ece += (mask.sum() / N) * abs(conf - acc)\n",
    "    return float(ece)\n",
    "\n",
    "def reliability_diagram(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15, ax=None):\n",
    "    confs = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    accs = []\n",
    "    avg_confs = []\n",
    "    counts = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (confs >= bins[i]) & (confs < bins[i+1])\n",
    "        counts.append(mask.sum())\n",
    "        if mask.sum() == 0:\n",
    "            accs.append(np.nan); avg_confs.append(np.nan)\n",
    "        else:\n",
    "            accs.append((preds[mask] == labels[mask]).mean())\n",
    "            avg_confs.append(confs[mask].mean())\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.plot(bin_centers, accs, marker='o', label='accuracy per bin')\n",
    "    ax.plot([0,1],[0,1],'--', color='gray')\n",
    "    ax.set_xlabel('Confidence'); ax.set_ylabel('Accuracy'); ax.set_title('Reliability diagram')\n",
    "    return ax\n",
    "\n",
    "# Train a normal ensemble of K independently initialized MLPs\n",
    "def train_ensemble(base_cls, K: int, train_loader, epochs: int = 5, lr: float = 1e-3):\n",
    "    models = []\n",
    "    ensemble_forward_calls = 0\n",
    "    for k in range(K):\n",
    "        m_k = base_cls().to(device)\n",
    "        opt = optim.Adam(m_k.parameters(), lr=lr)\n",
    "        lossfn = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            m_k.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device).float()\n",
    "                yb = yb.to(device).long()\n",
    "                opt.zero_grad()\n",
    "                ensemble_forward_calls += 1\n",
    "                out = m_k(xb)\n",
    "                loss = lossfn(out, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        models.append(m_k)\n",
    "    print(f\"Ensemble total forward calls: {ensemble_forward_calls}\")\n",
    "    return models\n",
    "\n",
    "def train_mimo(mimo_model, train_loader, epochs=epochs, lr=lr, M=K):\n",
    "    opt = torch.optim.Adam(mimo_model.parameters(), lr=lr)\n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    forward_calls = 0  # Zähler\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mimo_model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device).float()\n",
    "            yb = yb.to(device).long()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            forward_calls += 1  # Jeder Forward-Pass zählt einmal, liefert K Outputs\n",
    "            xb_cat = xb.repeat(1, M)\n",
    "            out = mimo_model(xb_cat)\n",
    "\n",
    "            # Mittelwert über die M Heads für Loss\n",
    "            out_mean = out.view(out.size(0), M, -1).mean(dim=1)\n",
    "            loss = lossfn(out_mean, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    print(f\"MIMO total forward calls: {forward_calls}\")\n",
    "    return mimo_model\n",
    "\n",
    "# Evaluation helpers\n",
    "def eval_ensemble_models(models, X_np: np.ndarray):\n",
    "    # returns mean_probs (N, C), member_probs (N, K, C)\n",
    "    X = torch.from_numpy(X_np).to(device).float()\n",
    "    member_probs = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = m(X).cpu().numpy()\n",
    "            member_probs.append(softmax_np(logits))\n",
    "    member_probs = np.stack(member_probs, axis=1)  # (N, K, C)\n",
    "    mean_probs = member_probs.mean(axis=1)\n",
    "    return mean_probs, member_probs\n",
    "\n",
    "def eval_mimo_model(mimo_model, X_np: np.ndarray, M: int):\n",
    "    # For MIMO we feed the same input concatenated M times to get M heads' outputs\n",
    "    X = torch.from_numpy(X_np).to(device).float()\n",
    "    X_cat = X.repeat(1, M)                         # (N, feat*M)\n",
    "    mimo_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = mimo_model(X_cat).cpu().numpy()  # (N, M, C)\n",
    "    member_probs = softmax_np(logits, axis=-1)    # (N, M, C)\n",
    "    mean_probs = member_probs.mean(axis=1)        # (N, C)\n",
    "    return mean_probs, member_probs\n",
    "\n",
    "\n",
    "# 1) (re)train mimo_model for fairness (short run)\n",
    "mimo_model = MIMO(M=K).to(device)\n",
    "train_mimo(mimo_model, train_loader, epochs=epochs, lr=lr, M=K)\n",
    "\n",
    "# 2) train K independent MLPs\n",
    "ensemble_models = train_ensemble(MLP, K, train_loader, epochs=epochs, lr=lr)\n",
    "\n",
    "# Eval on validation set\n",
    "X_val_np = X_val.astype('float32')\n",
    "y_val_np = y_val.astype('int64')\n",
    "\n",
    "mimo_mean, mimo_members = eval_mimo_model(mimo_model, X_val_np, M=K)\n",
    "ens_mean, ens_members = eval_ensemble_models(ensemble_models, X_val_np)\n",
    "\n",
    "# Metrics\n",
    "def print_metrics(name, mean_probs, member_probs):\n",
    "    acc = accuracy_score(y_val_np, mean_probs.argmax(axis=1))\n",
    "    nll = log_loss(y_val_np, mean_probs)\n",
    "    ece = ece_score(mean_probs, y_val_np, n_bins=15)\n",
    "    # disagreement (mutual information proxy)\n",
    "    H_mean = entropy_np(mean_probs)\n",
    "    H_members = entropy_np(member_probs, axis=-1).mean(axis=1)\n",
    "    mi = (H_mean - H_members).mean()\n",
    "    print(f\"{name} — acc: {acc:.4f}, nll: {nll:.4f}, ece: {ece:.4f}, MI(disagreement): {mi:.4f}\")\n",
    "\n",
    "print_metrics(\"MIMO\", mimo_mean, mimo_members)\n",
    "print_metrics(\"Ensemble\", ens_mean, ens_members)\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
    "\n",
    "# 1) Reliability diagrams\n",
    "ax = axes[0,0]\n",
    "reliability_diagram(mimo_mean, y_val_np, n_bins=15, ax=ax)\n",
    "ax.set_title(\"MIMO Reliability\")\n",
    "ax = axes[0,1]\n",
    "reliability_diagram(ens_mean, y_val_np, n_bins=15, ax=ax)\n",
    "ax.set_title(\"Ensemble Reliability\")\n",
    "\n",
    "# 2) Decision boundary (mean-predictive probability for class 1)\n",
    "ax = axes[1,0]\n",
    "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200),\n",
    "                     np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 200))\n",
    "grid = np.stack([xx.ravel(), yy.ravel()], axis=1).astype('float32')\n",
    "mimo_grid_mean, _ = eval_mimo_model(mimo_model, grid, M=K)\n",
    "ens_grid_mean, _  = eval_ensemble_models(ensemble_models, grid)\n",
    "cs = ax.contourf(xx, yy, mimo_grid_mean[:,1].reshape(xx.shape), levels=20, cmap='RdBu', alpha=0.6)\n",
    "ax.scatter(X_val[:,0], X_val[:,1], c=y_val, s=12, cmap='RdBu', edgecolor='k', alpha=0.8)\n",
    "ax.set_title('MIMO mean prob (class 1)')\n",
    "ax = axes[1,1]\n",
    "cs = ax.contourf(xx, yy, ens_grid_mean[:,1].reshape(xx.shape), levels=20, cmap='RdBu', alpha=0.6)\n",
    "ax.scatter(X_val[:,0], X_val[:,1], c=y_val, s=12, cmap='RdBu', edgecolor='k', alpha=0.8)\n",
    "ax.set_title('Ensemble mean prob (class 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extra: plot disagreement histogram\n",
    "plt.figure(figsize=(6,3))\n",
    "mimo_MI_per_example = entropy_np(mimo_mean) - entropy_np(mimo_members, axis=-1).mean(axis=1)\n",
    "ens_MI_per_example  = entropy_np(ens_mean ) - entropy_np(ens_members, axis=-1).mean(axis=1)\n",
    "\n",
    "max_val = max(float(np.nanmax(mimo_MI_per_example)), float(np.nanmax(ens_MI_per_example)))\n",
    "bins = np.linspace(0.0, max_val, 50)\n",
    "\n",
    "plt.hist(mimo_MI_per_example, bins=bins, density=True, alpha=0.6, label='MIMO MI', color='C0')\n",
    "plt.hist(ens_MI_per_example,  bins=bins, density=True, alpha=0.6, label='Ensemble MI', color='C1')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Disagreement (mutual information) distribution')\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bddb902",
   "metadata": {},
   "source": [
    "Comment: We see that MIMO has nearly 1/3 of the forward calls. Note, that the acc, nll, ece-score is nearly the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
