{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of a Model Transformation API\n",
    "\n",
    "Here is a small demo of how the existing Drop-out/Connect representations might be implemented in a more extensible manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following nested model.\n",
    "Here, we would expect Dropout layers to be inserted before layers `1` and `3.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', Sequential(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "))\n",
      "('1', Linear(in_features=10, out_features=10, bias=True))\n",
      "('2', ReLU())\n",
      "('3', Sequential(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "))\n",
      "tensor([0.0951, 0.2140, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4059,\n",
      "        0.0000], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import probly.representation.dropout as probly_dropout\n",
    "import probly.traverse_representation.dropout as probly_traverse_dropout\n",
    "\n",
    "\n",
    "def showModel(m):\n",
    "    print(\"\\n\".join(map(str, m.named_children())))\n",
    "\n",
    "\n",
    "s = torch.nn.Sequential(\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 10),\n",
    "        torch.nn.ReLU(),\n",
    "    ),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 10),\n",
    "        torch.nn.ReLU(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "showModel(s)\n",
    "s.eval()\n",
    "print(s(torch.tensor([1.0] * 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Implementation\n",
    "\n",
    "Using the current implementation, we actually have a bug, because the recursive information does not properly propagate the `first_layer` down the call stack.\n",
    "More specifically, no `Dropout` is inserted before layer `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Dropout(p=0.4, inplace=False)\n",
      "      (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "  )\n",
      "))\n",
      "tensor([[[0.0954, 0.1076, 0.0954, 0.1024, 0.0982, 0.0954, 0.0954, 0.0954,\n",
      "          0.1194, 0.0954],\n",
      "         [0.1091, 0.1149, 0.0893, 0.0893, 0.0893, 0.0893, 0.0893, 0.0893,\n",
      "          0.1508, 0.0893]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s2 = probly_dropout.Dropout(s, p=0.4)\n",
    "\n",
    "showModel(s2)\n",
    "s2.eval()\n",
    "print(s2.predict_representation(torch.tensor([[1.0] * 10]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Traverser-based Implementation\n",
    " \n",
    "The same API can be realized using the extensible traverser approach.\n",
    "The new implementation fixes the bug, flattens `Sequential` layers and is much more flexible in general (as we will show next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', Sequential(\n",
      "  (0_0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (0_1): ReLU()\n",
      "  (1_0): Dropout(p=0.4, inplace=False)\n",
      "  (1_1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3_0_0): Dropout(p=0.4, inplace=False)\n",
      "  (3_0_1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3_1): ReLU()\n",
      "))\n",
      "tensor([[[0.1093, 0.0939, 0.0939, 0.0939, 0.0939, 0.1055, 0.0939, 0.0939,\n",
      "          0.1279, 0.0939],\n",
      "         [0.0977, 0.1066, 0.0958, 0.0966, 0.0958, 0.0958, 0.0958, 0.0958,\n",
      "          0.1241, 0.0958]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s3 = probly_traverse_dropout.Dropout(s, p=0.4)\n",
    "\n",
    "showModel(s3)\n",
    "s3.eval()\n",
    "print(s3.predict_representation(torch.tensor([[1.0] * 10]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the extensibility of the new approach, let's implement our own `Linear` layer, which we would like to be considered during rewriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', Sequential(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "))\n",
      "('1', Linear(in_features=10, out_features=10, bias=True))\n",
      "('2', ReLU())\n",
      "('3', Sequential(\n",
      "  (0): MyLinear()\n",
      "  (1): ReLU()\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "class MyLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.K = torch.nn.Parameter(\n",
    "            torch.empty(out_features, in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.K\n",
    "\n",
    "\n",
    "myS = torch.nn.Sequential(\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 10),\n",
    "        torch.nn.ReLU(),\n",
    "    ),\n",
    "    torch.nn.Linear(10, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Sequential(\n",
    "        MyLinear(10, 10),\n",
    "        torch.nn.ReLU(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "showModel(myS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the current implementation, the custom linear layer is ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): MyLinear()\n",
      "    (1): ReLU()\n",
      "  )\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "myS2 = probly_dropout.Dropout(myS, p=0.4)\n",
    "\n",
    "showModel(myS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the extensible approach, we can dynamically extend the implementation in user-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', Sequential(\n",
      "  (0_0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (0_1): ReLU()\n",
      "  (1_0): Dropout(p=0.4, inplace=False)\n",
      "  (1_1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3_0_0): Dropout(p=0.4, inplace=False)\n",
      "  (3_0_1): MyLinear()\n",
      "  (3_1): ReLU()\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "probly_traverse_dropout.register(MyLinear)  # Registers the new layer type\n",
    "\n",
    "myS3 = probly_traverse_dropout.Dropout(myS, p=0.4)\n",
    "\n",
    "showModel(myS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Discussion\n",
    "\n",
    "This is just a very basic demo of how an extensible API could look like.\n",
    "The underlying traverser system is significantly more powerful.\n",
    "\n",
    "Right now it uses a combination of different extensible recursive traversers for Python datastructures, Torch modules and problem specific replacement code.\n",
    "\n",
    "It was designed with the goal of enabling a cross-framework implementation, i.e., the Same Dropout class could be adapted for JAX or TensorFlow.\n",
    "\n",
    "A potential future API could then look something like this:\n",
    "```python\n",
    "my_torch = MyTorch()\n",
    "my_jax = MyJAX()\n",
    "\n",
    "import future.representation.dropout as dropout\n",
    "\n",
    "my_torch_dropout = dropout.Dropout(my_torch) # Works\n",
    "my_jax_dropout = dropout.Dropout(my_jax) # => Error: Unknown model type!\n",
    "\n",
    "import future.representation.extension.jax\n",
    "\n",
    "my_jax_dropout = dropout.Dropout(my_jax) # Works\n",
    "```\n",
    "\n",
    "The advantage of such an approach would be, that extensions of our package to entirely different settings could be provided by the community as external plugins.\n",
    "\n",
    "The main risk with such an approach is reduced maintainablity due to complexity.\n",
    "While some complexity cannot be avoided when going down this path, it is possible to hide this complexity behind a well-designed Traverser API which (ideally) would require little to no maintenance after being completed and tested.\n",
    "How such an API should look like is still to be discussed.\n",
    "\n",
    "One first alpha attempt can be found in the `traverse` (generic datastructure traversal) and `nn_traverse` (extensions for traversal of neural networks).\n",
    "While not perfect, those modules showcase how extensibility could be achieved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
