##########################
Examples and Tutorials
##########################

This section contains practical, end-to-end examples that demonstrate how `probly` can be used in real applications. Each tutorial provides a guided workflow from start to finish, including model transformation, execution and interpretation of the results. The examples also directly correspond to the advanced modeling patterns discussed in :doc:`Advanced Topics <advanced_topics>`, providing at least one worked example for concepts such as uncertainty-aware transformations, ensemble methods, and mixed-model workflows. **In other words, this page is the “worked examples” companion to :doc:`Advanced Topics <advanced_topics>`: each tutorial takes one of the advanced ideas and shows how it looks in a complete, runnable pipeline.** They are self-contained and can be adapted to individual projects and datasets.

For deeper background before running the examples, see :doc:`Advanced Topics <advanced_topics>`. **If you want more context on why particular design choices are made (e.g. why Dropout can be interpreted as approximate Bayesian inference, or how ensembles trade compute for robustness), the Advanced Topics chapter provides the conceptual framing.**

Users who are new to `probly` are encouraged to begin with the introductory Dropout example before exploring ensemble-based methods and more advanced uncertainty-aware workflows. **This ordering mirrors the recommended learning path in Advanced Topics: start with a single-model uncertainty mechanism, then move to ensemble-based uncertainty, and finally explore mixed-model workflows.**

# Mini gallery (quick links)

These are short, focused example pages (generated by Sphinx-Gallery) that are relevant to this page and :doc:`Advanced Topics <advanced_topics>`.

.. minigallery::
../../examples/plot_gallery_smoke_test.py
../../examples/plot_samples_with_array_sample.py
../../examples/plot_create_sample_dispatch.py
../../examples/plot_using_predict_protocol.py

1. Uncertainty estimation with Dropout on MNIST
   ===============================================

## What you will learn (I)

In this tutorial, you will learn how to use `probly` to make a standard neural network uncertainty-aware with the Dropout transformation. You start from a conventional PyTorch model trained on MNIST and then apply `probly` so that Dropout remains active during inference. By running multiple stochastic forward passes, you obtain a distribution of predictions and estimate predictive uncertainty.

This workflow is conceptually based on treating Dropout as a Bayesian approximation in deep neural networks, as proposed by Gal and Ghahramani :cite:`gal2016dropout`, and follows the standard deep learning setup described in :cite:`lecun1998gradient,goodfellow2016deep`. **From the perspective of :doc:`Advanced Topics <advanced_topics>`, this example is the canonical introduction to uncertainty-aware transformations: a standard deterministic model is augmented with a transformation that changes inference-time behaviour without requiring architectural changes. In particular, it illustrates the “adapter” role of transformations discussed there: inference can operate in an uncertainty-aware way while the surrounding training and data pipeline remain unchanged.**

## Prerequisites

This example requires Python 3.8 or later and the packages `probly`, `torch` and `torchvision`:

.. code-block:: bash

pip install probly torch torchvision

The use of PyTorch and torchvision for MNIST follows standard practice in modern deep learning workflows, as also outlined in :cite:`goodfellow2016deep`. **This also reflects the integration mindset described in :doc:`Advanced Topics <advanced_topics>`: `probly` is designed to plug into existing PyTorch code with minimal changes at clear boundaries (data in, predictions out).**

Step 1: Load the MNIST dataset

```

In this step, you load the MNIST dataset, a canonical benchmark for handwritten digit recognition consisting of 60,000 training and 10,000 test images :cite:`lecun1998gradient`. The dataset is widely used to illustrate methods for uncertainty estimation because it is small, well-understood, and easy to train on. **Using a familiar benchmark helps isolate the uncertainty workflow itself, matching the Advanced Topics recommendation to validate advanced components in a controlled, low-complexity setting before scaling up.**

.. code-block:: python

   import torch
   from torch.utils.data import DataLoader
   from torchvision import datasets, transforms

   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

   transform = transforms.ToTensor()

   train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
   test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
   test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

By wrapping MNIST with ``DataLoader`` and using ``ToTensor()``, you obtain batched tensors suitable for GPU-accelerated training. This setup is standard for supervised image classification tasks :cite:`goodfellow2016deep`.

Step 2: Define a base convolutional model
```

Here, you define a simple convolutional neural network (CNN) with two convolution–ReLU–max-pooling stages followed by a fully connected classification head. CNNs are a natural choice for image classification and build on the ideas introduced by LeCun et al. :cite:`lecun1998gradient`.

.. code-block:: python

import torch.nn as nn

class SimpleCNN(nn.Module):
def **init**(self):
super().**init**()
self.net = nn.Sequential(
nn.Conv2d(1, 32, 3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
nn.Conv2d(32, 64, 3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
nn.Flatten(),
nn.Linear(64 * 7 * 7, 10),
)

```
   def forward(self, x):
       return self.net(x)
```

model = SimpleCNN().to(device)

The network is deliberately compact, keeping training quick while still being expressive enough to benefit from uncertainty estimation techniques discussed in Bayesian deep learning :cite:`bishop2006pattern,gal2016dropout`. **This aligns with the Advanced Topics emphasis on starting simple and adding complexity only once the end-to-end workflow is correct and interpretable.**

Step 3: Train the model briefly

```

You now train the CNN using the Adam optimizer and the cross-entropy loss, which is the standard objective for multi-class classification problems :cite:`bishop2006pattern`.

.. code-block:: python

   import torch.optim as optim
   import torch.nn.functional as F

   optimizer = optim.Adam(model.parameters(), lr=1e-3)

   model.train()
   for epoch in range(1):
       for x, y in train_loader:
           x, y = x.to(device), y.to(device)
           optimizer.zero_grad()
           logits = model(x)
           loss = F.cross_entropy(logits, y)
           loss.backward()
           optimizer.step()

A single training epoch is sufficient for demonstration purposes. In practice, you would typically train longer for higher accuracy, but the uncertainty-aware part of the pipeline is independent of the exact training duration. **This separation between “base model training” and “uncertainty mechanism” mirrors the workflow-oriented view in Advanced Topics: you can iterate on training quality and uncertainty estimation somewhat independently.**

Step 4: Apply ``probly``’s Dropout transformation
```

The crucial step is to transform the trained model into an uncertainty-aware model by enabling Dropout at inference time. `probly` provides a high-level transformation that keeps Dropout active even when the model is in `eval()` mode.

.. code-block:: python

from probly.transformation import dropout

prob_model = dropout(model, p=0.5, enable_at_eval=True)

This setup follows the Monte Carlo Dropout (MC Dropout) interpretation, where Dropout is treated as a variational approximation to a Bayesian neural network :cite:`gal2016dropout`. The probability `p=0.5` controls the amount of stochasticity, i.e. how strongly the model’s predictions will vary across stochastic forward passes. **In Advanced Topics terms, this is a concrete example of how a transformation can enforce a particular uncertainty representation (here: stochastic forward passes) while keeping the external model interface stable.**

Step 5: Perform Monte Carlo inference

```

You now perform multiple stochastic forward passes to obtain a Monte Carlo estimate of the predictive distribution. The mean of the sampled probabilities approximates the predictive mean, while the standard deviation provides a measure of epistemic uncertainty :cite:`gal2016dropout,kendall2017uncertainties`.

.. code-block:: python

   import torch.nn.functional as F
   import torch

   @torch.no_grad()
   def mc_predict(model, x, samples=30):
       model.eval()  # Dropout remains active
       probs = []
       for _ in range(samples):
           logits = model(x)
           p = F.softmax(logits, dim=-1)
           probs.append(p.unsqueeze(0))
       probs = torch.cat(probs, dim=0)
       return probs.mean(0), probs.std(0)

   x_batch, _ = next(iter(test_loader))
   x_batch = x_batch.to(device)

   mean_probs, std_probs = mc_predict(prob_model, x_batch[0:1])

   print("Mean probabilities:", mean_probs.squeeze().cpu())
   print("Std probabilities:", std_probs.squeeze().cpu())

The function ``mc_predict`` implements the Monte Carlo estimator by repeatedly sampling from the implicit model posterior induced by Dropout. The resulting uncertainty estimates are particularly useful under distribution shift and for downstream decision making :cite:`ovadia2019trust`. **The parameter ``samples`` also acts as a practical “performance knob” in the sense discussed in Advanced Topics: increasing it improves the stability of the Monte Carlo estimate but increases inference cost, making it a simple way to trade latency against uncertainty quality.**

Step 6: Visualize uncertainty
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Visualizing both the predictive mean and the associated uncertainty (e.g. as error bars or shaded regions) can help you identify ambiguous or out-of-distribution samples :cite:`kendall2017uncertainties,ovadia2019trust`. **This connects the numerical uncertainty representation back to interpretation, which is a recurring theme in Advanced Topics: uncertainty is most useful when it can be inspected, validated, and related to concrete modelling questions (ambiguity, shift, or downstream risk).**

.. image:: mc_dropout_example.png
   :width: 550px
   :align: center
   :alt: Monte Carlo Dropout uncertainty visualization

Summary (I)
-----------

In this example, ``probly`` was used to transform a standard neural network into an uncertainty-aware model. Dropout remains active during inference and multiple forward passes allow you to obtain predictive uncertainty without modifying the original architecture. This approach builds on the MC Dropout framework for approximate Bayesian inference in deep networks :cite:`gal2016dropout` and follows standard best practices in deep learning :cite:`goodfellow2016deep,bishop2006pattern`. **As a result, this tutorial provides the most direct “worked example” for the Advanced Topics discussion of transformations and uncertainty-aware inference, and it establishes the baseline workflow that the ensemble-based examples build on.**


2. Creating a SubEnsemble with ``probly``
========================================

What you will learn (II)
------------------------

In this tutorial, you will learn how to construct an ensemble using ``probly`` and how to derive a smaller ``SubEnsemble`` without retraining. This allows you to trade inference speed for accuracy and predictive uncertainty quality in a controlled way. The design follows the deep ensemble methodology of Lakshminarayanan et al. :cite:`lakshminarayanan2017simple` and classical ensemble learning ideas :cite:`dietterich2000ensemble`. **In the framing of :doc:`Advanced Topics <advanced_topics>`, this tutorial demonstrates the ensemble pattern as a practical, modular recipe: you increase robustness and uncertainty quality by combining independently trained members, and you retain deployment flexibility by subsampling members at inference time.**

Step 1: Define a simple base model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You first define a small multilayer perceptron (MLP) that will serve as the base architecture for all ensemble members. Even though CNNs often achieve higher accuracy on MNIST, MLPs remain a simple and effective choice for illustrating ensemble techniques :cite:`bishop2006pattern`.

.. code-block:: python

   import torch
   import torch.nn as nn
   from torch.utils.data import DataLoader
   from torchvision import datasets, transforms

   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

   transform = transforms.ToTensor()

   train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
   test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
   test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

   class SmallMLP(nn.Module):
       def __init__(self):
           super().__init__()
           self.net = nn.Sequential(
               nn.Flatten(),
               nn.Linear(28 * 28, 128),
               nn.ReLU(),
               nn.Linear(128, 10),
           )

       def forward(self, x):
           return self.net(x)

The shared base model architecture ensures that differences between ensemble members arise primarily from random initialization and stochastic optimization, which is essential for diverse deep ensembles :cite:`fort2019deep`.

Step 2: Create an Ensemble with ``probly``
```

You now instantiate multiple independent copies of the base model and wrap them into a `probly` `Ensemble`. Each member will be trained separately but evaluated jointly.

.. code-block:: python

from probly.ensemble import Ensemble

num_members = 5
members = [SmallMLP().to(device) for _ in range(num_members)]
ensemble = Ensemble(members)

This construction corresponds to the **deep ensemble** paradigm :cite:`lakshminarayanan2017simple`, where several independently trained networks are combined to obtain improved accuracy and better-calibrated uncertainty estimates compared to a single model. **This is the central ensemble-based uncertainty mechanism discussed in Advanced Topics, and it complements MC Dropout by representing uncertainty through member diversity rather than repeated stochastic passes through one model.**

Step 3: Train ensemble members

```

Each ensemble member is trained independently on the same data. Due to random initialization and mini-batch sampling, the members converge to different local optima in the loss landscape, which is a key factor for the effectiveness of ensembles :cite:`fort2019deep`.

.. code-block:: python

   import torch.nn.functional as F
   import torch.optim as optim

   def train_member(model, loader, epochs=1):
       optimizer = optim.Adam(model.parameters(), lr=1e-3)
       model.train()
       for _ in range(epochs):
           for x, y in loader:
               x, y = x.to(device), y.to(device)
               optimizer.zero_grad()
               logits = model(x)
               loss = F.cross_entropy(logits, y)
               loss.backward()
               optimizer.step()

   for m in members:
       train_member(m, train_loader, epochs=1)

While only one epoch is used here for brevity, additional epochs typically increase accuracy. The crucial property is that each member learns a slightly different function, giving rise to ensemble diversity :cite:`dietterich2000ensemble,fort2019deep`. **In Advanced Topics terms, this is the “diversity through independent training” pattern that makes ensembles useful under distribution shift and for uncertainty estimation.**

Step 4: Evaluate the Ensemble
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ensemble prediction is obtained by aggregating individual member predictions (e.g. by averaging logits or probabilities). This aggregation reduces variance and often improves both accuracy and calibration compared to single models :cite:`lakshminarayanan2017simple`.

.. code-block:: python

   @torch.no_grad()
   def evaluate(model, loader):
       model.eval()
       correct = 0
       total = 0
       for x, y in loader:
           x, y = x.to(device), y.to(device)
           preds = model(x).argmax(dim=1)
           correct += (preds == y).sum().item()
           total += x.size(0)
       return correct / total

   full_acc = evaluate(ensemble, test_loader)
   print("Ensemble accuracy:", full_acc)

In ``probly``, the ``Ensemble`` abstraction takes care of combining member outputs internally, making it straightforward to compare an ensemble to a single model in terms of accuracy and uncertainty. **This corresponds to the “reusable templates” viewpoint in Advanced Topics: once the ensemble abstraction is in place, you can swap base architectures or inference settings without rewriting the evaluation logic.**

Step 5: Create and evaluate a SubEnsemble
```

Using the trained ensemble, you can construct a `SubEnsemble` that uses only a subset of the ensemble members. This allows for a flexible accuracy–latency trade-off at deployment time without needing to retrain any models. **This is a concrete instance of the “performance knob” idea in Advanced Topics: you can dial compute up or down by changing the number of active members while keeping the modelling approach fixed.**

.. code-block:: python

from probly.ensemble import SubEnsemble

sub = SubEnsemble(ensemble, indices=[0, 1])
sub_acc = evaluate(sub, test_loader)
print("SubEnsemble accuracy:", sub_acc)

The idea of using partial ensembles or subnetworks to control computational budget is related to recent work on training independent subnetworks for robust predictions :cite:`havasi2021training` and subsampling strategies for efficient uncertainty estimation :cite:`cunningham2020ensemble`. With `probly`, this pattern becomes a simple configuration choice.

Visual result SubEnsemble

```

.. image:: subensemble_comparison.png
   :width: 500px
   :align: center
   :alt: Accuracy comparison between full ensemble and SubEnsemble

Summary (II)
------------

In this example, ``probly`` was used to create both a full Ensemble and a SubEnsemble without retraining. The full Ensemble generally provides the highest accuracy and most reliable uncertainty, while the SubEnsemble offers reduced inference cost with still useful performance. This illustrates how deep ensembles :cite:`lakshminarayanan2017simple` can be adapted to practical deployment constraints using ``probly``’s ensemble abstractions. **Seen through the lens of :doc:`Advanced Topics <advanced_topics>`, this tutorial emphasises that “advanced” often means making trade-offs explicit and controllable: SubEnsembles let you keep the same modelling approach while adapting runtime cost to the deployment setting.**


3. MixedEnsemble with ``probly``
================================

What you will learn (III)
-------------------------

In this tutorial, you will learn how to build a ``MixedEnsemble`` using ``probly`` by combining different neural network architectures into a single probabilistic ensemble. You will compare it to a homogeneous ensemble and observe how model diversity may influence performance and robustness. This follows the general idea that heterogeneous ensembles can outperform homogeneous ones when models capture complementary inductive biases :cite:`opitz1999popular,jacobs1991adaptive`. **In the Advanced Topics taxonomy, this tutorial is the most direct example of a mixed-model workflow: instead of varying only initialisation, you vary inductive bias by mixing architectures, which can change robustness and failure modes.**

Step 1: Prepare data
~~~~~~~~~~~~~~~~~~~~

As in the previous tutorials, you use MNIST as a benchmark dataset :cite:`lecun1998gradient`. The data loading pipeline is identical, which highlights that ``probly``’s advanced ensemble types can be introduced without changing the dataset interface. **This reinforces a key Advanced Topics principle: keep boundaries stable (data pipeline, evaluation loop) while you vary the modelling components.**

.. code-block:: python

   import torch
   from torch.utils.data import DataLoader
   from torchvision import datasets, transforms

   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

   transform = transforms.ToTensor()

   train_dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
   test_dataset = datasets.MNIST(root="./data", train=False, transform=transform, download=True)

   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
   test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

Step 2: Define different architectures
```

You now define two different architectures: a small CNN and a small MLP. The CNN leverages spatial structure in the images, while the MLP operates on flattened pixels. Combining these architectures in a MixedEnsemble reflects the idea of mixing experts with different inductive biases :cite:`jacobs1991adaptive`.

.. code-block:: python

import torch.nn as nn
import torch.nn.functional as F

class SmallCNN(nn.Module):
def **init**(self):
super().**init**()
self.conv = nn.Sequential(
nn.Conv2d(1, 32, 3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
nn.Conv2d(32, 64, 3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2),
)
self.head = nn.Sequential(
nn.Flatten(),
nn.Linear(64 * 7 * 7, 128),
nn.ReLU(),
nn.Linear(128, 10),
)

```
   def forward(self, x):
       x = self.conv(x)
       x = self.head(x)
       return x
```

class SmallMLP(nn.Module):
def **init**(self):
super().**init**()
self.net = nn.Sequential(
nn.Flatten(),
nn.Linear(28 * 28, 256),
nn.ReLU(),
nn.Linear(256, 10),
)

```
   def forward(self, x):
       return self.net(x)
```

The architectural diversity is the main driver of improved robustness in heterogeneous ensembles :cite:`opitz1999popular`, since different architectures often fail on different inputs. **This is exactly the motivation highlighted in Advanced Topics for mixed-model recipes: diversity is not only about random seeds but also about representational bias and complementary error patterns.**

Step 3: Create Ensemble and MixedEnsemble

```

You first construct a homogeneous ensemble consisting only of CNNs, and then a ``MixedEnsemble`` containing both CNN and MLP members.

.. code-block:: python

   from probly.ensemble import Ensemble, MixedEnsemble

   cnn_members = [SmallCNN().to(device) for _ in range(3)]
   cnn_ensemble = Ensemble(cnn_members)

   mixed_members = [
       SmallCNN().to(device),
       SmallCNN().to(device),
       SmallMLP().to(device),
   ]
   mixed_ensemble = MixedEnsemble(mixed_members)

This setup mirrors the idea of mixtures of experts :cite:`jacobs1991adaptive` and modern large-scale sparse ensembles :cite:`shazeer2017outrageously`, but in a simplified form where ``probly`` handles the aggregation of member predictions without a separate gating network. **In Advanced Topics terms, this illustrates how heterogeneous components can still be managed under a single, consistent interface, making it easier to experiment with architecture-level diversity without rewriting the surrounding pipeline.**

Step 4: Train all members
~~~~~~~~~~~~~~~~~~~~~~~~~

All ensemble members, both in the homogeneous CNN ensemble as well as the mixed ensemble are trained independently using the same training loop.

.. code-block:: python

   import torch.optim as optim

   def train(model, loader, epochs=1):
       optimizer = optim.Adam(model.parameters(), lr=1e-3)
       model.train()
       for _ in range(epochs):
           for x, y in loader:
               x, y = x.to(device), y.to(device)
               optimizer.zero_grad()
               logits = model(x)
               loss = F.cross_entropy(logits, y)
               loss.backward()
               optimizer.step()

   for m in cnn_members:
       train(m, train_loader, epochs=1)

   for m in mixed_members:
       train(m, train_loader, epochs=1)

Independent training encourages diversity in the learned decision boundaries, which is critical for ensemble performance under distribution shift :cite:`opitz1999popular,ovadia2019trust`. **This also ties back to the Advanced Topics emphasis on robustness: heterogeneous ensembles can provide different “views” of the same data and may degrade more gracefully under shift than a purely homogeneous set of models.**

Step 5: Evaluate both ensembles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Finally, you evaluate both the homogeneous CNN ensemble and the MixedEnsemble on the test set and compare their accuracies.

.. code-block:: python

   @torch.no_grad()
   def evaluate(model, loader):
       model.eval()
       correct = 0
       total = 0
       for x, y in loader:
           x, y = x.to(device), y.to(device)
           logits = model(x)
           preds = logits.argmax(dim=1)
           correct += (preds == y).sum().item()
           total += x.size(0)
       return correct / total

   acc_cnn = evaluate(cnn_ensemble, test_loader)
   acc_mixed = evaluate(mixed_ensemble, test_loader)

   print("Homogeneous CNN Ensemble accuracy:", acc_cnn)
   print("MixedEnsemble accuracy:", acc_mixed)

Beyond accuracy, you could also compare calibration and robustness under distribution shift, as suggested by Ovadia et al. :cite:`ovadia2019trust`. Mixed ensembles often exhibit different failure modes than homogeneous ones, which can be beneficial in safety-critical applications. **This naturally connects to the Advanced Topics recommendation to evaluate not only accuracy but also calibration and behaviour under shift when uncertainty estimates are intended for downstream decisions.**

Visual result
-------------

.. image:: mixed_ensemble_comparison.png
   :width: 500px
   :align: center
   :alt: Accuracy comparison between homogeneous and mixed ensembles

Summary (III)
-------------

In this example, you used ``probly`` to construct both a homogeneous ensemble and a ``MixedEnsemble`` combining different model types. The MixedEnsemble may capture complementary model behaviour and can therefore improve robustness and calibration in some settings :cite:`opitz1999popular,ovadia2019trust`. By providing a unified abstraction for homogeneous and heterogeneous ensembles, ``probly`` makes it straightforward to explore such design choices in practical applications. **Taken together, Tutorials I–III implement the progression described in :doc:`Advanced Topics <advanced_topics>`: uncertainty-aware transformations (Dropout), scalable ensemble-based uncertainty with deployment trade-offs (SubEnsemble), and mixed-model workflows that use architectural diversity as an additional robustness tool (MixedEnsemble).**
```

