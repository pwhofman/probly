{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6951c7b9-605e-480d-9844-b35f56edbcef",
   "metadata": {},
   "source": [
    "# Understanding Prior Networks and Regression Prior Networks\n",
    "   ### A Practical and Mathematical Guide to Uncertainty Modeling in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5e408-9234-4b32-92ec-c6318924f296",
   "metadata": {},
   "source": [
    "##  1. Introduction\n",
    "\n",
    "Deep learning models are powerful, but standard neural networks cannot express uncertainty about their predictions. In many applicationsâ€”such as self-driving cars, medical diagnosis, or other safety-critical systemsâ€”it is important to know *how confident* a model is in its output.\n",
    "\n",
    "Traditionally, **neural network ensembles** are used to estimate uncertainty because they capture both:\n",
    "- **Data uncertainty (aleatoric uncertainty)** â†’ noise inherent in the data\n",
    "- **Knowledge uncertainty (epistemic uncertainty)** â†’ model ignorance, OOD inputs\n",
    "\n",
    "However, ensembles are **computationally expensive**, because they require running **many neural networks** at inference time. This makes them impractical for real-world systems that need fast predictions.\n",
    "\n",
    "To solve this, **Prior Networks** were introduced for **classification tasks**. Instead of predicting class probabilities directly (e.g., through a softmax layer), Prior Networks predict the parameters of a **Dirichlet distribution**, allowing a single model to mimic the uncertainty behavior of an entire ensemble.\n",
    "\n",
    "This idea was later extended to continuous outputs in **Regression Prior Networks (Malinin et al., 2020)**, which use a **Normalâ€“Wishart prior** and produce **Student-t predictive distributions**. These models allow single-network uncertainty estimation for regression problems.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Understand why ensembles provide strong uncertainty\n",
    "- Explain why ensembles are expensive at inference\n",
    "- Show how Prior Networks solve the ensemble cost problem\n",
    "- Explain how **classification Prior Networks** work (Dirichlet priors)\n",
    "- Explain how **Regression Prior Networks** work (Normalâ€“Wishart priors â†’ Student-t output)\n",
    "- Show how this relates to **Evidential Deep Learning (Sensoy et al., 2018)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbb708-f3f1-47a7-89c4-b0d81cdf6d1c",
   "metadata": {},
   "source": [
    "## 2. Ensembles and Why They Matter\n",
    "\n",
    "Deep learning ensembles combine several independently trained models:\n",
    "\n",
    "\n",
    "$ f^{(1)}, f^{(2)}, \\dots, f^{(M)}$\n",
    "\n",
    "\n",
    "Ensembles capture two types of uncertainty:\n",
    "\n",
    "### ðŸ”¹ Data Uncertainty (Aleatoric)\n",
    "Noise in the data.\n",
    "\n",
    "### ðŸ”¹ Knowledge Uncertainty (Epistemic)\n",
    "Model uncertainty due to lack of knowledge.\n",
    "\n",
    "Ensembles are powerful but computationally expensive:\n",
    "\n",
    "- $MÃ—$ compute\n",
    "- $MÃ—$ memory\n",
    "- $MÃ—$ inference time\n",
    "\n",
    " but we need something cheaper that still captures uncertainty.\n",
    "\n",
    "This leads to **Prior Networks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807721b-ab58-4cdf-aa1b-3169d8484efe",
   "metadata": {},
   "source": [
    "## 3. Prior Networks for Classification (Dirichlet Prior Networks)\n",
    "\n",
    "Prior Networks were first introduced for **classification tasks** (Malinin & Gales, 2019).\n",
    "Instead of predicting class probabilities directly using softmax, a Prior Network predicts the\n",
    "parameters of a **Dirichlet distribution**:\n",
    "\n",
    "$\n",
    "p(\\mathbf{p} \\mid x) = \\mathrm{Dirichlet}(\\alpha_1(x), \\dots , \\alpha_K(x))\n",
    "$\n",
    "\n",
    "Where:\n",
    "- Each $ \\alpha_k\\ $  represents **evidence** for class \\(k\\).\n",
    "- High $ \\alpha_k\\ $  â†’ strong belief â†’ low epistemic uncertainty\n",
    "- Low $ \\alpha_k\\ $  â†’ weak belief â†’ high epistemic uncertainty\n",
    "\n",
    "## ðŸ”¹ Why Dirichlet?\n",
    "\n",
    "The Dirichlet distribution is the **conjugate prior** of the categorical distribution.\n",
    "This makes it ideal for classification because it models *distributions over class probabilities*.\n",
    "\n",
    "## ðŸ”¹ Predictive Distribution\n",
    "\n",
    "Instead of directly predicting $p(y \\mid x)$, a Prior Network computes:\n",
    "\n",
    "\n",
    "$ p(y \\mid x) = \\int p(y \\mid \\mathbf{p}) \\, p(\\mathbf{p} \\mid x) \\, d\\mathbf{p}$\n",
    "\n",
    "\n",
    "This allows clean decomposition into:\n",
    "\n",
    "### âœ” Total uncertainty\n",
    "### âœ” Data (aleatoric) uncertainty\n",
    "### âœ” Knowledge (epistemic) uncertainty\n",
    "\n",
    "## ðŸ”¹ Why is this useful?\n",
    "\n",
    "âž¡ A single Prior Network can capture **ensemble-like uncertainty**\n",
    "âž¡ Without needing to run an ensemble at inference time\n",
    "âž¡ Saving compute, memory, and latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd4a2-1ee8-4cc9-b3c4-ef40f7e47552",
   "metadata": {},
   "source": [
    "## 4. Transition From Classification to Regression Prior Networks\n",
    "\n",
    "Dirichlet Prior Networks work extremely well for classification.\n",
    "\n",
    "BUT\n",
    "\n",
    "Regression tasks deal with **continuous outputs**, not discrete class probabilities.\n",
    "\n",
    "So we need:\n",
    "- A predictive likelihood: **Normal distribution**\n",
    "- A prior over Normal parameters (mean + precision)\n",
    "\n",
    "The correct conjugate prior to a Normal distribution is:\n",
    "\n",
    "âž¡ **Normalâ€“Wishart distribution**\n",
    "\n",
    "This leads directly to:\n",
    "\n",
    "âœ” Regression Prior Networks\n",
    "âœ” Continuous uncertainty modeling\n",
    "âœ” Student-t predictive distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53133db1-7732-45a2-b110-df09d57abb9c",
   "metadata": {},
   "source": [
    "## 5. Regression Prior Networks (RPNs)\n",
    "\n",
    "Regression Prior Networks generalize Prior Networks to **continuous-valued predictions**.\n",
    "\n",
    "### Step 1 â€” Use a Normal likelihood\n",
    "\n",
    "A normal regression model predicts:\n",
    "\n",
    "\n",
    "$p(y \\mid x, \\mu, \\Lambda) = \\mathcal{N}(y \\mid \\mu, \\Lambda^{-1})$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\mu\\ $ = predicted mean\n",
    "- $ \\Lambda\\ $ = precision (inverse covariance matrix)\n",
    "\n",
    "### Step 2 â€” Predict a *distribution* over (\\mu, \\Lambda)\n",
    "\n",
    "RPNs do NOT predict a single Normal distribution.\n",
    "They predict a **Normalâ€“Wishart prior** over the regression parameters:\n",
    "\n",
    "\n",
    "$(\\mu, \\Lambda) \\sim \\mathrm{NormalWishart}(m, L, \\kappa, \\nu)$\n",
    "\n",
    "Where:\n",
    "- $ m $ = prior mean\n",
    "- $ L $ = prior precision structure\n",
    "- $ \\kappa\\ $ = belief strength in $ m $\n",
    "- $ \\nu\\ $ = belief strength in $ L $\n",
    "\n",
    "These control epistemic uncertainty.\n",
    "\n",
    "### Step 3 â€” Predictive distribution is a Student-t\n",
    "\n",
    "When integrating out the uncertainty in $ \\mu\\ $ and $ \\Lambda\\ $:\n",
    "\n",
    "$\n",
    "p(y \\mid x) = \\mathrm{Student\\text{-}t}(y)\n",
    "$\n",
    "\n",
    "Why Student-t?\n",
    "- Because **Normal likelihood + Normalâ€“Wishart prior = Student-t**.\n",
    "- Student-t has **heavier tails**, which naturally capture uncertainty.\n",
    "\n",
    "### Benefits of Student-t\n",
    "\n",
    "âœ” Detects OOD inputs\n",
    "âœ” Captures both aleatoric + epistemic uncertainty\n",
    "âœ” More robust than a Gaussian\n",
    "\n",
    "## 5.1 The Student-t Distribution\n",
    "\n",
    "When a Regression Prior Network predicts uncertainty, it does not produce a simple Normal\n",
    "distribution. Instead, it produces a **Student-t distribution**, which is the result of:\n",
    "\n",
    "$\n",
    "\\text{Normal likelihood} + \\text{Normalâ€“Wishart prior} \\;\\Rightarrow\\; \\text{Student-t predictive distribution}\n",
    "$\n",
    "\n",
    "This is a fundamental Bayesian identity.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why Student-t instead of Normal?\n",
    "\n",
    "A Normal distribution assumes:\n",
    "- fixed variance\n",
    "- no epistemic uncertainty\n",
    "\n",
    "But in real-world scenarios:\n",
    "- the mean (Î¼) is uncertain\n",
    "- the variance (ÏƒÂ²) is uncertain\n",
    "- the model is uncertain about its parameters\n",
    "\n",
    "A Student-t distribution has *heavier tails*, meaning:\n",
    "\n",
    "- more probability in extreme values\n",
    "- it naturally expresses **model uncertainty**\n",
    "- it becomes wider when the model lacks knowledge\n",
    "- it shrinks toward a Normal distribution when confident\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Shape of a Student-t distribution\n",
    "\n",
    "The parameter **Î½** (nu, degrees of freedom) controls how heavy the tails are:\n",
    "\n",
    "- Small Î½ â†’ *very heavy tails* (uncertain)\n",
    "- Medium Î½ â†’ *moderately heavy* (some uncertainty)\n",
    "- Large Î½ â†’ approaches a Normal distribution (confident)\n",
    "\n",
    "Examples:\n",
    "- Î½ = 1 â†’ Cauchy distribution (extremely heavy-tailed)\n",
    "- Î½ = 3 â†’ high uncertainty\n",
    "- Î½ = 30 â†’ almost Normal\n",
    "- Î½ â†’ âˆž â†’ Normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why is this important for RPNs?\n",
    "\n",
    "Because Student-t naturally captures the behavior of an ensemble:\n",
    "\n",
    "- if models disagree â†’ tail gets heavier â†’ epistemic uncertainty rises\n",
    "- if data is noisy â†’ variance stays large â†’ aleatoric uncertainty\n",
    "- if model is confident â†’ distribution becomes narrow & Gaussian\n",
    "\n",
    "Thus, Student-t is the *perfect* predictive distribution for expressing uncertainty in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c684817-f835-4147-863a-4a63bc9f34d7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "# Values to plot\n",
    "x = np.linspace(-8, 8, 400)\n",
    "\n",
    "# Different degrees of freedom\n",
    "t1 = t.pdf(x, df=1)  # very heavy-tailed\n",
    "t3 = t.pdf(x, df=3)  # moderate uncertainty\n",
    "t30 = t.pdf(x, df=30)  # almost normal\n",
    "\n",
    "normal = norm.pdf(x, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t1, label=\"Student-t (v=1)\", linestyle=\"--\")\n",
    "plt.plot(x, t3, label=\"Student-t (v=3)\", linestyle=\"--\")\n",
    "plt.plot(x, t30, label=\"Student-t (v=30)\", linestyle=\"--\")\n",
    "plt.plot(x, normal, label=\"Normal (mu=0, sigma=1)\", linewidth=2)\n",
    "\n",
    "plt.title(\"Student-t vs Normal Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f37d7a-a1a5-49c6-bbd9-f7e1bdbab532",
   "metadata": {},
   "source": [
    "## 6. Ensemble Distribution Distillation (EnDÂ²)\n",
    "\n",
    "Neural network ensembles are great for uncertainty, but expensive.\n",
    "RPNs support **Ensemble Distribution Distillation (EnDÂ²)** to learn ensemble behavior\n",
    "without needing to run an ensemble at inference.\n",
    "\n",
    "### How EnDÂ² works\n",
    "\n",
    "1. Train an ensemble of regression models\n",
    "2. Collect their predicted $ (\\muáµ, \\Lambdaáµ)$  values\n",
    "3. Treat these values as samples from an empirical distribution\n",
    "4. Train ONE Regression Prior Network to *match this distribution*\n",
    "5. Use **temperature annealing**:\n",
    "   - High T â†’ learn the ensemble mean\n",
    "   - Low T â†’ learn full ensemble diversity (variance, disagreement)\n",
    "\n",
    "### Why this is important?\n",
    "\n",
    "âœ” The RPN learns:\n",
    "- Ensemble mean\n",
    "- Ensemble variance\n",
    "- Ensemble disagreement\n",
    "\n",
    "âž¤ But runs as **one single model** at inference time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c72a0-682f-47a9-8e3c-270b20598619",
   "metadata": {},
   "source": [
    "## 7. Implementation pipeline for Regression Prior Networks (RPN) â€” Ensemble-Based Distillation\n",
    "  we will now implements a complete Regression Prior Network (RPN) using an\n",
    "**ensemble of probabilistic regression models** as the teacher.\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "1. **Train an ensemble**\n",
    "   Each of the K models outputs a Normal distribution\n",
    "   â†’ Î¼â‚–(x), Ïƒâ‚–Â²(x)\n",
    "\n",
    "2. **Collect ensemble distributions**\n",
    "   Aggregate {Î¼â‚–(x), Ïƒâ‚–Â²(x)} from all models\n",
    "\n",
    "3. **Regression Prior Network (RPN)**\n",
    "   RPN outputs the parameters of a Normal-Wishart distribution:\n",
    "   - m(x)\n",
    "   - L(x)\n",
    "   - Îº(x)\n",
    "   - Î½(x)\n",
    "\n",
    "4. **Distillation Loss**\n",
    "   Match the RPN Normal-Wishart distribution to the ensemble's empirical distribution\n",
    "\n",
    "5. **Predictive Student-t distribution**\n",
    "   - total uncertainty\n",
    "   - aleatoric uncertainty\n",
    "   - epistemic uncertainty\n",
    "\n",
    "We will implement everything step by step, with visual tests at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5696eaf-6a79-4ea7-9791-b20a0261f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d516294-1e6f-476d-8755-b226836bb1c9",
   "metadata": {},
   "source": [
    "### 1. Ensemble Probabilistic Regression Models\n",
    "\n",
    "Each ensemble model predicts a **Normal distribution**:\n",
    "\n",
    "$\n",
    "y | x \\sim \\mathcal{N}(\\mu(x), \\sigma^2(x))\n",
    "$\n",
    "\n",
    "For K ensemble members, we obtain:\n",
    "\n",
    "$\n",
    "\\{ (\\mu_k(x), \\sigma_k^2(x)) \\}_{k=1..K}\n",
    "$\n",
    "\n",
    "These Normal distributions will later be distilled into a Normal-Wishart distribution in the RPN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456e302-f2fd-470e-9827-40f0e6355c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    \"\"\"Probabilistic regression model used for building the ensemble.\n",
    "\n",
    "    This model outputs:\n",
    "        - mu(x): predicted mean\n",
    "        - log_var(x): predicted log-variance (to ensure positivity).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the regression model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Size of the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mu_head = nn.Linear(hidden_dim, 1)\n",
    "        self.log_var_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - mu: predicted mean\n",
    "                - log_var: predicted log-variance (var = exp(log_var))\n",
    "        \"\"\"\n",
    "        h = self.feature(x)\n",
    "        mu = self.mu_head(h)\n",
    "        log_var = self.log_var_head(h)\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24277ef8-070a-4186-9385-40e342c2fb42",
   "metadata": {},
   "source": [
    "### 2. Gaussian Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e021a-5ec4-4718-a022-1315218d94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(\n",
    "    mu: torch.Tensor,\n",
    "    log_var: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Gaussian negative log-likelihood.\n",
    "\n",
    "    Computes the NLL for a Gaussian with predicted mean and log-variance:\n",
    "        0.5 * [ log(sigma^2) + (y - mu)^2 / sigma^2 ]\n",
    "    \"\"\"\n",
    "    var = torch.exp(log_var)\n",
    "    return 0.5 * (log_var + (y - mu) ** 2 / var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a635e99-a3e0-47c6-9bc6-6b01929d3c72",
   "metadata": {},
   "source": [
    "### 3. Train a Single Ensemble Member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba963fc-e77f-46cf-b9b1-fd2d7af7a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(\n",
    "    model: nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Train a single regression model using Gaussian NLL loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The regression model to train.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in loader:\n",
    "            mu, log_var = model(x)\n",
    "            loss = gaussian_nll(mu, log_var, y).mean()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss={total_loss / len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66032f9-7232-4961-b30e-349f12f5a9fa",
   "metadata": {},
   "source": [
    "### 4. Build & Train the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fbae5-7b61-45b0-8e60-a64686bc8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble(k: int, input_dim: int) -> list[RegressionModel]:\n",
    "    \"\"\"Build an ensemble of probabilistic regression models.\n",
    "\n",
    "    Args:\n",
    "        k (int): Number of ensemble members.\n",
    "        input_dim (int): Number of input features.\n",
    "\n",
    "    Returns:\n",
    "        list[RegressionModel]: List of trained ensemble models.\n",
    "    \"\"\"\n",
    "    return [RegressionModel(input_dim) for _ in range(k)]\n",
    "\n",
    "\n",
    "def train_ensemble(\n",
    "    ensemble: list[RegressionModel],\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 20,\n",
    ") -> list[RegressionModel]:\n",
    "    \"\"\"Train all models in the ensemble.\n",
    "\n",
    "    Args:\n",
    "        ensemble (list[RegressionModel]): List of models to train.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        list[RegressionModel]: List of trained ensemble models.\n",
    "    \"\"\"\n",
    "    trained = []\n",
    "\n",
    "    for i, model in enumerate(ensemble):\n",
    "        print(f\"\\nTraining Ensemble Model {i + 1}/{len(ensemble)}\")\n",
    "        trained_model = train_single_model(model, loader, epochs)\n",
    "        trained.append(trained_model)\n",
    "\n",
    "    return trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf599362-cd42-4974-9dd4-e2608821ea65",
   "metadata": {},
   "source": [
    "### 5. Synthetic Dataset for training the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a5aa4-cd2c-4ec5-97cc-c326618f8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "y = torch.sin(x) + 0.2 * torch.randn_like(x)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d05b03-8f42-44c2-a022-f502851ed91c",
   "metadata": {},
   "source": [
    "### 6. Train the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe7a20-b231-4b52-b4bf-59d74f0c8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = build_ensemble(k=5, input_dim=1)\n",
    "ensemble = train_ensemble(ensemble, loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753aacb-c0f9-49db-8ee2-40541368c14c",
   "metadata": {},
   "source": [
    "### 7. Collect Î¼â‚– and Ïƒâ‚–Â², this are the Ensemble Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab8922-6be0-477b-aaff-fd15f0a05cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_distributions(\n",
    "    ensemble: list[RegressionModel],\n",
    "    x: torch.Tensor,\n",
    ") -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n",
    "    \"\"\"Compute ensemble predictive distributions.\n",
    "\n",
    "    Args:\n",
    "        ensemble (list[RegressionModel]): List of trained ensemble models.\n",
    "        x (torch.Tensor): Input batch to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[torch.Tensor], list[torch.Tensor]]:\n",
    "            - List of predicted means from each ensemble member.\n",
    "            - List of predicted variances from each ensemble member.\n",
    "    \"\"\"\n",
    "    mus: list[torch.Tensor] = []\n",
    "    vars_: list[torch.Tensor] = []\n",
    "\n",
    "    for model in ensemble:\n",
    "        model.eval()\n",
    "        mu, log_var = model(x)\n",
    "        mus.append(mu.detach())\n",
    "        vars_.append(torch.exp(log_var.detach()))\n",
    "\n",
    "    return mus, vars_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab60d-fbe8-48b4-9562-bb28949a9e55",
   "metadata": {},
   "source": [
    "### 8. Regression Prior Network (RPN)\n",
    "\n",
    "The RPN outputs parameters of a **Normal-Wishart distribution**, which is a\n",
    "distribution *over Normal distributions from the Ensemble*:\n",
    "\n",
    "- m(x) : prior mean for Î¼\n",
    "- L(x) : precision parameter\n",
    "- Îº(x) : evidence about Î¼\n",
    "- Î½(x) : evidence about Î£\n",
    "\n",
    "From the Normal-Wishart parameters, the predictive distribution becomes a\n",
    "Student-t distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4d7a9-4635-442f-8531-9c41d96f2ed2",
   "metadata": {},
   "source": [
    "### 9. RPN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14ad2a-3a96-455d-a022-3fa9e477d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionPriorNetwork(nn.Module):\n",
    "    \"\"\"Regression Prior Network: outputs Normal-Wishart parameters.\n",
    "\n",
    "    This implementation uses a univariate Normal-Wishart distribution for\n",
    "    evidential regression. It predicts the four parameters:\n",
    "    - m(x): location parameter (prior mean)\n",
    "    - l_precision(x): precision (must be > 0)\n",
    "    - kappa(x): strength of belief in m\n",
    "    - nu(x): degrees of freedom (> 2, controls heaviness of Student-t tails).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the Regression Prior Network.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Hidden layer width.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Heads for the four Normal-Wishart parameters\n",
    "        self.m_head = nn.Linear(hidden_dim, 1)\n",
    "        self.l_precision_head = nn.Linear(hidden_dim, 1)\n",
    "        self.kappa_head = nn.Linear(hidden_dim, 1)\n",
    "        self.nu_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Forward pass for predicting Normal-Wishart parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - m (torch.Tensor): Prior mean.\n",
    "                - l_precision (torch.Tensor): Precision (> 0).\n",
    "                - kappa (torch.Tensor): Strength (> 0).\n",
    "                - nu (torch.Tensor): Degrees of freedom (> 2).\n",
    "        \"\"\"\n",
    "        h = self.feature(x)\n",
    "\n",
    "        m = self.m_head(h)\n",
    "\n",
    "        # Rename L â†’ l_precision to satisfy Ruff rule N806\n",
    "        l_precision = torch.exp(self.l_precision_head(h))  # must be > 0\n",
    "\n",
    "        # kappa must be strictly positive\n",
    "        kappa = F.softplus(self.kappa_head(h)) + 1e-3\n",
    "\n",
    "        # nu must be > 2 to define a valid Student-t distribution\n",
    "        nu = F.softplus(self.nu_head(h)) + 3.0\n",
    "\n",
    "        return m, l_precision, kappa, nu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a26655-9884-47f8-8acc-62844cb8a07b",
   "metadata": {},
   "source": [
    "### 10. Distillation Loss\n",
    "\n",
    "We distill the ensemble Normal distributions\n",
    "$\n",
    "(\\mu_k, \\sigma_k^2)\n",
    "$\n",
    "into a single Normal-Wishart distribution predicted by the RPN.\n",
    "\n",
    "Loss:\n",
    "$\n",
    "L = -\\frac{1}{K}\\sum_k \\log p_{NW}(\\mu_k, \\sigma_k^2)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e0d6e-7728-4bcd-82ef-76a6f2a0cb95",
   "metadata": {},
   "source": [
    "### 11. Normal-Wishart Log-Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10892afd-2382-438a-962a-ea17aea36626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def normal_wishart_log_prob(\n",
    "    m: Tensor,\n",
    "    l_precision: Tensor,\n",
    "    kappa: Tensor,\n",
    "    nu: Tensor,\n",
    "    mu_k: Tensor,\n",
    "    sigma2_k: Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute simplified univariate Normal-Wishart log-likelihood.\n",
    "\n",
    "    Args:\n",
    "        m (Tensor): Prior mean parameter.\n",
    "        l_precision (Tensor): Precision (> 0), formerly `L`.\n",
    "        kappa (Tensor): Strength parameter (> 0).\n",
    "        nu (Tensor): Degrees of freedom (> 2).\n",
    "        mu_k (Tensor): Sample mean from ensemble.\n",
    "        sigma2_k (Tensor): Sample variance from ensemble.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Log-likelihood under the Normal-Wishart model.\n",
    "    \"\"\"\n",
    "    # Likelihood of ensemble mean under Normal prior for mean\n",
    "    log_p_mu = -0.5 * kappa * l_precision * (mu_k - m) ** 2\n",
    "\n",
    "    # Likelihood of variance under Wishart prior on precision\n",
    "    log_p_sigma = 0.5 * (nu - 1) * torch.log(l_precision) - 0.5 * nu * (sigma2_k * l_precision)\n",
    "\n",
    "    return log_p_mu + log_p_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf6f59-6712-400e-abae-237e14336fa0",
   "metadata": {},
   "source": [
    "### 12. Distillation Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0538bc-7b2f-4dae-badb-19600f32668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def rpn_distillation_loss(\n",
    "    rpn_params: tuple[Tensor, Tensor, Tensor, Tensor],\n",
    "    mus: list[Tensor],\n",
    "    variances: list[Tensor],\n",
    ") -> Tensor:\n",
    "    \"\"\"Compute the distillation loss for Regression Prior Networks (RPN).\n",
    "\n",
    "    This loss measures how well the RPN's Normal-Wishart distribution\n",
    "    matches the empirical ensemble distributions (mu_k, var_k).\n",
    "\n",
    "    Args:\n",
    "        rpn_params (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
    "            The RPN output parameters (m, l_precision, kappa, nu).\n",
    "        mus (list[Tensor]): Ensemble predicted means.\n",
    "        variances (list[Tensor]): Ensemble predicted variances.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss value.\n",
    "    \"\"\"\n",
    "    m, l_precision, kappa, nu = rpn_params  # formerly \"L\"\n",
    "\n",
    "    losses: list[Tensor] = []\n",
    "\n",
    "    for mu_k, var_k in zip(mus, variances, strict=False):\n",
    "        log_prob = normal_wishart_log_prob(\n",
    "            m,\n",
    "            l_precision,\n",
    "            kappa,\n",
    "            nu,\n",
    "            mu_k,\n",
    "            var_k,\n",
    "        )\n",
    "        losses.append(-log_prob.mean())  # negative log-likelihood\n",
    "\n",
    "    return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28360fe5-dd29-47f5-894b-827bff96602f",
   "metadata": {},
   "source": [
    "### 13. RPN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f1d51-1e49-4368-98f6-4a20604349d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_rpn(\n",
    "    rpn: RegressionPriorNetwork,\n",
    "    ensemble: list[RegressionModel],\n",
    "    loader: DataLoader,\n",
    "    epochs: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"Train the Regression Prior Network (RPN) using ensemble distillation.\n",
    "\n",
    "    Args:\n",
    "        rpn (RegressionPriorNetwork): The RPN model to train.\n",
    "        ensemble (list[RegressionModel]): Ensemble of trained regression models.\n",
    "        loader (DataLoader): Training data loader.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(rpn.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, _ in loader:\n",
    "            mus, vars_ = get_ensemble_distributions(ensemble, x)\n",
    "\n",
    "            rpn_params = rpn(x)\n",
    "            loss = rpn_distillation_loss(rpn_params, mus, vars_)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: RPN Loss={total_loss / len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53433f-d48c-4f23-bece-3a6fc3c49d6c",
   "metadata": {},
   "source": [
    "### 14. Train the RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2305717-c814-4cbc-a4be-ebcff78db338",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = RegressionPriorNetwork(input_dim=1)\n",
    "train_rpn(rpn, ensemble, loader, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32e353-8144-49da-9b9e-8d4d0c672eba",
   "metadata": {},
   "source": [
    "### 15. Student-t Predictive Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1762cd-bbbe-47d5-adc6-c1c59b2a9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def student_t_prediction(\n",
    "    m: Tensor,\n",
    "    l_precision: Tensor,\n",
    "    kappa: Tensor,\n",
    "    nu: Tensor,\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Compute predictive Student-t distribution parameters from Normal-Wishart posterior.\n",
    "\n",
    "    Args:\n",
    "        m (Tensor): Posterior mean parameter.\n",
    "        l_precision (Tensor): Posterior precision parameter (> 0).\n",
    "        kappa (Tensor): Posterior scaling parameter (> 0).\n",
    "        nu (Tensor): Posterior degrees of freedom (> 2).\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor, Tensor]:\n",
    "            - mean of Student-t distribution\n",
    "            - variance of Student-t distribution\n",
    "            - degrees of freedom (df)\n",
    "    \"\"\"\n",
    "    df = nu - 1\n",
    "    var = (kappa + 1) / (kappa * l_precision * df)\n",
    "\n",
    "    return m, var, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d2b09-f267-4d3a-91bf-a55e5dd17352",
   "metadata": {},
   "source": [
    "### 16. RPN Output testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f705f-78b1-4722-a93c-ae34d1408c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "m, L, kappa, nu = rpn(x_test)\n",
    "\n",
    "m_pred, var_pred, df = student_t_prediction(m, L, kappa, nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85672b9f-1058-4517-bb0b-fc7016699127",
   "metadata": {},
   "source": [
    "### 17. Plotten von Mean & Unsicherheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec87cdc-f6b8-4a9c-96f7-26be8f315c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Convert x and y to numpy for plotting\n",
    "plt.plot(\n",
    "    x.detach().cpu().numpy(),\n",
    "    y.detach().cpu().numpy(),\n",
    "    \"k.\",\n",
    "    alpha=0.3,\n",
    "    label=\"Data\",\n",
    ")\n",
    "\n",
    "# RPN mean\n",
    "plt.plot(\n",
    "    x_test.detach().cpu().numpy(),\n",
    "    m_pred.detach().cpu().numpy(),\n",
    "    \"b-\",\n",
    "    label=\"RPN mean\",\n",
    ")\n",
    "\n",
    "# Standard deviation\n",
    "std = var_pred.sqrt()\n",
    "\n",
    "# Fill between (convert EVERYTHING to numpy)\n",
    "plt.fill_between(\n",
    "    x_test.squeeze().detach().cpu().numpy(),\n",
    "    (m_pred - 2 * std).squeeze().detach().cpu().numpy(),\n",
    "    (m_pred + 2 * std).squeeze().detach().cpu().numpy(),\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"Â±2 std RPN\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"RPN Prediction + Uncertainty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7472499-6c8d-4191-b078-a99cf0c93850",
   "metadata": {},
   "source": [
    "## 8.Implementation of a unified Regression Model inspired by the papers(Deep evidential Regression and  Regression Prior Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815ae13-7c31-4234-99fe-82e916940dd8",
   "metadata": {},
   "source": [
    "### Unified Evidential Regression (Short Explanation)\n",
    "\n",
    "The unified evidential regression model combines **Deep Evidential Regression (DER)** and  \n",
    "**Regression Prior Networks (RPN)** to handle both *in-distribution* and *out-of-distribution* uncertainty.\n",
    "\n",
    "###  DER (Amini et al., 2020)\n",
    "- Predicts Normalâ€“Inverse-Gamma parameters: (Î¼, Îº, Î±, Î²)  \n",
    "- Learns regression + uncertainty from data  \n",
    "- Works well **inside** the training distribution  \n",
    "- Limitation: becomes overconfident **outside** the training region\n",
    "\n",
    "###  RPN (Malinin & Gales, 2021)\n",
    "- Defines a **zero-evidence prior** for OOD inputs  \n",
    "- Uses **KL-divergence** to force the model to output low evidence outside the data  \n",
    "- Produces high **epistemic uncertainty** in unseen regions  \n",
    "- Limitation: cannot learn regression by itself\n",
    "\n",
    "###  Unified Model\n",
    "We apply:\n",
    "$\n",
    "L = L_{\\text{DER}}(\\text{ID data}) + \\lambda_{\\text{RPN}} \\cdot L_{\\text{KL}}(\\text{OOD data})\n",
    "$\n",
    "\n",
    "- **DER loss** trains regression + in-distribution uncertainty  \n",
    "- **RPN KL loss** forces uncertainty to grow in OOD regions  \n",
    "\n",
    "###  Result\n",
    "A single model that:\n",
    "- fits the data well (DER),\n",
    "- is confident where it has evidence,\n",
    "- becomes highly uncertain where no data exists (RPN),\n",
    "- works **without ensembles**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029d678-587c-4cc0-b55d-55ca877336b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class EvidentialRegression(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the neural network layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tuple of four tensors:\n",
    "                mu: Mean parameter of Normal-Gamma\n",
    "                kappa: Strength of belief in mean (>= 0)\n",
    "                alpha: Shape parameter (> 1)\n",
    "                beta: Scale parameter (> 0)\n",
    "        \"\"\"\n",
    "        out = self.layers(x)\n",
    "\n",
    "        mu = out[:, 0:1]\n",
    "        kappa = F.softplus(out[:, 1:2])\n",
    "        alpha = F.softplus(out[:, 2:3]) + 1.0\n",
    "        beta = F.softplus(out[:, 3:4])\n",
    "\n",
    "        return mu, kappa, alpha, beta\n",
    "\n",
    "\n",
    "def der_loss(\n",
    "    y: Tensor,\n",
    "    mu: Tensor,\n",
    "    kappa: Tensor,\n",
    "    alpha: Tensor,\n",
    "    beta: Tensor,\n",
    "    lam: float = 0.01,\n",
    ") -> Tensor:\n",
    "    \"\"\"Deep Evidential Regression loss (Student-t NLL + evidence regularizer).\"\"\"\n",
    "    eps = 1e-8\n",
    "    two_bv = 2.0 * beta * (1.0 + kappa) + eps\n",
    "\n",
    "    lnll = (\n",
    "        0.5 * torch.log(torch.pi / (kappa + eps))\n",
    "        - alpha * torch.log(two_bv)\n",
    "        + (alpha + 0.5) * torch.log(kappa * (y - mu).pow(2) + two_bv)\n",
    "        + torch.lgamma(alpha)\n",
    "        - torch.lgamma(alpha + 0.5)\n",
    "    )\n",
    "\n",
    "    evidence = 2.0 * kappa + alpha\n",
    "    reg = torch.abs(y - mu) * evidence\n",
    "\n",
    "    return (lnll + lam * reg).mean()\n",
    "\n",
    "\n",
    "def rpn_prior(\n",
    "    shape: torch.Size | tuple[int, ...],\n",
    "    device: torch.device,\n",
    ") -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "    eps = 1e-6\n",
    "    mu0 = torch.zeros(shape, device=device)\n",
    "    kappa0 = torch.ones(shape, device=device) * eps\n",
    "    alpha0 = torch.ones(shape, device=device) * (1.0 + eps)\n",
    "    beta0 = torch.ones(shape, device=device) * eps\n",
    "    return mu0, kappa0, alpha0, beta0\n",
    "\n",
    "\n",
    "def rpn_ng_kl(\n",
    "    mu: Tensor,\n",
    "    kappa: Tensor,\n",
    "    alpha: Tensor,\n",
    "    beta: Tensor,\n",
    "    mu0: Tensor,\n",
    "    kappa0: Tensor,\n",
    "    alpha0: Tensor,\n",
    "    beta0: Tensor,\n",
    ") -> Tensor:\n",
    "    eps = 1e-8\n",
    "\n",
    "    kappa = kappa + eps\n",
    "    kappa0 = kappa0 + eps\n",
    "    beta = beta + eps\n",
    "    beta0 = beta0 + eps\n",
    "\n",
    "    ratio_kappa = kappa / kappa0\n",
    "\n",
    "    term_mu = 0.5 * (alpha / beta) * kappa0 * (mu - mu0).pow(2)\n",
    "    term_kappa = 0.5 * (ratio_kappa - torch.log(ratio_kappa) - 1.0)\n",
    "    term_gamma = (\n",
    "        alpha0 * torch.log(beta / beta0)\n",
    "        - torch.lgamma(alpha)\n",
    "        + torch.lgamma(alpha0)\n",
    "        + (alpha - alpha0) * torch.digamma(alpha)\n",
    "        - (beta - beta0) * (alpha / beta)\n",
    "    )\n",
    "\n",
    "    return (term_mu + term_kappa + term_gamma).mean()\n",
    "\n",
    "\n",
    "def unified_loss(\n",
    "    y: Tensor,\n",
    "    mu: Tensor,\n",
    "    kappa: Tensor,\n",
    "    alpha: Tensor,\n",
    "    beta: Tensor,\n",
    "    is_ood: Tensor,\n",
    "    lam_der: float = 0.01,\n",
    "    lam_rpn: float = 50.0,\n",
    ") -> Tensor:\n",
    "    is_ood = is_ood.bool()\n",
    "    id_mask = ~is_ood\n",
    "    ood_mask = is_ood\n",
    "\n",
    "    device = y.device\n",
    "\n",
    "    loss_id = torch.tensor(0.0, device=device)\n",
    "    loss_ood = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # --- DER loss for ID samples ---\n",
    "    if id_mask.any():\n",
    "        loss_id = der_loss(\n",
    "            y[id_mask],\n",
    "            mu[id_mask],\n",
    "            kappa[id_mask],\n",
    "            alpha[id_mask],\n",
    "            beta[id_mask],\n",
    "            lam=lam_der,\n",
    "        )\n",
    "\n",
    "    # --- KL loss for OOD samples ---\n",
    "    if ood_mask.any():\n",
    "        shape = mu[ood_mask].shape\n",
    "        mu0, kappa0, alpha0, beta0 = rpn_prior(shape, device)\n",
    "\n",
    "        loss_ood = rpn_ng_kl(\n",
    "            mu[ood_mask],\n",
    "            kappa[ood_mask],\n",
    "            alpha[ood_mask],\n",
    "            beta[ood_mask],\n",
    "            mu0,\n",
    "            kappa0,\n",
    "            alpha0,\n",
    "            beta0,\n",
    "        )\n",
    "\n",
    "    return loss_id + lam_rpn * loss_ood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da126f-34c1-4470-8a0d-d9544f3611be",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd73f2-2ee0-4583-9af3-818b76fbbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets() -> tuple[Tensor, Tensor, Tensor]:\n",
    "    # In-domain samples\n",
    "    x_id = torch.linspace(-3, 3, 300).unsqueeze(-1)\n",
    "    y_id = torch.sin(x_id) + 0.1 * torch.randn_like(x_id)\n",
    "\n",
    "    # Out-of-domain samples far away from the sin input range\n",
    "    x_ood = torch.linspace(8, 15, 100).unsqueeze(-1)\n",
    "    y_ood = torch.zeros_like(x_ood)\n",
    "\n",
    "    x = torch.cat([x_id, x_ood], dim=0)\n",
    "    y = torch.cat([y_id, y_ood], dim=0)\n",
    "\n",
    "    # OOD mask\n",
    "    is_ood = torch.cat(\n",
    "        [\n",
    "            torch.zeros(len(x_id), dtype=torch.bool),\n",
    "            torch.ones(len(x_ood), dtype=torch.bool),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return x, y, is_ood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfbd57-5199-428b-ac1f-508e1fd44932",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bb94b-d972-4c2b-84b6-eaf45c08ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    epochs: int = 2000,\n",
    "    lr: float = 1e-3,\n",
    ") -> nn.Module:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch, is_ood in loader:\n",
    "            mu, kappa, alpha, beta = model(x_batch)\n",
    "\n",
    "            loss = unified_loss(\n",
    "                y_batch,\n",
    "                mu,\n",
    "                kappa,\n",
    "                alpha,\n",
    "                beta,\n",
    "                is_ood,\n",
    "                lam_der=0.01,\n",
    "                lam_rpn=50.0,  # Strong KL for OOD uncertainty reinforcement\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Optional: print training progress\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"[Epoch {epoch}] Loss = {loss.item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b20d4e-38c5-42da-83e8-2e8e3ce4b0d1",
   "metadata": {},
   "source": [
    "### Plot der Unisicherheit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bcf99-a0aa-4189-adbd-ddb209f421d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty(\n",
    "    model: nn.Module,\n",
    "    x: Tensor,\n",
    "    y: Tensor,\n",
    "    is_ood: Tensor,\n",
    ") -> None:\n",
    "    \"\"\"Plot model predictions and predictive uncertainty for unified evidential regression.\n",
    "\n",
    "    Args:\n",
    "        model: Trained evidential regression model.\n",
    "        x: Input tensor of shape (N, 1).\n",
    "        y: Target tensor of shape (N, 1).\n",
    "        is_ood: Boolean mask indicating which samples are out-of-distribution.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a matplotlib figure.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        mu, _kappa, alpha, beta = model(x)\n",
    "        std = torch.sqrt(beta / (alpha - 1.0))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # In-domain points\n",
    "    plt.scatter(x[~is_ood], y[~is_ood], s=8, label=\"ID Data\")\n",
    "\n",
    "    # Out-of-domain points\n",
    "    plt.scatter(x[is_ood], y[is_ood], s=8, color=\"red\", label=\"OOD Data\")\n",
    "\n",
    "    # Mean prediction\n",
    "    plt.plot(x, mu, \"k\", label=\"Prediction\")\n",
    "\n",
    "    # Uncertainty band\n",
    "    plt.fill_between(\n",
    "        x.squeeze(),\n",
    "        (mu - 2.0 * std).squeeze(),\n",
    "        (mu + 2.0 * std).squeeze(),\n",
    "        alpha=0.3,\n",
    "        color=\"orange\",\n",
    "        label=\"Uncertainty\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Unified Evidential Regression â€” Predictive Uncertainty\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f6f1c-06f9-4315-9eda-83ee6ec3bdf2",
   "metadata": {},
   "source": [
    "### Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69147f6b-f310-4c14-87cb-8730dce27954",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, Y, is_ood = make_datasets()\n",
    "\n",
    "    dataset = TensorDataset(X, Y, is_ood)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = EvidentialRegression()\n",
    "    model = train(model, loader)\n",
    "\n",
    "    plot_uncertainty(model, X, Y, is_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2877717-8879-429b-8604-e01a74325847",
   "metadata": {},
   "source": [
    "## 9. Unified Evidential Regression on MNIST â€” Practical Demonstration\n",
    "\n",
    "In this section, we test the unified evidential regression model on a real-world dataset.  \n",
    "We combine:\n",
    "\n",
    "- **Deep Evidential Regression (DER)** â†’ learns uncertainty directly from in-distribution data  \n",
    "- **Regression Prior Networks (RPN)** â†’ enforces high epistemic uncertainty for out-of-distribution (OOD) inputs  \n",
    "- **FashionMNIST as OOD samples** â†’ simulates unfamiliar inputs  \n",
    "\n",
    "The unified model uses the combined loss:\n",
    "\n",
    "$\n",
    "L = L_{\\text{DER}}(\\text{ID}) + \\lambda_{\\text{RPN}} \\cdot KL(\\text{OOD})\n",
    "$\n",
    "\n",
    "### What does this achieve?\n",
    "\n",
    "| Component | Purpose |\n",
    "|----------|---------|\n",
    "| **DER loss** | Learns regression + aleatoric uncertainty |\n",
    "| **RPN KL loss** | Forces epistemic uncertainty to increase on OOD inputs |\n",
    "| **is\\_ood mask** | Tells the model which batch elements are OOD |\n",
    "| **Unified loss** | Merges both mechanisms into a single coherent approach |\n",
    "\n",
    "### Experiment overview\n",
    "1. MNIST digits are turned into a **regression problem** (1D input â†’ normalized digit label).  \n",
    "2. FashionMNIST is used as **OOD data**.  \n",
    "3. The unified evidential model is trained to be confident where it has evidence and uncertain where it lacks evidence.  \n",
    "4. The predictive mean and uncertainty are visualized\n",
    "\n",
    " **A single model now behaves like an ensemble + prior-driven OOD regulator â€” without needing an actual ensemble.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42ff1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded (ID).\n",
      "Loaded datasets with 60000 samples.\n"
     ]
    }
   ],
   "source": [
    "# test mit unified function\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNIST1DRegression(Dataset):\n",
    "    def __init__(self, mnist_dataset: Dataset) -> None:  # noqa: D107\n",
    "        self.mnist = mnist_dataset\n",
    "\n",
    "    def __len__(self) -> int:  # noqa: D105\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:  # noqa: D105\n",
    "        x, y = self.mnist[idx]\n",
    "\n",
    "        # 1D input\n",
    "        x_1d = x.mean().view(1)\n",
    "\n",
    "        # convert int -> tensor\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # regression target\n",
    "        y_reg = y / 9.0\n",
    "\n",
    "        return x_1d, y_reg.view(1)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# In-distribution data\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_data = MNIST1DRegression(train_data)\n",
    "test_data = MNIST1DRegression(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"MNIST loaded (ID).\")\n",
    "\n",
    "# Out-of-distribution data\n",
    "ood_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"~/datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "ood_data = MNIST1DRegression(ood_data)\n",
    "\n",
    "ood_loader = DataLoader(ood_data, batch_size=256, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Loaded datasets with {len(train_data)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from probly.losses.evidential.torch import rpn_loss\n",
    "from probly.models.evidential.torch import EvidentialRegressionModel\n",
    "from probly.train.evidential.torch import unified_evidential_train\n",
    "\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    \"\"\"Simple MLP encoder used to transform inputs into feature embeddings.\n",
    "\n",
    "    This module contains no evidential logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 1, hidden_dim: int = 64, latent_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Size of input features.\n",
    "            hidden_dim: Number of neurons in hidden layers.\n",
    "            latent_dim: Dimension of the output feature representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute feature embedding.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Feature tensor of shape (N, feature_dim).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "enc = MLPEncoder()\n",
    "\n",
    "model = EvidentialRegressionModel(encoder=enc)\n",
    "unified_evidential_train(mode=\"RPN\", model=model, dataloader=train_loader, loss_fn=rpn_loss, oodloader=ood_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc01e6-bb55-4ccc-987c-880ab235be37",
   "metadata": {},
   "source": [
    "## 10. Encoder + Evidential Head for Evidential Regression\n",
    "\n",
    "In our unified evidential framework, we separate the model into two logical components:\n",
    "\n",
    "### **1. Encoder**\n",
    "The encoder maps the raw input features into a learned representation (feature embedding).\n",
    "This module can be replaced by any architecture (MLP, CNN, Transformer), as long as it\n",
    "returns a feature vector of fixed size.  \n",
    "It contains **no evidential logic**, only feature extraction.\n",
    "\n",
    "### **2. Evidential Head**\n",
    "The evidential head converts the feature vector into the four Normalâ€“Gamma parameters:\n",
    "\n",
    "- **Î¼ (mu)**: predicted mean  \n",
    "- **Îº (kappa)**: strength of belief in the mean  \n",
    "- **Î± (alpha)**: shape parameter  \n",
    "- **Î² (beta)**: scale parameter  \n",
    "\n",
    "These form the Normalâ€“Gamma distribution used by DER and RPN.\n",
    "\n",
    "### **3. Full Model**\n",
    "The final model simply combines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663929b5-61bb-4b61-b8ff-1dc651c1c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. General MLP Encoder\n",
    "# ------------------------------\n",
    "class MLPEncoder(nn.Module):\n",
    "    \"\"\"Simple MLP encoder used to transform inputs into feature embeddings.\n",
    "\n",
    "    This module contains no evidential logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 1, hidden_dim: int = 64, latent_dim: int = 64) -> None:\n",
    "        \"\"\"Initialize the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Size of input features.\n",
    "            hidden_dim: Number of neurons in hidden layers.\n",
    "            latent_dim: Dimension of the output feature representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Compute feature embedding.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (N, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            Feature tensor of shape (N, feature_dim).\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Evidential Head\n",
    "# ------------------------------\n",
    "class EvidentialHead(nn.Module):\n",
    "    \"\"\"Head that converts encoded features into evidential Normal-Gamma parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        \"\"\"Initialize the head.\n",
    "\n",
    "        Args:\n",
    "            latent_dim: Dimension of input features coming from the encoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(latent_dim, 4)\n",
    "\n",
    "    def forward(self, features: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Convert features into (mu, kappa, alpha, beta).\n",
    "\n",
    "        Args:\n",
    "            features: Feature tensor (N, feature_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of four tensors representing Normal-Gamma parameters.\n",
    "        \"\"\"\n",
    "        raw = self.linear(features)\n",
    "\n",
    "        mu = raw[:, 0:1]\n",
    "        kappa = F.softplus(raw[:, 1:2])\n",
    "        alpha = F.softplus(raw[:, 2:3]) + 1.0\n",
    "        beta = F.softplus(raw[:, 3:4])\n",
    "\n",
    "        return mu, kappa, alpha, beta\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# 3. Full adjusted Evidential Regression Model to fit with the Encoder andd Head\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class EvidentialRegressionModel(nn.Module):\n",
    "    \"\"\"Full evidential regression model combining encoder and evidential head.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder: nn.Module) -> None:\n",
    "        \"\"\"Initialize the full model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = EvidentialHead(latent_dim=encoder.latent_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Forward pass through encoder and head.\"\"\"\n",
    "        features = self.encoder(x)\n",
    "        return self.head(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95e013-1095-415d-9860-4f8c80e91f7d",
   "metadata": {},
   "source": [
    "## 11. Comparison: Ensemble RPN vs. True RPN vs. DER vs. Unified Evidential Model\n",
    "\n",
    "### Ensemble-based RPN\n",
    "- Excellent epistemic uncertainty  \n",
    "- Behaves very well under distributional shift  \n",
    "- But computationally expensive: K models at training AND inference  \n",
    "\n",
    "---\n",
    "\n",
    "### Regression Prior Networks (Malinin & Gales, 2021)\n",
    "- Predict a Normalâ€“Wishart distribution  \n",
    "- Produce **Student-t predictive distributions**  \n",
    "- Naturally detect OOD inputs  \n",
    "- Do *not* require ensembles  \n",
    "- But: epistemic uncertainty is structural, not data-driven  \n",
    "\n",
    "---\n",
    "\n",
    "### Deep Evidential Regression (Amini et al., 2020)\n",
    "- Predict Normalâ€“Inverse-Gamma parameters (Î¼, Îº, Î±, Î²)  \n",
    "- Learns aleatoric + epistemic uncertainty from data  \n",
    "- Works extremely well for ID regions  \n",
    "- Can become overconfident for OOD regions  \n",
    "\n",
    "---\n",
    "\n",
    "### Unified Evidential Regression (UER)\n",
    "\n",
    "This model combines the strengths of both papers:\n",
    "\n",
    "- DER for in-distribution learning + aleatoric uncertainty  \n",
    "- RPN-KL for epistemic uncertainty growth on OOD samples  \n",
    "- Student-t predictive structure  \n",
    "- No ensemble needed  \n",
    "\n",
    "#### Full Comparison\n",
    "\n",
    "| Aspect | Ensemble RPN | RPN | DER | Unified Model |\n",
    "|--------|--------------|------|------|----------------|\n",
    "| Requires ensemble | âœ”ï¸ | âŒ | âŒ | âŒ |\n",
    "| Aleatoric uncertainty | medium | moderate | **excellent** | **excellent** |\n",
    "| Epistemic uncertainty | **excellent** | high | moderate | **excellent** |\n",
    "| OOD behavior | strong | **very strong** | weak | **best** |\n",
    "| Predictive form | Student-t | Student-t | Gaussian / NIG | Student-t-like |\n",
    "| Computational cost | very high | low | low | low |\n",
    "\n",
    "---\n",
    "\n",
    "#### Why the Unified Model is Better Than Ensemble-RPN\n",
    "\n",
    "- Matches ensemble epistemic behavior **without K models**  \n",
    "- Provides better aleatoric modeling (from DER)  \n",
    "- Stronger and more controlled OOD uncertainty (from RPN-KL)  \n",
    "- Single-model deployment, faster inference  \n",
    "- More stable training  \n",
    "- Student-t predictive distribution emerges naturally  \n",
    "\n",
    "**The unified model is essentially an ensemble-quality uncertainty estimator that is cheaper, simpler, and more robust.**\n",
    "at inspired RPN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a12da-e5e4-4568-933a-0c36c7d8d622",
   "metadata": {},
   "source": [
    "## 12. Conclusion â€” A Unified Perspective on Uncertainty\n",
    "\n",
    "In this notebook we explored three major approaches for uncertainty in deep regression, and finally merged them into a unified evidential framework.\n",
    "\n",
    "### 1. Ensembles\n",
    "- Provide excellent epistemic uncertainty  \n",
    "- But expensive: multiple models, high compute + memory  \n",
    "\n",
    "### 2. Regression Prior Networks (RPN)\n",
    "- Learn a **Normalâ€“Wishart prior** instead of point estimates  \n",
    "- Produce **Student-t predictive distributions**, ideal for OOD  \n",
    "- Much cheaper than ensembles  \n",
    "\n",
    "### 3. Deep Evidential Regression (DER)\n",
    "- Learns aleatoric + epistemic uncertainty from data  \n",
    "- Strong performance inside the training distribution  \n",
    "- Overconfident outside the training range  \n",
    "\n",
    "---\n",
    "\n",
    "### Unified Evidential Regression (UER)\n",
    "\n",
    "By combining DER and RPN, we obtain a model that:\n",
    "\n",
    "- fits the data well (DER),\n",
    "- expresses meaningful epistemic uncertainty (RPN),\n",
    "- becomes highly uncertain on unfamiliar inputs,\n",
    "- does **not** require ensembles,\n",
    "- provides **Student-t-like predictive behavior**.\n",
    "\n",
    "\n",
    "$L = L_{\\text{DER}} + \\lambda_{\\text{RPN}} \\cdot KL_{\\text{OOD}}\n",
    "$\n",
    "\n",
    "#### Result\n",
    "\n",
    "| Capability | Ensemble | RPN | DER | Unified Model |\n",
    "|------------|----------|-----|------|----------------|\n",
    "| Aleatoric uncertainty | medium | good | **excellent** | **excellent** |\n",
    "| Epistemic uncertainty | excellent | excellent | medium | **excellent** |\n",
    "| OOD detection | good | **very strong** | weak | **excellent** |\n",
    "| Computational cost | very high | low | low | low |\n",
    "| Requires multiple models | âœ”ï¸ | âŒ | âŒ | âŒ |\n",
    "\n",
    "**Unified Evidential Regression provides ensemble-level uncertainty at a fraction of the cost.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
