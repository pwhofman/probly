{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6224021b",
   "metadata": {},
   "source": [
    "# Bayesian Transformation \n",
    "\n",
    "This notebook is a practical introduction to the Bayesian transformation in `probly`. Bayesian Neural Networks are a more advanced topic than Dropout or DropConnect,\n",
    "so this tutorial aims to provide an intuitive, hands-on understanding.\n",
    "\n",
    "We will start by explaining the core idea behind Bayesian Neural Networks (BNNs) and then see how the `probly` transformation enables you to create them. After that, we will look at a PyTorch example to inspect the transformed model and use it to estimate uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a7f1e",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: Introduction to BNNs and the Bayesian Transformation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4b1b6",
   "metadata": {},
   "source": [
    "## 1.Concept: What is a Bayesian Neural Network?\n",
    "\n",
    "To understand the Bayesian transformation, we first need to understand the difference between a standard neural network and a Bayesian one.\n",
    "\n",
    "### 1.1 Standard Neural Networks\n",
    "\n",
    "In a standard neural network, each weight is a single, deterministic number. After training, these weights are fixed. \n",
    "When you pass an input through the model, it follows one exact path, producing one exact output.\n",
    "The model has no inherent way to express how \"sure\" it is about the values of its weights.\n",
    "\n",
    "### 1.2 Bayesian Neural Networks (BNNs)\n",
    "\n",
    "In a Bayesian Neural Network, we replace the deterministic weights with probability distributions.\n",
    "Instead of a weight being a single number, it might be represented by a Gaussian (normal) distribution \n",
    "with a mean and a standard deviation.\n",
    "\n",
    "- The mean represents the most likely value for that weight.\n",
    "\n",
    "- The standard deviation represents the model's uncertainty about that weight. A small standard deviation means the model\n",
    " is very confident in the weight's value, while a large one means it is very unsure.\n",
    "\n",
    "During a forward pass, we don't use the mean value directly. Instead, we sample a value for each weight from its distribution.\n",
    "Because we get a slightly different set of weights every time, each forward pass on the same input will produce a slightly different\n",
    " output. This natural variation is a direct reflection of the model's parameter uncertainty.\n",
    "\n",
    "### 1.3 The Bayesian Transformation `(probly)`\n",
    "\n",
    "The Bayesian transformation in `probly` automates the process of converting a standard network into a BNN.\n",
    "\n",
    "The transformation does the following:\n",
    "\n",
    "It walks through your PyTorch model and finds all compatible layers (e.g., nn.Linear and nn.Conv2d).\n",
    "It programmatically replaces each standard layer with a corresponding custom Bayesian layer (e.g., BayesLinear, BayesConv2d).\n",
    "These new layers contain weight distributions instead of single values and are inherently stochastic, even during inference.\n",
    "\n",
    "This allows us to get a distribution of predictions by running multiple forward passes, which we can then use to quantify the model's uncertainty.\n",
    "\n",
    "\n",
    "### 1.4. What that entails\n",
    "| Aspect                       |Bayesian Transformation `(probly)`                                                |\n",
    "|------------------------------|--------------------------------------------------------                          |\n",
    "| **Main Idea**                | \"Weights are distributions\"                                                      | \n",
    "| Stochastic Element           | Weights are sampled from probability distributions.                              | \n",
    "| Architectural Change         | Replaces `nn.Linear` and `nn.Conv2d` with `BayesLinear`/`BayesConv2d` layers.    | \n",
    "| Uncertainty Interpretation   | A principled, direct measure of the model's parameter uncertainty.               | \n",
    "|Supported Layers              | `Linear` and `Conv2d`                                                            | \n",
    "|Key Parameters                | `prior_mean`, `prior_std`, `posterior_std`                                       | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee989c",
   "metadata": {},
   "source": [
    "## 2. Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `bayesian(model)`, and inspect the modified architecture to see the layer replacement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13aefdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "With Bayesian transformation:\n",
      " Sequential(\n",
      "  (0): BayesLinear()\n",
      "  (1): ReLU()\n",
      "  (2): BayesLinear()\n",
      "  (3): ReLU()\n",
      "  (4): BayesLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import bayesian\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 1) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "# Apply the Bayesian transformation with default parameters\n",
    "model_bnn = bayesian(model)\n",
    "print(\"\\nWith Bayesian transformation:\\n\", model_bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051a356",
   "metadata": {},
   "source": [
    "#### Notes on the structure\n",
    "\n",
    "- Notice that each `Linear` layer has been replaced by a `BayesLinear` layer.\n",
    "- The new layers manage the distributions for the weights and biases internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b1d28",
   "metadata": {},
   "source": [
    "## 3. Uncertainty via Stochastic Forward Passes\n",
    "\n",
    "To obtain predictive uncertainty, we run multiple forward passes. In each pass, a new set of weights is sampled from the learned distributions.\n",
    "We then compute the mean and variance of the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cd81e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive mean (first 5):\n",
      " tensor([-1.1153,  3.2737,  1.8071, -1.3878, -0.6908])\n",
      "\n",
      "Predictive variance (first 5):\n",
      " tensor([0.0594, 0.0913, 0.0668, 0.0288, 0.0506])\n"
     ]
    }
   ],
   "source": [
    "# Toy regression data\n",
    "torch.manual_seed(0)\n",
    "n = 128\n",
    "X = torch.randn(n, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(n, 1)\n",
    "\n",
    "# Build and transform the model\n",
    "model = build_mlp(in_dim=10, hidden=64, out_dim=1)\n",
    "model_bnn = bayesian(model)\n",
    "\n",
    "# Simple training loop (for illustration)\n",
    "opt = torch.optim.Adam(model_bnn.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _step in range(200):\n",
    "    opt.zero_grad()\n",
    "    pred = model_bnn(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Prediction function (stochastic passes)\n",
    "@torch.no_grad()\n",
    "def stochastic_predict(\n",
    "    bayesian_model: nn.Module,\n",
    "    inputs: torch.Tensor,\n",
    "    n_samples: int = 50,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    preds = []\n",
    "    for _ in range(n_samples):\n",
    "        preds.append(bayesian_model(inputs).detach())\n",
    "    stacked = torch.stack(preds, dim=0)  # [n_samples, N, out_dim]\n",
    "    mean = stacked.mean(dim=0)\n",
    "    var = stacked.var(dim=0, unbiased=False)\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "mean_pred, var_pred = stochastic_predict(model_bnn, X[:5], n_samples=100)\n",
    "print(\"Predictive mean (first 5):\\n\", mean_pred.squeeze())\n",
    "print(\"\\nPredictive variance (first 5):\\n\", var_pred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a015d7",
   "metadata": {},
   "source": [
    "## 4. Part A Summary\n",
    "\n",
    "In Part A, we introduced the core concept of Bayesian Neural Networks, where weights are represented as probability distributions rather than single numbers.\n",
    "This inherently captures the model's uncertainty about its own parameters. We saw how the `probly.transformation.bayesian` Bayesian Transformation makes creating BNNs simple: \n",
    "it traverses a standard PyTorch model and replaces `nn.Linear` and `nn.Conv2d` layers with their Bayesian counterparts. This transformed model naturally producesIn Part A, \n",
    "we introduced the core concept of Bayesian Neural Networks, where weights are represented as probability distributions rather than single numbers.\n",
    "This inherently captures the model's uncertainty about its own parameters. We saw how the probly Bayesian Transformation makes creating BNNs simple: \n",
    "it traverses a standard PyTorch model and replaces nn.Linear and nn.Conv2d layers with their Bayesian counterparts. This transformed model naturally\n",
    "produces a distribution of outputs for any given input, allowing us to directly quantify predictive uncertainty. a distribution of outputs for any given input, \n",
    "allowing us to directly quantify predictive uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7f3aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B — Applied BNN Transformation\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416309a",
   "metadata": {},
   "source": [
    "In Part A, we learned what the Bayesian transformation in `probly` does.\n",
    "In this Part B, we will apply it to a model containing both linear and convolutional layers, run several stochastic predictions, and visualize the resulting uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f3b45",
   "metadata": {},
   "source": [
    "An indepth tutorial showing:\n",
    "\n",
    "- How to define a standard neural network (LeNet) and make it Bayesian using the bayesian transformation.\n",
    "\n",
    "- How to set up the specialized training loop required for a BNN using the ELBO loss function.\n",
    "\n",
    "- How to train the BNN on a real-world dataset (FashionMNIST).\n",
    "\n",
    "- How to evaluate the final classification accuracy of the trained Bayesian model.\n",
    "\n",
    "Can be found in the here:  **[Training a BNN for Classification](train_bnn_classification.ipynb)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f3a1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Summary — Bayesian Transformation Tutorial\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial introduced the core concepts of Bayesian Neural Networks (BNNs), where weights are treated as probability distributions to capture model uncertainty. We demonstrated how `probly`'s **`bayesian` transformation** automates this by replacing standard `nn.Linear` and `nn.Conv2d` layers with their stochastic Bayesian counterparts. We also walked through a simplified example of how to run multiple forward passes to get a predictive mean and variance.\n",
    "\n",
    "While this notebook covered the fundamentals, a proper BNN requires a specialized training procedure. For a complete, end-to-end guide that shows you how to train a Bayesian LeNet on the FashionMNIST dataset using the correct **ELBO loss**, please see the next tutorial: **[Training a BNN for Classification](train_bnn_classification.ipynb)**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
