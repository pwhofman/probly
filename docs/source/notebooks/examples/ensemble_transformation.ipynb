{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96610ee4",
   "metadata": {},
   "source": [
    "# Ensemble Transformation\n",
    "\n",
    "This notebook is a practical introduction to the Ensemble transformation in `probly`. Deep Ensembles are one of the most robust and high-performing methods for uncertainty quantification.\n",
    "\n",
    "We will start by explaining the core idea behind Deep Ensembles and see how  `probly`'s transformation enables you to create them. We will then walk through a PyTorch example to see how to train the ensemble and use the disagreement between its members to estimate predictive uncertainty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29225e2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: Introduction to Ensembles and the Ensemble Transformation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d22a68",
   "metadata": {},
   "source": [
    "## 1. Concept: What is a Deep Ensemble?\n",
    "### 1.1 The Core Idea: Wisdom of the Crowd\n",
    "\n",
    "The idea behind an ensemble is simple and powerful: instead of relying on the prediction of a single model, we train multiple models independently\n",
    "and aggregate their predictions. The core principle is that if we have a diverse set of \"experts\" (the models), their collective judgment will be\n",
    "more robust and reliable than any single expert's.\n",
    "\n",
    "### 1.2 Deep Ensembles for Uncertainty\n",
    "\n",
    "In Deep Learning, a **Deep Ensemble** consists of multiple neural networks. To create a diverse ensemble, each network is\n",
    "trained from a different random initialization.\n",
    "\n",
    "When we give the same input to every model in the ensemble, we will get a set of different predictions.\n",
    "\n",
    "- The mean of these predictions gives us a robust final prediction.\n",
    "- The variance (or disagreement) among these predictions gives us a direct and high-quality measure of the model's uncertainty.\n",
    "If all models agree, uncertainty is low. If they disagree significantly, uncertainty is high.\n",
    "\n",
    "While very effective, creating and managing deep ensembles manually can be cumbersome.\n",
    "\n",
    "### 1.3 The Ensemble Transformation\n",
    "\n",
    "The Ensemble transformation in `probly` automates the creation and management of a deep ensemble.\n",
    "\n",
    "The transformation does the following:\n",
    "\n",
    "- It takes a single, user-defined base model as a template.\n",
    "- It creates a specified number of deep copies of this model.\n",
    "- Crucially, it re-initializes the parameters of each copy, so every model in the ensemble starts from a different random state.\n",
    "- It packages all these independent models into a single torch.nn.ModuleList.\n",
    "\n",
    "This provides a convenient way to train and query all ensemble members simultaneously.\n",
    "\n",
    "### 1.4. What that entails\n",
    "| Aspect                       |Ensemble Transformation in `probly`                                                |\n",
    "|------------------------------|--------------------------------------------------------                          |\n",
    "| **Main Idea**                | \"Wisdom of the crowd\"                                                            |\n",
    "| Stochastic Element           | Disagreement between multiple independent models.                                |\n",
    "| Architectural Change         | Creates an nn.ModuleList of cloned and re-initialized models.                    |\n",
    "| Uncertainty Interpretation   | A very strong and robust measure of model uncertainty.                           |\n",
    "| Training Cost                |High (training N models)                                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2024b0",
   "metadata": {},
   "source": [
    "## 2. Ensemble Quickstart (PyTorch)\n",
    "\n",
    "Below: build a small MLP, apply `ensemble(model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9dcec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Ensemble with 5 members:\n",
      " ModuleList(\n",
      "  (0-4): 5 x Sequential(\n",
      "    (0): Linear(in_features=10, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import ensemble\n",
    "\n",
    "\n",
    "def build_mlp(in_dim: int = 10, hidden: int = 32, out_dim: int = 1) -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden, out_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "model = build_mlp()\n",
    "print(\"Original model:\\n\", model)\n",
    "\n",
    "# Apply the Ensemble transformation\n",
    "num_models = 5\n",
    "ensemble_model = ensemble(model, num_members=num_models)\n",
    "print(f\"\\nEnsemble with {num_models} members:\\n\", ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37237c5b",
   "metadata": {},
   "source": [
    "#### Notes on the structure\n",
    "\n",
    "- Notice that the ensemble_model is a ModuleList containing 5 independent copies of the original Sequential model.\n",
    "- By default, probly has already re-initialized the weights of each of these 5 models so they are different from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c4270",
   "metadata": {},
   "source": [
    "## 3. Training and Uncertainty with an Ensemble\n",
    "\n",
    "Training an ensemble involves feeding the data to all members and aggregating their losses. At inference time, we aggregate their predictions to get a final output and an uncertainty score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy regression data\n",
    "torch.manual_seed(0)\n",
    "n = 128\n",
    "X = torch.randn(n, 10)\n",
    "true_w = torch.randn(10, 1)\n",
    "y = X @ true_w + 0.1 * torch.randn(n, 1)\n",
    "\n",
    "# Build and transform the model\n",
    "model = build_mlp(in_dim=10, hidden=64, out_dim=1)\n",
    "ensemble_model = ensemble(model, num_members=5)\n",
    "\n",
    "# Training loop for an ensemble\n",
    "# We can use a single optimizer for all parameters\n",
    "all_params = [p for member in ensemble_model for p in member.parameters()]\n",
    "opt = torch.optim.Adam(all_params, lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _step in range(200):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    total_loss = 0\n",
    "    # Get a prediction from each member\n",
    "    for member in ensemble_model:\n",
    "        pred = member(X)\n",
    "        total_loss += loss_fn(pred, y)\n",
    "\n",
    "    # Average the loss and backpropagate\n",
    "    avg_loss = total_loss / len(ensemble_model)\n",
    "    avg_loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "# Prediction function for an ensemble\n",
    "@torch.no_grad()\n",
    "def ensemble_predict(\n",
    "    ensemble: nn.ModuleList,\n",
    "    inputs: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    preds = []\n",
    "    # Get a prediction from each member\n",
    "    for member in ensemble:\n",
    "        preds.append(member(inputs).detach())\n",
    "\n",
    "    stacked = torch.stack(preds, dim=0)  # [n_members, N, out_dim]\n",
    "    mean = stacked.mean(dim=0)\n",
    "    var = stacked.var(dim=0, unbiased=False)\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "mean_pred, var_pred = ensemble_predict(ensemble_model, X[:5])\n",
    "print(\"Predictive mean (first 5):\\n\", mean_pred.squeeze())\n",
    "print(\"\\nPredictive variance (first 5):\\n\", var_pred.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbd8c8",
   "metadata": {},
   "source": [
    "## 4. Part A Summary\n",
    "\n",
    "In Part A, we introduced Deep Ensembles as a robust method for uncertainty quantification based on the \"wisdom of the crowd.\"\n",
    "We saw that probly's ensemble transformation simplifies the creation of an ensemble by automatically cloning and re-initializing a base model,\n",
    "packaging the members into a convenient nn.ModuleList. Unlike other transformations that modify a model's internal layers, the ensemble method\n",
    "creates multiple, independent models. The disagreement in their predictions provides a powerful and high-quality estimate of model uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61b783",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B — Applied Ensemble Transformation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089386a7",
   "metadata": {},
   "source": [
    "In **Part A**, we learned what the **Ensemble transformation** in `probly` does.\n",
    "In this **Part B** , we will apply it to a classification model, get predictions from each member, and visualize the resulting uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8b935",
   "metadata": {},
   "source": [
    "An in depth walkthrough of:\n",
    "\n",
    "- How to apply the ensemble function to a real model.\n",
    "\n",
    "- How to train all the models in the ensemble on a real dataset   (FashionMNIST).\n",
    "\n",
    "- How to use the trained ensemble to make predictions.\n",
    "\n",
    "- Most importantly: How to use the disagreement between the models to measure uncertainty and detect Out-of-Distribution (OOD) data—in this case, telling the difference between clothing (FashionMNIST) and handwritten digits (MNIST).\n",
    "\n",
    "Can be found here:\n",
    "[FashionMNIST Out-of-Distribution Example](fashionmnist_ood_ensemble.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea88294",
   "metadata": {},
   "source": [
    "## Final Summary — Ensemble Transformation Tutorial\n",
    "You have now learned the fundamentals of the **Ensemble Transformation** in `probly`—how to automatically create an `nn.ModuleList` of independent models to capture predictive uncertainty from their disagreement. We saw that this allows us to get both a robust mean prediction and a measure of the model's confidence.\n",
    "\n",
    "To put this theory into practice, the [FashionMNIST OOD Example](fashionmnist_ood_ensemble.ipynb) will show you how to apply this technique to a real-world problem: building a classifier that knows when it's uncertain and can detect out-of-distribution data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d63d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
