{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62fb88d",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "This notebook provides a practical introduction to the specialized loss functions in `probly`. While standard losses like `nn.CrossEntropyLoss` are sufficient for deterministic models, probabilistic models often require custom loss functions to handle uncertainty.\n",
    "\n",
    "We will cover three key types of custom losses:\n",
    "-   **Negative Log-Likelihood (NLL) Losses:** Adaptations for probabilistic outputs.\n",
    "-   **Evidential Losses:** Specialized functions for models that learn \"evidence.\"\n",
    "-   **Calibration-Aware Losses:** Losses that directly optimize for model calibration.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Negative Log-Likelihood (NLL) Losses\n",
    "NLL losses are a foundational concept in training probabilistic models. Instead of just penalizing wrong predictions, they evaluate how well the entire predicted *distribution* explains the true target.\n",
    "\n",
    "### Example: The ELBO Loss for Bayesian Neural Networks\n",
    "A Bayesian Neural Network (BNN) requires a unique loss function that balances two goals:\n",
    "1.  **Fit the data:** Make accurate predictions (similar to a standard loss).\n",
    "2.  **Stay simple:** Keep the weight distributions close to a simple prior distribution.\n",
    "\n",
    "The **Evidence Lower Bound (ELBO)** loss achieves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e893dbe264a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from probly.train.bayesian.torch import collect_kl_divergence\n",
    "from probly.transformation import bayesian\n",
    "\n",
    "\n",
    "class ELBOLoss(nn.Module):\n",
    "    \"\"\"Evidential Lower Bound Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, kl_penalty: float = 1e-5) -> None:\n",
    "        \"\"\"Initialize the loss.\n",
    "\n",
    "        Args:\n",
    "            kl_penalty: The penalty weight for the KL divergence term.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kl_penalty = kl_penalty\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor, kl: torch.tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute the ELBO loss.\n",
    "\n",
    "        Args:\n",
    "            inputs: The input tensor.\n",
    "            targets: The target tensor.\n",
    "            kl: The KL divergence tensor.\n",
    "\n",
    "        Returns:\n",
    "            The calculated loss.\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. Standard Cross-Entropy\n",
    "        cross_entropy_loss = F.cross_entropy(inputs, targets)\n",
    "\n",
    "        # 2. KL Divergence Regularizer\n",
    "        kl_divergence = self.kl_penalty * kl\n",
    "\n",
    "        return cross_entropy_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964cf1",
   "metadata": {},
   "source": [
    "The ELBOLoss combines a standard cross-entropy loss with a KL Divergence term, which penalizes the model for having weight distributions that are too complex or far from the initial prior\n",
    "For more information on how Bayesian models work, see the [Bayesian Transformation](../bayesian_transformation.ipynb) tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2e474",
   "metadata": {},
   "source": [
    "## 2. Evidential Losses\n",
    "\n",
    "Evidential Deep Learning models do not output probabilities directly.\n",
    "Instead, they output **evidence** for each class, which requires specialized loss functions.\n",
    "\n",
    "### Example: Evidential Losses for Classification and Regression\n",
    "\n",
    "The `probly` library provides custom loss functions for evidential learning, based on the original research papers:\n",
    "\n",
    "- **EvidentialLogLoss (Classification)**\n",
    "  Adapts the standard log loss to work with evidence scores (`alpha`) instead of probabilities.\n",
    "\n",
    "- **EvidentialNIGNLLLoss (Regression)**\n",
    "  A more complex negative log-likelihood (NLL) loss that handles the four parameters of an evidential regression model:\n",
    "  `gamma`, `nu`, `alpha`, and `beta`.\n",
    "\n",
    "### Training an Evidential Model\n",
    "\n",
    "The training loop for an evidential model typically combines:\n",
    "\n",
    "1. An evidential NLL loss (classification or regression), and\n",
    "2. A regularization term that encourages the model to remain uncertain on out-of-distribution data.\n",
    "The total loss is a weighted sum of these two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87edf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training with ELBO Loss\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Create a simple model and transform it to Bayesian\n",
    "model = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 3))\n",
    "bnn_model = bayesian(model)\n",
    "\n",
    "# Create the ELBO loss\n",
    "criterion = ELBOLoss(kl_penalty=1e-5)\n",
    "\n",
    "# Dummy data (8 samples, 10 features, 3 classes)\n",
    "inputs = torch.randn(8, 10)\n",
    "targets = torch.randint(0, 3, (8,))\n",
    "\n",
    "# Forward pass\n",
    "outputs = bnn_model(inputs)\n",
    "\n",
    "# Collect KL divergence from all Bayesian layers\n",
    "kl = collect_kl_divergence(bnn_model)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets, kl)\n",
    "\n",
    "print(f\"ELBO Loss: {loss.item():.4f}\")\n",
    "print(f\"  - Cross-Entropy component: {nn.functional.cross_entropy(outputs, targets).item():.4f}\")\n",
    "print(f\"  - KL Divergence component: {(criterion.kl_penalty * kl).item():.6f}\")\n",
    "print(f\"\\nTotal KL from all layers: {kl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073bbd0",
   "metadata": {},
   "source": [
    "For full implementations, see the [**Evidential Classification**](../train_evidential_classification.ipynb) and [**Evidential Regression**](../train_evidential_regression.ipynb) tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db127c1",
   "metadata": {},
   "source": [
    "## 3. Calibration-Aware Losses\n",
    "\n",
    "Sometimes, the most effective way to achieve good calibration is to include a calibration objective directly in the loss function.\n",
    "This forces the model to optimize calibration as part of training.\n",
    "\n",
    "### Example: Label Relaxation\n",
    "\n",
    "**Label Relaxation** is a simple but effective technique for reducing over-confidence and improving model calibration.\n",
    "Instead of using hard one-hot encoded labels (e.g., `[0, 0, 1]`), the labels are softened:\n",
    "\n",
    "- The true class is assigned a slightly lower value (e.g., `0.9`).\n",
    "- The remaining probability mass (e.g., `0.1`) is distributed across the other classes.\n",
    "\n",
    "This discourages the model from producing extreme, over-confident predictions.\n",
    "\n",
    "The `probly` library provides a direct implementation of this approach through the **`LabelRelaxationLoss`**.\n",
    "The `LabelRelaxationLoss` can be used as a drop-in replacement for standard losses like `nn.CrossEntropyLoss`, making it easy to integrate into existing training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training with Label Relaxation\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.train.calibration.torch import LabelRelaxationLoss\n",
    "\n",
    "# Create a simple classifier\n",
    "model = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 3))\n",
    "\n",
    "# Use Label Relaxation instead of CrossEntropyLoss\n",
    "# alpha=0.1 means: true class gets 0.9, other classes share 0.1\n",
    "criterion = LabelRelaxationLoss(alpha=0.1)\n",
    "\n",
    "# Standard CrossEntropyLoss for comparison\n",
    "standard_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn(8, 10)\n",
    "targets = torch.randint(0, 3, (8,))\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Compare losses\n",
    "relaxed_loss = criterion(outputs, targets)\n",
    "standard_loss = standard_criterion(outputs, targets)\n",
    "\n",
    "print(f\"Standard CrossEntropy Loss: {standard_loss.item():.4f}\")\n",
    "print(f\"Label Relaxation Loss:      {relaxed_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3431d4",
   "metadata": {},
   "source": [
    "By optimizing this \"softer\" objective, the model learns to produce better-calibrated probability estimates.\n",
    "\n",
    "For a full implementation, see the [**Label Relaxation Calibration**](../label_relaxation_calibration.ipynb) tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
