{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93268c59",
   "metadata": {},
   "source": [
    "# Key Probabilistic Layers in `probly`\n",
    "\n",
    "## 1. `BayesLinear` & `BayesConv2d`\n",
    "\n",
    "**What they do:**\n",
    "These layers replace standard `Linear` and `Conv2d` layers to build a **Bayesian Neural Network (BNN)**.\n",
    "\n",
    "**How they work:**\n",
    "Instead of using fixed weights, each weight is modeled as a probability distribution (e.g., Gaussian).\n",
    "On every forward pass, weights are sampled from these distributions.\n",
    "\n",
    "**Result:**\n",
    "The model can explicitly represent **uncertainty in its own parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `DropConnectLinear`\n",
    "\n",
    "**What it does:**\n",
    "Implements the **DropConnect** technique as a neural network layer.\n",
    "\n",
    "**How it works:**\n",
    "During training (and during inference when performing uncertainty quantification), individual weights are randomly set to zero.\n",
    "This is a more general form of Dropout.\n",
    "\n",
    "**Result:**\n",
    "Running multiple forward passes produces a **distribution of outputs** that captures model uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `NormalInverseGammaLinear` (for Evidential Regression)\n",
    "\n",
    "**What it does:**\n",
    "A specialized output layer designed for **Evidential Regression**.\n",
    "\n",
    "**How it works:**\n",
    "Instead of predicting a single value, the layer outputs the four parameters of a **Normal-Inverse-Gamma (NIG)** distribution:\n",
    "\n",
    "- `gamma`\n",
    "- `nu`\n",
    "- `alpha`\n",
    "- `beta`\n",
    "\n",
    "**Result:**\n",
    "This distribution directly models:\n",
    "\n",
    "- The predictive mean\n",
    "- The predictive variance\n",
    "- The modelâ€™s confidence in its own predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: BayesLinear and BayesConv2d (via bayesian transformation)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import bayesian\n",
    "\n",
    "# Create a standard model with Linear and Conv2d layers\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 8, kernel_size=3), nn.ReLU(), nn.Flatten(), nn.Linear(8 * 26 * 26, 32), nn.ReLU(), nn.Linear(32, 10)\n",
    ")\n",
    "\n",
    "print(\"Original model:\")\n",
    "print(model)\n",
    "\n",
    "# Transform to Bayesian - replaces Linear -> BayesLinear, Conv2d -> BayesConv2d\n",
    "bnn_model = bayesian(model)\n",
    "\n",
    "print(\"\\nBayesian model:\")\n",
    "print(bnn_model)\n",
    "\n",
    "# Test with dummy input\n",
    "dummy_input = torch.randn(2, 1, 28, 28)\n",
    "output = bnn_model(dummy_input)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: DropConnectLinear (via dropconnect transformation)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import dropconnect\n",
    "\n",
    "# Create a standard model\n",
    "model = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 3))\n",
    "\n",
    "print(\"Original model:\")\n",
    "print(model)\n",
    "\n",
    "# Transform to DropConnect with p=0.25 (25% of weights dropped)\n",
    "dc_model = dropconnect(model, p=0.25)\n",
    "\n",
    "print(\"\\nDropConnect model:\")\n",
    "print(dc_model)\n",
    "\n",
    "# Test stochastic behavior\n",
    "dummy_input = torch.randn(4, 10)\n",
    "\n",
    "dc_model.train()  # Enable stochastic weight dropping\n",
    "output1 = dc_model(dummy_input)\n",
    "output2 = dc_model(dummy_input)\n",
    "\n",
    "print(\"\\nSame input, different outputs due to random weight dropping:\")\n",
    "print(f\"Output 1: {output1[0, :3].detach().numpy()}\")\n",
    "print(f\"Output 2: {output2[0, :3].detach().numpy()}\")\n",
    "print(f\"Difference: {(output1[0] - output2[0]).abs().mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: NormalInverseGammaLinear (via evidential_regression transformation)\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from probly.transformation import evidential_regression\n",
    "\n",
    "# Create a standard regression model\n",
    "model = nn.Sequential(nn.Linear(10, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "\n",
    "print(\"Original model (outputs single value):\")\n",
    "print(model)\n",
    "\n",
    "# Transform to evidential regression\n",
    "# Replaces final Linear with NormalInverseGammaLinear\n",
    "evid_model = evidential_regression(model)\n",
    "\n",
    "print(\"\\nEvidential regression model (outputs 4 NIG parameters):\")\n",
    "print(evid_model)\n",
    "\n",
    "# Test output\n",
    "dummy_input = torch.randn(2, 10)\n",
    "output = evid_model(dummy_input)\n",
    "\n",
    "print(\"\\nOutput is a dictionary with 4 parameters:\")\n",
    "print(f\"Keys: {output.keys()}\")\n",
    "print(\n",
    "    f\"Shapes: gamma={output['gamma'].shape}, nu={output['nu'].shape}, \"\n",
    "    f\"alpha={output['alpha'].shape}, beta={output['beta'].shape}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
