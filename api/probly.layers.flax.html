<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" />

    <!-- Generated with Sphinx 8.2.3 and Furo 2024.08.06 -->
        <title>probly.layers.flax - probly 0.3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=201d0c9a" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">probly 0.3.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><div class="sidebar-scroll"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo/logo_light.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo/logo_dark.png" alt="Dark Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">The <code class="docutils literal notranslate"><span class="pre">probly</span></code> Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core_concepts.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../main_components.html">Main Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_topics.html">Advanced Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_and_tutorials.html">Examples and Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../methods.html">Implemented methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing_to_probly.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References and Further Reading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ and Troubleshooting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notebooks</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../notebooks/examples/index.html">Notebook Examples</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Notebook Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/examples/utilities_and_layers/index.html">Utilities and Layers</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Utilities and Layers</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/utilities_and_layers/custom_loss_functions.html">Custom Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/utilities_and_layers/metrics.html">Evaluation Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/utilities_and_layers/probabilistic_layers.html">Key Probabilistic Layers in <code class="docutils literal notranslate"><span class="pre">probly</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/utilities_and_layers/utility_functions.html">Utility Functions</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../notebooks/examples/evaluation_and_quantification/index.html">Evaluation and Quantification</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Evaluation and Quantification</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/evaluation_and_quantification/calibration_metrics.html">Calibration Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/evaluation_and_quantification/interpretation_techniques.html">Interpretation techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../notebooks/examples/evaluation_and_quantification/visualization_tools.html">Visualisation Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/batch_ensemble_notebook.html"><strong>Batch Ensemble Networks</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/bayesian_transformation.html">Bayesian Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/dropconnect_transformation.html">Dropconnect Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/dropout_transformation.html">Dropout Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/ensemble_subensemble_comparison.html"><strong>Ensemble vs SubEnsemble Notebook</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/ensemble_transformation.html">Ensemble Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html">Potential Advantages of Ensembling Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#theoretical-background">2. Theoretical Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#data-generation">3. Data Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#random-forest-ensemble-prototype">4. Random Forest Ensemble Prototype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#uncertainty-analysis">5. Uncertainty Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#performance-metrics">6. Performance Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/Ensembling_RandomForests.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/evidential_classification_transformation.html">Evidential Classification Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/evidential_regression_transformation.html">Evidential Regression Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/fashionmnist_ood_ensemble.html">Out-of-Distribution Detection with an Ensemble</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/label_relaxation_calibration.html">Calibration with Label Relaxation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/lazy_dispatch_test.html">Lazy Dispatch Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/multilib_demo.html">Multilib Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/pytraverse_tutorial.html">A Brief Introduction to PyTraverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/sklearn_ensemble_tutorial.html">Uncertainty Quantification using scikit-learn-Ensembles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/sklearn_selective_prediction.html">Using probly with scikit-learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/subensembles_fast_uncertainty.html">Sub-Ensembles for Fast Uncertainty Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/synthetic_regression_dropout.html">Uncertainty for a Synthetic Regression Task using probly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/temperature_scaling_calibration.html">Calibration using Temperature Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/train_bnn_classification.html">Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/train_evidential_classification.html">Evidential Model for Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/train_evidential_regression.html">Evidential Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notebooks/examples/transformations_comparison.html">Transformation Comparison: Dropout vs DropConnect vs Ensemble vs Bayesian vs Evidential (PyTorch)</a></li>
</ul>
</li>
</ul>

</div>
</div><div style="text-align: center; margin-top: 1rem; margin-bottom: 1rem;">
  <a href="https://github.com/pwhofman/probly" target="_blank" rel="noopener noreferrer" title="GitHub Repository">
    <img src="../_static/github-mark.svg" alt="GitHub Logo" width="28" height="28" style="display: inline-block;">
  </a>
</div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          

<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="module-probly.layers.flax">
<span id="probly-layers-flax"></span><h1>probly.layers.flax<a class="headerlink" href="#module-probly.layers.flax" title="Link to this heading">¶</a></h1>
<p>flax layer implementation.</p>
<p class="rubric">Classes</p>
<div class="table-wrapper autosummary longtable docutils container">
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv" title="probly.layers.flax.BatchEnsembleConv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchEnsembleConv</span></code></a>(*args, **kwargs)</p></td>
<td><p>Implements a BatchEnsemble convolutional layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear" title="probly.layers.flax.BatchEnsembleLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchEnsembleLinear</span></code></a>(*args, **kwargs)</p></td>
<td><p>Implements a BatchEnsemble Linear layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#probly.layers.flax.DropConnectLinear" title="probly.layers.flax.DropConnectLinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DropConnectLinear</span></code></a>(*args, **kwargs)</p></td>
<td><p>Custom Linear layer with DropConnect applied to weights during training based on <span id="id1">[<a class="reference internal" href="../references.html#id6" title="Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. 2020. URL: https://proceedings.neurips.cc/paper/2020/hash/aab085461de182608ee9f607f3f7d18f-Abstract.html.">ASSR20</a>]</span>.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">probly.layers.flax.</span></span><span class="sig-name descname"><span class="pre">BatchEnsembleConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#BatchEnsembleConv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv</span></code></p>
<p>Implements a BatchEnsemble convolutional layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>Any</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Any</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.kernel_shape">
<span class="sig-name descname"><span class="pre">kernel_shape</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.kernel_shape" title="Link to this definition">¶</a></dt>
<dd><p>Sequence[int], (in_features, out_features, kernel_size)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.kernel">
<span class="sig-name descname"><span class="pre">kernel</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.kernel" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, weight matrix of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.bias" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, bias of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.in_features">
<span class="sig-name descname"><span class="pre">in_features</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.in_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of input features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.out_features">
<span class="sig-name descname"><span class="pre">out_features</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.out_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of output features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.kernel_size">
<span class="sig-name descname"><span class="pre">kernel_size</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.kernel_size" title="Link to this definition">¶</a></dt>
<dd><p>int or Sequence[int], size of the kernel.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.strides">
<span class="sig-name descname"><span class="pre">strides</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.strides" title="Link to this definition">¶</a></dt>
<dd><p>tp.Union[None, int, tp.Sequence[int]], representing the inter-window strides.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.padding">
<span class="sig-name descname"><span class="pre">padding</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.padding" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PaddingLike, either the string <code class="docutils literal notranslate"><span class="pre">'SAME'</span></code>, the string <code class="docutils literal notranslate"><span class="pre">'VALID'</span></code>, the string
<code class="docutils literal notranslate"><span class="pre">'CIRCULAR'</span></code> (periodic boundary conditions), the string <cite>‘REFLECT’</cite>
(reflection across the padding boundary), or a sequence of <code class="docutils literal notranslate"><span class="pre">n</span></code>
<code class="docutils literal notranslate"><span class="pre">(low,</span> <span class="pre">high)</span></code> integer pairs that give the padding to apply before and after each
spatial dimension. A single int is interpreted as applying the same padding
in all dims and passing a single int in a sequence causes the same padding
to be used on both sides. <code class="docutils literal notranslate"><span class="pre">'CAUSAL'</span></code> padding for a 1D convolution will
left-pad the convolution axis, resulting in same-sized output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.input_dilation">
<span class="sig-name descname"><span class="pre">input_dilation</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.input_dilation" title="Link to this definition">¶</a></dt>
<dd><p>tp.Union[None, int, tp.Sequence[int]], giving the
dilation factor to apply in each spatial dimension of <code class="docutils literal notranslate"><span class="pre">inputs</span></code>
(default: 1). Convolution with input dilation <code class="docutils literal notranslate"><span class="pre">d</span></code> is equivalent to
transposed convolution with stride <code class="docutils literal notranslate"><span class="pre">d</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.kernel_dilation">
<span class="sig-name descname"><span class="pre">kernel_dilation</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.kernel_dilation" title="Link to this definition">¶</a></dt>
<dd><p>tp.Union[None, int, tp.Sequence[int]], giving the
dilation factor to apply in each spatial dimension of the convolution
kernel (default: 1). Convolution with kernel dilation
is also known as ‘atrous convolution’.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.feature_group_count">
<span class="sig-name descname"><span class="pre">feature_group_count</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.feature_group_count" title="Link to this definition">¶</a></dt>
<dd><p>int, If specified divides the input features into groups.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.use_bias">
<span class="sig-name descname"><span class="pre">use_bias</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.use_bias" title="Link to this definition">¶</a></dt>
<dd><p>bool, whether to add bias to the output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.mask">
<span class="sig-name descname"><span class="pre">mask</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.mask" title="Link to this definition">¶</a></dt>
<dd><p>typing.Optional[Array], Optional .</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.dtype" title="Link to this definition">¶</a></dt>
<dd><p>typing.Optional[flax.typing.Dtype], the dtype of the computation (default: infer from input and params).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.param_dtype">
<span class="sig-name descname"><span class="pre">param_dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.param_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, the dtype passed to parameter initializers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.precision" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PrecisionLike, numerical precision of the computation see <code class="docutils literal notranslate"><span class="pre">jax.lax.Precision</span></code>
for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.conv_general_dilated">
<span class="sig-name descname"><span class="pre">conv_general_dilated</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.conv_general_dilated" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.DotGeneralT, dot product function.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.promote_dtype">
<span class="sig-name descname"><span class="pre">promote_dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.promote_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PromoteDtypeFn, function to promote the dtype of the arrays to the desired
dtype. The function should accept a tuple of <code class="docutils literal notranslate"><span class="pre">(inputs,</span> <span class="pre">kernel,</span> <span class="pre">bias)</span></code>
and a <code class="docutils literal notranslate"><span class="pre">dtype</span></code> keyword argument, and return a tuple of arrays with the
promoted dtype.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.preferred_element_type">
<span class="sig-name descname"><span class="pre">preferred_element_type</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.preferred_element_type" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, Optional parameter controls the data type output by
the dot product. This argument is passed to <code class="docutils literal notranslate"><span class="pre">dot_general</span></code> function.
See <code class="docutils literal notranslate"><span class="pre">jax.lax.dot</span></code> for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.num_members">
<span class="sig-name descname"><span class="pre">num_members</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.num_members" title="Link to this definition">¶</a></dt>
<dd><p>int, number of batch ensemble members.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.s">
<span class="sig-name descname"><span class="pre">s</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.s" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, rank-one factor for input features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.r">
<span class="sig-name descname"><span class="pre">r</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.r" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, rank-one factor for output features.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#BatchEnsembleConv.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.__call__" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass of the BatchEnsembleConv layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>Array</em>) – jax.Array, the input of shape [B, kernel_size(n-dimensional), in_features]
or [E, B, kernel_size(n-dimensional), in_features],
where B is the batch size and E is the ensemble_size.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>jax.Array, Output of shape [E, B, kernel_size(n-dimensional), out_features].</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Array</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.eval" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to evaluation mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">eval</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=True</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=True</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.iter_children">
<span class="sig-name descname"><span class="pre">iter_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.iter_children" title="Link to this definition">¶</a></dt>
<dd><p>Iterates over all children <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module. This
method is similar to <a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.iter_modules" title="probly.layers.flax.BatchEnsembleConv.iter_modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">iter_modules()</span></code></a>, except it only iterates over the
immediate children, and does not recurse further down.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_children</span></code> creates a generator that yields the key and the Module instance,
where the key is a string representing the attribute name of the Module to access
the corresponding child Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_children</span><span class="p">():</span>
<span class="gp">... </span> <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">batch_norm BatchNorm</span>
<span class="go">dropout Dropout</span>
<span class="go">linear Linear</span>
<span class="go">submodule SubModule</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.iter_modules">
<span class="sig-name descname"><span class="pre">iter_modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.iter_modules" title="Link to this definition">¶</a></dt>
<dd><p>Recursively iterates over all nested <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module, including
the current Module.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_modules</span></code> creates a generator that yields the path and the Module instance, where
the path is a tuple of strings or integers representing the path to the Module from the
root Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_modules</span><span class="p">():</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">(&#39;batch_norm&#39;,) BatchNorm</span>
<span class="go">(&#39;dropout&#39;,) Dropout</span>
<span class="go">(&#39;linear&#39;,) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear1&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear2&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;,) SubModule</span>
<span class="go">() Block</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, …], <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.perturb">
<span class="sig-name descname"><span class="pre">perturb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable_type=&lt;class</span> <span class="pre">'flax.nnx.variablelib.Perturbation'&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.perturb" title="Link to this definition">¶</a></dt>
<dd><p>Add an zero-value variable (“perturbation”) to the intermediate value.</p>
<p>The gradient of <code class="docutils literal notranslate"><span class="pre">value</span></code> would be the same as the gradient of this
perturbation variable. Therefore, if you define your loss function with
both params and perturbations as standalone arguments, you can get the
intermediate gradients of <code class="docutils literal notranslate"><span class="pre">value</span></code> by running <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> on the
perturbation variable.</p>
<p>Since the shape of the perturbation value depends on the shape of the input,
a perturbation variable is only created after you run a sample input through
the model once.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This creates extra dummy variables of the same size as <code class="docutils literal notranslate"><span class="pre">value</span></code>, thus
occupies more memory. Use it only to debug gradients in training.</p>
</div>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">perturb</span><span class="p">(</span><span class="s1">&#39;xgrad&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;xgrad&#39;</span><span class="p">)</span>  <span class="c1"># perturbation requires a sample input run</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># same as the intermediate value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Take gradients on the Param and Perturbation variables</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@nnx</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span><span class="k">def</span><span class="w"> </span><span class="nf">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">model</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># `perturbations.xgrad.value` is the intermediate gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">perturbations</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name for the
perturbation value.</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a>) – The value to take intermediate gradient.</p></li>
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>]</em><em>]</em>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored perturbation.
Defaulted at <code class="xref py py-class docutils literal notranslate"><span class="pre">nnx.Perturbation</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.set_attributes">
<span class="sig-name descname"><span class="pre">set_attributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">raise_if_not_found</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.set_attributes" title="Link to this definition">¶</a></dt>
<dd><p>Sets the attributes of nested Modules including the current Module.
If the attribute is not found in the Module, it is ignored.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Filter</span></code>’s can be used to set the attributes of specific Modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Only the dropout will be modified</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*filters</strong> (<em>filterlib.Filter</em>) – Filters to select the Modules to set the attributes of.</p></li>
<li><p><strong>raise_if_not_found</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>) – If True (default), raises a ValueError if at least one attribute
instance is not found in one of the selected Modules.</p></li>
<li><p><strong>**attributes</strong> (<em>tp.Any</em>) – The attributes to set.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.sow">
<span class="sig-name descname"><span class="pre">sow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.sow" title="Link to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">sow()</span></code> can be used to collect intermediate values without
the overhead of explicitly passing a container through each Module call.
<code class="docutils literal notranslate"><span class="pre">sow()</span></code> stores a value in a new <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute, denoted by <code class="docutils literal notranslate"><span class="pre">name</span></code>.
The value will be wrapped by a <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> of type <code class="docutils literal notranslate"><span class="pre">variable_type</span></code>,
which can be useful to filter for in <code class="xref py py-func docutils literal notranslate"><span class="pre">split()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">state()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">pop()</span></code>.</p>
<p>By default the values are stored in a tuple and each stored value
is appended at the end. This way all intermediates can be tracked when
the same module is called multiple times.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="n">add</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="c1"># tuple of length 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="c1"># tuple of length 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<p>Alternatively, a custom init/reduce function can be passed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">+</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;product&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">*</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><em>B</em><em>]</em><em>] </em><em>| </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored value.
Typically <code class="xref py py-class docutils literal notranslate"><span class="pre">Intermediate</span></code> is used to indicate an
intermediate value.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name, where
the sowed value is stored.</p></li>
<li><p><strong>value</strong> (<em>A</em>) – The value to be stored.</p></li>
<li><p><strong>reduce_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>B</em><em>, </em><em>A</em><em>]</em><em>, </em><em>B</em><em>]</em>) – The function used to combine the existing value with the new
value. The default is to append the value to a tuple.</p></li>
<li><p><strong>init_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>]</em><em>, </em><em>B</em><em>]</em>) – For the first value stored, <code class="docutils literal notranslate"><span class="pre">reduce_fn</span></code> will be passed the result
of <code class="docutils literal notranslate"><span class="pre">init_fn</span></code> together with the value to be stored. The default is an
empty tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleConv.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleConv.train" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to training mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=False</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=False</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># initialize Dropout and BatchNorm in eval mode</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">probly.layers.flax.</span></span><span class="sig-name descname"><span class="pre">BatchEnsembleLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#BatchEnsembleLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></p>
<p>Implements a BatchEnsemble Linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>Any</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Any</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.kernel">
<span class="sig-name descname"><span class="pre">kernel</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.kernel" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, weight matrix of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.bias" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, bias of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.in_features">
<span class="sig-name descname"><span class="pre">in_features</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.in_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of input features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.out_features">
<span class="sig-name descname"><span class="pre">out_features</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.out_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of output features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.use_bias">
<span class="sig-name descname"><span class="pre">use_bias</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.use_bias" title="Link to this definition">¶</a></dt>
<dd><p>bool, whether to add bias to the output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.dtype" title="Link to this definition">¶</a></dt>
<dd><p>typing.Optional[flax.typing.Dtype], the dtype of the computation (default: infer from input and params).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.param_dtype">
<span class="sig-name descname"><span class="pre">param_dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.param_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, the dtype passed to parameter initializers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.precision" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PrecisionLike, numerical precision of the computation see <code class="docutils literal notranslate"><span class="pre">jax.lax.Precision</span></code>
for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.dot_general">
<span class="sig-name descname"><span class="pre">dot_general</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.dot_general" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.DotGeneralT, dot product function.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.promote_dtype">
<span class="sig-name descname"><span class="pre">promote_dtype</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.promote_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PromoteDtypeFn, function to promote the dtype of the arrays to the desired
dtype. The function should accept a tuple of <code class="docutils literal notranslate"><span class="pre">(inputs,</span> <span class="pre">kernel,</span> <span class="pre">bias)</span></code>
and a <code class="docutils literal notranslate"><span class="pre">dtype</span></code> keyword argument, and return a tuple of arrays with the
promoted dtype.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.preferred_element_type">
<span class="sig-name descname"><span class="pre">preferred_element_type</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.preferred_element_type" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, Optional parameter controls the data type output by
the dot product. This argument is passed to <code class="docutils literal notranslate"><span class="pre">dot_general</span></code> function.
See <code class="docutils literal notranslate"><span class="pre">jax.lax.dot</span></code> for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.num_members">
<span class="sig-name descname"><span class="pre">num_members</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.num_members" title="Link to this definition">¶</a></dt>
<dd><p>int, number of batch ensemble members.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.s">
<span class="sig-name descname"><span class="pre">s</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.s" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, rank-one factor for input features</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.r">
<span class="sig-name descname"><span class="pre">r</span></span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.r" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, rank-one factor for output features</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#BatchEnsembleLinear.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.__call__" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass of the BatchEnsembleLinear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>Array</em>) – jax.Array, the input of shape [B, in_features] or [E, B, in_features].
where B is the batch size and E is the ensemble_size.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>jax.Array, Output of shape [E, B, out_features].</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Array</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.eval" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to evaluation mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">eval</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=True</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=True</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.iter_children">
<span class="sig-name descname"><span class="pre">iter_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.iter_children" title="Link to this definition">¶</a></dt>
<dd><p>Iterates over all children <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module. This
method is similar to <a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.iter_modules" title="probly.layers.flax.BatchEnsembleLinear.iter_modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">iter_modules()</span></code></a>, except it only iterates over the
immediate children, and does not recurse further down.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_children</span></code> creates a generator that yields the key and the Module instance,
where the key is a string representing the attribute name of the Module to access
the corresponding child Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_children</span><span class="p">():</span>
<span class="gp">... </span> <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">batch_norm BatchNorm</span>
<span class="go">dropout Dropout</span>
<span class="go">linear Linear</span>
<span class="go">submodule SubModule</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.iter_modules">
<span class="sig-name descname"><span class="pre">iter_modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.iter_modules" title="Link to this definition">¶</a></dt>
<dd><p>Recursively iterates over all nested <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module, including
the current Module.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_modules</span></code> creates a generator that yields the path and the Module instance, where
the path is a tuple of strings or integers representing the path to the Module from the
root Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_modules</span><span class="p">():</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">(&#39;batch_norm&#39;,) BatchNorm</span>
<span class="go">(&#39;dropout&#39;,) Dropout</span>
<span class="go">(&#39;linear&#39;,) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear1&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear2&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;,) SubModule</span>
<span class="go">() Block</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, …], <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.perturb">
<span class="sig-name descname"><span class="pre">perturb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable_type=&lt;class</span> <span class="pre">'flax.nnx.variablelib.Perturbation'&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.perturb" title="Link to this definition">¶</a></dt>
<dd><p>Add an zero-value variable (“perturbation”) to the intermediate value.</p>
<p>The gradient of <code class="docutils literal notranslate"><span class="pre">value</span></code> would be the same as the gradient of this
perturbation variable. Therefore, if you define your loss function with
both params and perturbations as standalone arguments, you can get the
intermediate gradients of <code class="docutils literal notranslate"><span class="pre">value</span></code> by running <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> on the
perturbation variable.</p>
<p>Since the shape of the perturbation value depends on the shape of the input,
a perturbation variable is only created after you run a sample input through
the model once.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This creates extra dummy variables of the same size as <code class="docutils literal notranslate"><span class="pre">value</span></code>, thus
occupies more memory. Use it only to debug gradients in training.</p>
</div>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">perturb</span><span class="p">(</span><span class="s1">&#39;xgrad&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;xgrad&#39;</span><span class="p">)</span>  <span class="c1"># perturbation requires a sample input run</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># same as the intermediate value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Take gradients on the Param and Perturbation variables</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@nnx</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span><span class="k">def</span><span class="w"> </span><span class="nf">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">model</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># `perturbations.xgrad.value` is the intermediate gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">perturbations</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name for the
perturbation value.</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a>) – The value to take intermediate gradient.</p></li>
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>]</em><em>]</em>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored perturbation.
Defaulted at <code class="xref py py-class docutils literal notranslate"><span class="pre">nnx.Perturbation</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.set_attributes">
<span class="sig-name descname"><span class="pre">set_attributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">raise_if_not_found</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.set_attributes" title="Link to this definition">¶</a></dt>
<dd><p>Sets the attributes of nested Modules including the current Module.
If the attribute is not found in the Module, it is ignored.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Filter</span></code>’s can be used to set the attributes of specific Modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Only the dropout will be modified</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*filters</strong> (<em>filterlib.Filter</em>) – Filters to select the Modules to set the attributes of.</p></li>
<li><p><strong>raise_if_not_found</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>) – If True (default), raises a ValueError if at least one attribute
instance is not found in one of the selected Modules.</p></li>
<li><p><strong>**attributes</strong> (<em>tp.Any</em>) – The attributes to set.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.sow">
<span class="sig-name descname"><span class="pre">sow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.sow" title="Link to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">sow()</span></code> can be used to collect intermediate values without
the overhead of explicitly passing a container through each Module call.
<code class="docutils literal notranslate"><span class="pre">sow()</span></code> stores a value in a new <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute, denoted by <code class="docutils literal notranslate"><span class="pre">name</span></code>.
The value will be wrapped by a <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> of type <code class="docutils literal notranslate"><span class="pre">variable_type</span></code>,
which can be useful to filter for in <code class="xref py py-func docutils literal notranslate"><span class="pre">split()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">state()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">pop()</span></code>.</p>
<p>By default the values are stored in a tuple and each stored value
is appended at the end. This way all intermediates can be tracked when
the same module is called multiple times.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="n">add</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="c1"># tuple of length 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="c1"># tuple of length 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<p>Alternatively, a custom init/reduce function can be passed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">+</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;product&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">*</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><em>B</em><em>]</em><em>] </em><em>| </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored value.
Typically <code class="xref py py-class docutils literal notranslate"><span class="pre">Intermediate</span></code> is used to indicate an
intermediate value.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name, where
the sowed value is stored.</p></li>
<li><p><strong>value</strong> (<em>A</em>) – The value to be stored.</p></li>
<li><p><strong>reduce_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>B</em><em>, </em><em>A</em><em>]</em><em>, </em><em>B</em><em>]</em>) – The function used to combine the existing value with the new
value. The default is to append the value to a tuple.</p></li>
<li><p><strong>init_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>]</em><em>, </em><em>B</em><em>]</em>) – For the first value stored, <code class="docutils literal notranslate"><span class="pre">reduce_fn</span></code> will be passed the result
of <code class="docutils literal notranslate"><span class="pre">init_fn</span></code> together with the value to be stored. The default is an
empty tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.BatchEnsembleLinear.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.BatchEnsembleLinear.train" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to training mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=False</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=False</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># initialize Dropout and BatchNorm in eval mode</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">probly.layers.flax.</span></span><span class="sig-name descname"><span class="pre">DropConnectLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#DropConnectLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.DropConnectLinear" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Custom Linear layer with DropConnect applied to weights during training based on <span id="id2">[<a class="reference internal" href="../references.html#id6" title="Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. 2020. URL: https://proceedings.neurips.cc/paper/2020/hash/aab085461de182608ee9f607f3f7d18f-Abstract.html.">ASSR20</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>Any</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Any</p>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.kernel">
<span class="sig-name descname"><span class="pre">kernel</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.kernel" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, weight matrix of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.bias" title="Link to this definition">¶</a></dt>
<dd><p>nnx.Param, bias of the layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.in_features">
<span class="sig-name descname"><span class="pre">in_features</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.in_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of input features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.out_features">
<span class="sig-name descname"><span class="pre">out_features</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.out_features" title="Link to this definition">¶</a></dt>
<dd><p>int, number of output features.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.use_bias">
<span class="sig-name descname"><span class="pre">use_bias</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.use_bias" title="Link to this definition">¶</a></dt>
<dd><p>bool, whether to add bias to the output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.dtype" title="Link to this definition">¶</a></dt>
<dd><p>typing.Optional[flax.typing.Dtype], the dtype of the computation (default: infer from input and params).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.param_dtype">
<span class="sig-name descname"><span class="pre">param_dtype</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.param_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, the dtype passed to parameter initializers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.precision" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PrecisionLike, numerical precision of the computation see <code class="docutils literal notranslate"><span class="pre">jax.lax.Precision</span></code>
for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.kernel_init">
<span class="sig-name descname"><span class="pre">kernel_init</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.kernel_init" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Initializer, initializer function for the weight matrix.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.bias_init">
<span class="sig-name descname"><span class="pre">bias_init</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.bias_init" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Initializer, initializer function for the bias.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.dot_general">
<span class="sig-name descname"><span class="pre">dot_general</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.dot_general" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.DotGeneralT, dot product function.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.promote_dtype">
<span class="sig-name descname"><span class="pre">promote_dtype</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.promote_dtype" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.PromoteDtypeFn, function to promote the dtype of the arrays to the desired
dtype. The function should accept a tuple of <code class="docutils literal notranslate"><span class="pre">(inputs,</span> <span class="pre">kernel,</span> <span class="pre">bias)</span></code>
and a <code class="docutils literal notranslate"><span class="pre">dtype</span></code> keyword argument, and return a tuple of arrays with the
promoted dtype.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.preferred_element_type">
<span class="sig-name descname"><span class="pre">preferred_element_type</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.preferred_element_type" title="Link to this definition">¶</a></dt>
<dd><p>flax.typing.Dtype, Optional parameter controls the data type output by
the dot product. This argument is passed to <code class="docutils literal notranslate"><span class="pre">dot_general</span></code> function.
See <code class="docutils literal notranslate"><span class="pre">jax.lax.dot</span></code> for details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.rate">
<span class="sig-name descname"><span class="pre">rate</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.rate" title="Link to this definition">¶</a></dt>
<dd><p>float, probability of dropping individual weights.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.deterministic">
<span class="sig-name descname"><span class="pre">deterministic</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.deterministic" title="Link to this definition">¶</a></dt>
<dd><p>bool, if false the inputs are scaled by <code class="docutils literal notranslate"><span class="pre">1/(1-rate)</span></code> and
masked, whereas if true, no mask is applied and the inputs are returned
as is.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.rng_collection">
<span class="sig-name descname"><span class="pre">rng_collection</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.rng_collection" title="Link to this definition">¶</a></dt>
<dd><p>str, the rng collection name to use when requesting a rng key.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.rngs">
<span class="sig-name descname"><span class="pre">rngs</span></span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.rngs" title="Link to this definition">¶</a></dt>
<dd><p>rnglib.Rngs or rnglib.RngStream or None, rng key.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rngs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#DropConnectLinear.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.__call__" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass of the DropConnectLinear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Array</em>) – jax.Array, input data.</p></li>
<li><p><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>) – bool, if false the inputs are masked, whereas if true, no mask
is applied and the inputs are returned as is.</p></li>
<li><p><strong>rngs</strong> (<em>Rngs</em><em> | </em><em>RngStream</em><em> | </em><em>Array</em><em> | </em><em>None</em>) – nnx.Rngs, nnx.RngStream or jax.Array, optional key used to generate the dropconnect mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>jax.Array, layer output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Array</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.eval" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to evaluation mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">eval</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=True</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=True</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.iter_children">
<span class="sig-name descname"><span class="pre">iter_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.iter_children" title="Link to this definition">¶</a></dt>
<dd><p>Iterates over all children <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module. This
method is similar to <a class="reference internal" href="#probly.layers.flax.DropConnectLinear.iter_modules" title="probly.layers.flax.DropConnectLinear.iter_modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">iter_modules()</span></code></a>, except it only iterates over the
immediate children, and does not recurse further down.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_children</span></code> creates a generator that yields the key and the Module instance,
where the key is a string representing the attribute name of the Module to access
the corresponding child Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_children</span><span class="p">():</span>
<span class="gp">... </span> <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">batch_norm BatchNorm</span>
<span class="go">dropout Dropout</span>
<span class="go">linear Linear</span>
<span class="go">submodule SubModule</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.iter_modules">
<span class="sig-name descname"><span class="pre">iter_modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.iter_modules" title="Link to this definition">¶</a></dt>
<dd><p>Recursively iterates over all nested <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>’s of the current Module, including
the current Module.</p>
<p><code class="docutils literal notranslate"><span class="pre">iter_modules</span></code> creates a generator that yields the path and the Module instance, where
the path is a tuple of strings or integers representing the path to the Module from the
root Module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SubModule</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">SubModule</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">iter_modules</span><span class="p">():</span>
<span class="gp">... </span>  <span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">(&#39;batch_norm&#39;,) BatchNorm</span>
<span class="go">(&#39;dropout&#39;,) Dropout</span>
<span class="go">(&#39;linear&#39;,) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear1&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;, &#39;linear2&#39;) Linear</span>
<span class="go">(&#39;submodule&#39;,) SubModule</span>
<span class="go">() Block</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.14)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a>[<em>Key</em>, …], <em>Module</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.perturb">
<span class="sig-name descname"><span class="pre">perturb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variable_type=&lt;class</span> <span class="pre">'flax.nnx.variablelib.Perturbation'&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.perturb" title="Link to this definition">¶</a></dt>
<dd><p>Add an zero-value variable (“perturbation”) to the intermediate value.</p>
<p>The gradient of <code class="docutils literal notranslate"><span class="pre">value</span></code> would be the same as the gradient of this
perturbation variable. Therefore, if you define your loss function with
both params and perturbations as standalone arguments, you can get the
intermediate gradients of <code class="docutils literal notranslate"><span class="pre">value</span></code> by running <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> on the
perturbation variable.</p>
<p>Since the shape of the perturbation value depends on the shape of the input,
a perturbation variable is only created after you run a sample input through
the model once.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This creates extra dummy variables of the same size as <code class="docutils literal notranslate"><span class="pre">value</span></code>, thus
occupies more memory. Use it only to debug gradients in training.</p>
</div>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">perturb</span><span class="p">(</span><span class="s1">&#39;xgrad&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;xgrad&#39;</span><span class="p">)</span>  <span class="c1"># perturbation requires a sample input run</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># same as the intermediate value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Param</span><span class="p">,</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Take gradients on the Param and Perturbation variables</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@nnx</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">... </span><span class="k">def</span><span class="w"> </span><span class="nf">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">model</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">graphdef</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">)</span> <span class="o">=</span> <span class="n">grad_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">perturbations</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># `perturbations.xgrad.value` is the intermediate gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">perturbations</span><span class="o">.</span><span class="n">xgrad</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name for the
perturbation value.</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a>) – The value to take intermediate gradient.</p></li>
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><em>Any</em></a><em>]</em><em>]</em>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored perturbation.
Defaulted at <code class="xref py py-class docutils literal notranslate"><span class="pre">nnx.Perturbation</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.set_attributes">
<span class="sig-name descname"><span class="pre">set_attributes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">raise_if_not_found</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.set_attributes" title="Link to this definition">¶</a></dt>
<dd><p>Sets the attributes of nested Modules including the current Module.
If the attribute is not found in the Module, it is ignored.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Filter</span></code>’s can be used to set the attributes of specific Modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">set_attributes</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Only the dropout will be modified</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*filters</strong> (<em>filterlib.Filter</em>) – Filters to select the Modules to set the attributes of.</p></li>
<li><p><strong>raise_if_not_found</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>) – If True (default), raises a ValueError if at least one attribute
instance is not found in one of the selected Modules.</p></li>
<li><p><strong>**attributes</strong> (<em>tp.Any</em>) – The attributes to set.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.set_mode">
<span class="sig-name descname"><span class="pre">set_mode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deterministic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/probly/layers/flax.html#DropConnectLinear.set_mode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.set_mode" title="Link to this definition">¶</a></dt>
<dd><p>Class method used by <code class="docutils literal notranslate"><span class="pre">nnx.set_mode</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em> | </em><em>None</em>) – if True, disables dropconnect masking.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.sow">
<span class="sig-name descname"><span class="pre">sow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_fn=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.sow" title="Link to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">sow()</span></code> can be used to collect intermediate values without
the overhead of explicitly passing a container through each Module call.
<code class="docutils literal notranslate"><span class="pre">sow()</span></code> stores a value in a new <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute, denoted by <code class="docutils literal notranslate"><span class="pre">name</span></code>.
The value will be wrapped by a <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> of type <code class="docutils literal notranslate"><span class="pre">variable_type</span></code>,
which can be useful to filter for in <code class="xref py py-func docutils literal notranslate"><span class="pre">split()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">state()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">pop()</span></code>.</p>
<p>By default the values are stored in a tuple and each stored value
is appended at the end. This way all intermediates can be tracked when
the same module is called multiple times.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">+</span><span class="n">add</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;i&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="c1"># tuple of length 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="c1"># tuple of length 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<p>Alternatively, a custom init/reduce function can be passed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Model</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">+</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">sow</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Intermediate</span><span class="p">,</span> <span class="s1">&#39;product&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">init_fn</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">reduce_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">prev</span><span class="p">,</span> <span class="n">curr</span><span class="p">:</span> <span class="n">prev</span><span class="o">*</span><span class="n">curr</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sum</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">product</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">intermediate</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><em>type</em></a><em>[</em><em>Variable</em><em>[</em><em>B</em><em>]</em><em>] </em><em>| </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – The <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> type for the stored value.
Typically <code class="xref py py-class docutils literal notranslate"><span class="pre">Intermediate</span></code> is used to indicate an
intermediate value.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – A string denoting the <code class="docutils literal notranslate"><span class="pre">Module</span></code> attribute name, where
the sowed value is stored.</p></li>
<li><p><strong>value</strong> (<em>A</em>) – The value to be stored.</p></li>
<li><p><strong>reduce_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>B</em><em>, </em><em>A</em><em>]</em><em>, </em><em>B</em><em>]</em>) – The function used to combine the existing value with the new
value. The default is to append the value to a tuple.</p></li>
<li><p><strong>init_fn</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><em>Callable</em></a><em>[</em><em>[</em><em>]</em><em>, </em><em>B</em><em>]</em>) – For the first value stored, <code class="docutils literal notranslate"><span class="pre">reduce_fn</span></code> will be passed the result
of <code class="docutils literal notranslate"><span class="pre">init_fn</span></code> together with the value to be stored. The default is an
empty tuple.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)">bool</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="probly.layers.flax.DropConnectLinear.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">attributes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#probly.layers.flax.DropConnectLinear.train" title="Link to this definition">¶</a></dt>
<dd><p>Sets the Module to training mode.</p>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code> uses <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code> to recursively set attributes <code class="docutils literal notranslate"><span class="pre">deterministic=False</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_running_average=False</span></code> of all nested Modules that have these attributes.
Its primarily used to control the runtime behavior of the <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
Modules.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">flax</span><span class="w"> </span><span class="kn">import</span> <span class="n">nnx</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">nnx</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rngs</span><span class="p">:</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">din</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># initialize Dropout and BatchNorm in eval mode</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nnx</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">rngs</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">rngs</span><span class="o">=</span><span class="n">nnx</span><span class="o">.</span><span class="n">Rngs</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(True, True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">deterministic</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">use_running_average</span>
<span class="go">(False, False)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**attributes</strong> – additional attributes passed to <code class="docutils literal notranslate"><span class="pre">set_attributes</span></code>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, probly team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">probly.layers.flax</a><ul>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv</span></code></a><ul>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.kernel_shape"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.kernel_shape</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.kernel"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.kernel</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.bias"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.in_features"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.in_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.out_features"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.out_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.kernel_size"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.kernel_size</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.strides"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.strides</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.padding"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.padding</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.input_dilation"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.input_dilation</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.kernel_dilation"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.kernel_dilation</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.feature_group_count"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.feature_group_count</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.use_bias"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.use_bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.mask"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.mask</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.param_dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.param_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.precision"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.precision</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.conv_general_dilated"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.conv_general_dilated</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.promote_dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.promote_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.preferred_element_type"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.preferred_element_type</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.num_members"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.num_members</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.s"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.s</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.r"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.r</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.__call__"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.__call__()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.eval"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.eval()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.iter_children"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.iter_children()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.iter_modules"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.iter_modules()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.perturb"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.perturb()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.set_attributes"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.set_attributes()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.sow"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.sow()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleConv.train"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleConv.train()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear</span></code></a><ul>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.kernel"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.kernel</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.bias"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.in_features"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.in_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.out_features"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.out_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.use_bias"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.use_bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.param_dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.param_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.precision"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.precision</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.dot_general"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.dot_general</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.promote_dtype"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.promote_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.preferred_element_type"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.preferred_element_type</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.num_members"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.num_members</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.s"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.s</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.r"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.r</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.__call__"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.__call__()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.eval"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.eval()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.iter_children"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.iter_children()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.iter_modules"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.iter_modules()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.perturb"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.perturb()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.set_attributes"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.set_attributes()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.sow"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.sow()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.BatchEnsembleLinear.train"><code class="docutils literal notranslate"><span class="pre">BatchEnsembleLinear.train()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear</span></code></a><ul>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.kernel"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.kernel</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.bias"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.in_features"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.in_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.out_features"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.out_features</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.use_bias"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.use_bias</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.dtype"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.param_dtype"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.param_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.precision"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.precision</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.kernel_init"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.kernel_init</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.bias_init"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.bias_init</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.dot_general"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.dot_general</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.promote_dtype"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.promote_dtype</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.preferred_element_type"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.preferred_element_type</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.rate"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.rate</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.deterministic"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.deterministic</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.rng_collection"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.rng_collection</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.rngs"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.rngs</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.__call__"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.__call__()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.eval"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.eval()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.iter_children"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.iter_children()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.iter_modules"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.iter_modules()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.perturb"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.perturb()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.set_attributes"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.set_attributes()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.set_mode"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.set_mode()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.sow"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.sow()</span></code></a></li>
<li><a class="reference internal" href="#probly.layers.flax.DropConnectLinear.train"><code class="docutils literal notranslate"><span class="pre">DropConnectLinear.train()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=4621528c"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=ccdb6887"></script>
    </body>
</html>